{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a390a737",
   "metadata": {},
   "source": [
    "# Day 5: Quantum ML Integration Project ðŸš€\n",
    "\n",
    "## **Welcome to Day 5 - Quantum Machine Learning Integration!**\n",
    "\n",
    "Today we'll bridge quantum chemistry and machine learning by working with the QM9 dataset and implementing state-of-the-art quantum ML models like SchNet. This is where quantum mechanics meets deep learning!\n",
    "\n",
    "### **Project Overview:**\n",
    "- **Section 1:** QM9 Dataset Mastery & Quantum Feature Engineering\n",
    "- **Section 2:** SchNet Implementation & 3D Molecular Understanding\n",
    "- **Section 3:** Delta Learning Framework for QM/ML Hybrid Models\n",
    "- **Section 4:** Advanced Quantum ML Architectures\n",
    "- **Section 5:** Production Pipeline & Integration Toolkit\n",
    "\n",
    "### **Learning Objectives:**\n",
    "- Master the QM9 dataset and quantum property prediction\n",
    "- Implement SchNet for 3D molecular property prediction\n",
    "- Build delta learning frameworks for QM/ML corrections\n",
    "- Create advanced quantum ML architectures\n",
    "- Develop production-ready quantum ML pipelines\n",
    "\n",
    "### **Prerequisites from Previous Days:**\n",
    "- Day 1: ML & Cheminformatics foundations\n",
    "- Day 2: Deep learning for molecules\n",
    "- Day 3: Molecular analysis pipelines\n",
    "- Day 4: Quantum chemistry calculations\n",
    "\n",
    "---\n",
    "\n",
    "## **Section 1: QM9 Dataset Mastery & Quantum Feature Engineering** ðŸ§¬\n",
    "\n",
    "Let's start by mastering the QM9 dataset - one of the most important quantum ML benchmarks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fdac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for Quantum ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core scientific computing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "# Chemistry and quantum computing\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, rdMolDescriptors\n",
    "import deepchem as dc\n",
    "from ase import Atoms\n",
    "from ase.io import read, write\n",
    "\n",
    "# ML and optimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "\n",
    "# Visualization and analysis\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import joblib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ðŸŽ¯ Quantum ML Integration Environment Ready!\")\n",
    "print(f\"ðŸ“Š PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ§ª RDKit available: {Chem is not None}\")\n",
    "print(f\"ðŸ”¬ DeepChem version: {dc.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ“ **DAY 5 ASSESSMENT FRAMEWORK INITIALIZATION**\n",
    "\n",
    "print(\"ðŸŽ“ DAY 5 ASSESSMENT FRAMEWORK INITIALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    from assessment_framework import create_assessment, create_widget, create_dashboard\n",
    "    print(\"âœ… Assessment framework loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Assessment framework not found. Please ensure assessment_framework.py is available.\")\n",
    "    print(\"ðŸ“ Expected location: same directory as this notebook\")\n",
    "    # Create a basic assessment object for fallback\n",
    "    class BasicAssessment:\n",
    "        def start_section(self, section): pass\n",
    "        def end_section(self, section): pass\n",
    "        def record_activity(self, activity, result, metadata=None): pass\n",
    "        def get_progress_summary(self): return {\"overall_score\": 0.0, \"section_scores\": {}}\n",
    "        def get_comprehensive_report(self): return {\"activities\": []}\n",
    "        def save_final_report(self, filename): pass\n",
    "    \n",
    "    class BasicWidget:\n",
    "        def display(self): print(\"ðŸ“‹ Assessment widget would appear here\")\n",
    "    \n",
    "    def create_assessment(student_id, day=5, track=\"quantum_ml\"):\n",
    "        return BasicAssessment()\n",
    "    \n",
    "    def create_widget(assessment, section, concepts, activities):\n",
    "        return BasicWidget()\n",
    "    \n",
    "    def create_dashboard(assessment):\n",
    "        return BasicWidget()\n",
    "\n",
    "# Student Information Collection\n",
    "print(\"\\nðŸ“ Student Assessment Setup:\")\n",
    "student_id = input(\"Enter your student ID: \").strip()\n",
    "if not student_id:\n",
    "    student_id = f\"student_day5_{np.random.randint(1000, 9999)}\"\n",
    "    print(f\"Generated ID: {student_id}\")\n",
    "\n",
    "# Track Selection for Day 5 Quantum ML Specialization\n",
    "print(\"\\nðŸŽ¯ Select your Quantum ML specialization track:\")\n",
    "print(\"1. ðŸ§¬ Quantum Molecular Property Prediction\")\n",
    "print(\"2. ðŸš€ Quantum Neural Network Development\") \n",
    "print(\"3. ðŸ”¬ Quantum-Classical Hybrid Systems\")\n",
    "print(\"4. ðŸ­ Production Quantum ML Engineering\")\n",
    "\n",
    "track_choice = input(\"Enter choice (1-4): \").strip()\n",
    "track_map = {\n",
    "    \"1\": \"quantum_molecular_prediction\",\n",
    "    \"2\": \"quantum_neural_networks\", \n",
    "    \"3\": \"quantum_classical_hybrid\",\n",
    "    \"4\": \"production_quantum_ml\"\n",
    "}\n",
    "\n",
    "track_selected = track_map.get(track_choice, \"quantum_molecular_prediction\")\n",
    "print(f\"Selected track: {track_selected}\")\n",
    "\n",
    "# Initialize Assessment System\n",
    "try:\n",
    "    assessment = create_assessment(student_id=student_id, day=5, track=track_selected)\n",
    "    print(f\"âœ… Assessment initialized for track: {track_selected}\")\n",
    "    print(f\"ðŸ‘¤ Student ID: {student_id}\")\n",
    "    \n",
    "    # Start Day 5 assessment\n",
    "    assessment.start_section(\"day_5_quantum_ml\")\n",
    "    print(\"\\nðŸŽ¯ Day 5 Assessment: Quantum ML Integration\")\n",
    "    print(\"ðŸ“Š Progress tracking enabled - All activities will be recorded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Assessment initialization warning: {e}\")\n",
    "    assessment = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ Ready to begin Day 5: Quantum ML Integration Project!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225754da",
   "metadata": {},
   "source": [
    "### **1.1 QM9 Dataset Handler - Professional Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32135f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QM9DatasetHandler:\n",
    "    \"\"\"\n",
    "    Professional QM9 dataset handler with advanced preprocessing capabilities.\n",
    "    \n",
    "    The QM9 dataset contains ~134k small organic molecules with quantum chemical properties\n",
    "    computed at the B3LYP/6-31G(2df,p) level of theory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"./qm9_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # QM9 property definitions with units and descriptions\n",
    "        self.qm9_properties = {\n",
    "            'mu': {'name': 'Dipole moment', 'unit': 'Debye', 'index': 0},\n",
    "            'alpha': {'name': 'Polarizability', 'unit': 'Bohr^3', 'index': 1},\n",
    "            'homo': {'name': 'HOMO energy', 'unit': 'Hartree', 'index': 2},\n",
    "            'lumo': {'name': 'LUMO energy', 'unit': 'Hartree', 'index': 3},\n",
    "            'gap': {'name': 'HOMO-LUMO gap', 'unit': 'Hartree', 'index': 4},\n",
    "            'r2': {'name': 'Electronic spatial extent', 'unit': 'Bohr^2', 'index': 5},\n",
    "            'zpve': {'name': 'Zero-point vibrational energy', 'unit': 'Hartree', 'index': 6},\n",
    "            'u0': {'name': 'Internal energy at 0K', 'unit': 'Hartree', 'index': 7},\n",
    "            'u298': {'name': 'Internal energy at 298K', 'unit': 'Hartree', 'index': 8},\n",
    "            'h298': {'name': 'Enthalpy at 298K', 'unit': 'Hartree', 'index': 9},\n",
    "            'g298': {'name': 'Free energy at 298K', 'unit': 'Hartree', 'index': 10},\n",
    "            'cv': {'name': 'Heat capacity at 298K', 'unit': 'cal/(mol*K)', 'index': 11}\n",
    "        }\n",
    "        \n",
    "        self.data = None\n",
    "        self.molecular_graphs = []\n",
    "        self.statistics = {}\n",
    "        \n",
    "    def load_qm9_dataset(self, subset_size: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load and preprocess QM9 dataset with caching.\n",
    "        \"\"\"\n",
    "        cache_file = self.cache_dir / f\"qm9_processed_{subset_size or 'full'}.pkl\"\n",
    "        \n",
    "        if cache_file.exists():\n",
    "            logger.info(f\"Loading cached QM9 data from {cache_file}\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                self.data = pickle.load(f)\n",
    "            return self.data\n",
    "        \n",
    "        logger.info(\"Loading QM9 dataset from DeepChem...\")\n",
    "        try:\n",
    "            # Load QM9 dataset using DeepChem\n",
    "            qm9_loader = dc.molnet.load_qm9(featurizer='ECFP', split='random')\n",
    "            train, valid, test = qm9_loader[0]\n",
    "            \n",
    "            # Combine all data\n",
    "            all_smiles = np.concatenate([train[0], valid[0], test[0]])\n",
    "            all_properties = np.concatenate([train[1], valid[1], test[1]])\n",
    "            \n",
    "            # Create DataFrame\n",
    "            property_names = list(self.qm9_properties.keys())\n",
    "            \n",
    "            data_dict = {'smiles': all_smiles}\n",
    "            for i, prop in enumerate(property_names):\n",
    "                data_dict[prop] = all_properties[:, i]\n",
    "            \n",
    "            self.data = pd.DataFrame(data_dict)\n",
    "            \n",
    "            # Apply subset if requested\n",
    "            if subset_size and subset_size < len(self.data):\n",
    "                self.data = self.data.sample(n=subset_size, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "            # Cache the processed data\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(self.data, f)\n",
    "            \n",
    "            logger.info(f\"QM9 dataset loaded: {len(self.data)} molecules\")\n",
    "            return self.data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading QM9 dataset: {e}\")\n",
    "            # Fallback: create synthetic QM9-like data for demonstration\n",
    "            return self._create_synthetic_qm9(subset_size or 1000)\n",
    "    \n",
    "    def _create_synthetic_qm9(self, n_samples: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create synthetic QM9-like data for demonstration purposes.\n",
    "        \"\"\"\n",
    "        logger.warning(\"Creating synthetic QM9-like data for demonstration\")\n",
    "        \n",
    "        # Generate simple organic molecules\n",
    "        simple_smiles = [\n",
    "            'C', 'CC', 'CCC', 'CCCC', 'CCCCC',  # Alkanes\n",
    "            'C=C', 'CC=C', 'C=CC=C',  # Alkenes\n",
    "            'C#C', 'CC#C',  # Alkynes\n",
    "            'c1ccccc1', 'Cc1ccccc1',  # Aromatics\n",
    "            'CO', 'CCO', 'CCCO',  # Alcohols\n",
    "            'C=O', 'CC=O', 'CCC=O',  # Aldehydes/Ketones\n",
    "            'CN', 'CCN', 'CCCN',  # Amines\n",
    "        ]\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        smiles_list = np.random.choice(simple_smiles, n_samples)\n",
    "        \n",
    "        # Generate synthetic properties with realistic ranges\n",
    "        data_dict = {'smiles': smiles_list}\n",
    "        \n",
    "        # Realistic property ranges based on QM9 statistics\n",
    "        property_ranges = {\n",
    "            'mu': (0, 5),  # Debye\n",
    "            'alpha': (10, 100),  # Bohr^3\n",
    "            'homo': (-0.3, -0.1),  # Hartree\n",
    "            'lumo': (-0.1, 0.1),  # Hartree\n",
    "            'gap': (0.05, 0.3),  # Hartree\n",
    "            'r2': (20, 200),  # Bohr^2\n",
    "            'zpve': (0.01, 0.3),  # Hartree\n",
    "            'u0': (-500, -100),  # Hartree\n",
    "            'u298': (-500, -100),  # Hartree\n",
    "            'h298': (-500, -100),  # Hartree\n",
    "            'g298': (-500, -100),  # Hartree\n",
    "            'cv': (5, 50)  # cal/(mol*K)\n",
    "        }\n",
    "        \n",
    "        for prop, (low, high) in property_ranges.items():\n",
    "            data_dict[prop] = np.random.uniform(low, high, n_samples)\n",
    "        \n",
    "        self.data = pd.DataFrame(data_dict)\n",
    "        return self.data\n",
    "    \n",
    "    def compute_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compute comprehensive statistics for QM9 properties.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_qm9_dataset first.\")\n",
    "        \n",
    "        stats = {}\n",
    "        \n",
    "        for prop in self.qm9_properties.keys():\n",
    "            if prop in self.data.columns:\n",
    "                values = self.data[prop].values\n",
    "                stats[prop] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'std': np.std(values),\n",
    "                    'min': np.min(values),\n",
    "                    'max': np.max(values),\n",
    "                    'median': np.median(values),\n",
    "                    'q25': np.percentile(values, 25),\n",
    "                    'q75': np.percentile(values, 75),\n",
    "                    'skewness': self._compute_skewness(values),\n",
    "                    'kurtosis': self._compute_kurtosis(values)\n",
    "                }\n",
    "        \n",
    "        self.statistics = stats\n",
    "        return stats\n",
    "    \n",
    "    def _compute_skewness(self, values: np.ndarray) -> float:\n",
    "        \"\"\"Compute skewness of the distribution.\"\"\"\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        return np.mean(((values - mean) / std) ** 3)\n",
    "    \n",
    "    def _compute_kurtosis(self, values: np.ndarray) -> float:\n",
    "        \"\"\"Compute kurtosis of the distribution.\"\"\"\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        return np.mean(((values - mean) / std) ** 4) - 3\n",
    "    \n",
    "    def visualize_property_distributions(self, properties: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization of QM9 property distributions.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_qm9_dataset first.\")\n",
    "        \n",
    "        if properties is None:\n",
    "            properties = list(self.qm9_properties.keys())\n",
    "        \n",
    "        # Filter available properties\n",
    "        available_props = [p for p in properties if p in self.data.columns]\n",
    "        \n",
    "        n_props = len(available_props)\n",
    "        n_cols = 3\n",
    "        n_rows = (n_props + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=n_rows, cols=n_cols,\n",
    "            subplot_titles=[f\"{prop} ({self.qm9_properties[prop]['unit']})\" \n",
    "                          for prop in available_props],\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        for i, prop in enumerate(available_props):\n",
    "            row = i // n_cols + 1\n",
    "            col = i % n_cols + 1\n",
    "            \n",
    "            values = self.data[prop].values\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=values,\n",
    "                    name=prop,\n",
    "                    nbinsx=50,\n",
    "                    showlegend=False,\n",
    "                    marker_color=px.colors.qualitative.Set3[i % len(px.colors.qualitative.Set3)]\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"QM9 Property Distributions\",\n",
    "            height=300 * n_rows,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Initialize QM9 handler\n",
    "qm9_handler = QM9DatasetHandler()\n",
    "print(\"\\nðŸŽ¯ QM9 Dataset Handler initialized!\")\n",
    "print(\"ðŸ“Š Ready to load and analyze quantum chemical properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c4872",
   "metadata": {},
   "source": [
    "### **1.2 Load and Explore QM9 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QM9 dataset (using subset for faster processing)\n",
    "print(\"Loading QM9 dataset...\")\n",
    "qm9_data = qm9_handler.load_qm9_dataset(subset_size=5000)  # Start with 5k molecules\n",
    "\n",
    "print(f\"\\nðŸ“Š QM9 Dataset Overview:\")\n",
    "print(f\"   â€¢ Total molecules: {len(qm9_data)}\")\n",
    "print(f\"   â€¢ Properties: {len(qm9_handler.qm9_properties)}\")\n",
    "print(f\"   â€¢ Data shape: {qm9_data.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nðŸ” Sample data:\")\n",
    "display(qm9_data.head())\n",
    "\n",
    "# Compute and display statistics\n",
    "print(\"\\nComputing property statistics...\")\n",
    "stats = qm9_handler.compute_statistics()\n",
    "\n",
    "# Create statistics summary table\n",
    "stats_df = pd.DataFrame({\n",
    "    prop: {\n",
    "        'Mean': f\"{data['mean']:.4f}\",\n",
    "        'Std': f\"{data['std']:.4f}\",\n",
    "        'Min': f\"{data['min']:.4f}\",\n",
    "        'Max': f\"{data['max']:.4f}\",\n",
    "        'Unit': qm9_handler.qm9_properties[prop]['unit']\n",
    "    }\n",
    "    for prop, data in stats.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\nðŸ“ˆ QM9 Property Statistics:\")\n",
    "display(stats_df)\n",
    "\n",
    "# Visualize property distributions\n",
    "print(\"\\nGenerating property distribution plots...\")\n",
    "fig = qm9_handler.visualize_property_distributions(['mu', 'alpha', 'homo', 'lumo', 'gap', 'cv'])\n",
    "\n",
    "print(\"\\nâœ… QM9 dataset successfully loaded and analyzed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ **MID-SECTION EXERCISE CHECKPOINT 1.1: QM9 Data Mastery**\n",
    "\n",
    "print(\"ðŸŽ¯ MID-SECTION EXERCISE CHECKPOINT 1.1: QM9 Data Mastery\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Quick hands-on exercise: Analyze specific molecular properties\n",
    "exercise_widget_1_1 = create_widget(\n",
    "    assessment, \n",
    "    section=\"1.1\",\n",
    "    concepts=[\n",
    "        \"QM9 dataset structure understanding\",\n",
    "        \"Molecular property distributions\",\n",
    "        \"Statistical analysis of quantum properties\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Loaded and explored QM9 dataset\",\n",
    "        \"Analyzed property statistics and distributions\", \n",
    "        \"Interpreted quantum property ranges\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if assessment:\n",
    "    assessment.record_activity(\n",
    "        activity=\"qm9_data_exploration\",\n",
    "        result=\"completed\",\n",
    "        metadata={\n",
    "            \"dataset_size\": len(qm9_data),\n",
    "            \"properties_analyzed\": len(stats),\n",
    "            \"section\": \"1.1_mid_checkpoint\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "exercise_widget_1_1.display()\n",
    "print(\"ðŸŽ“ Mid-section checkpoint 1.1 completed! Continue with feature engineering...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3e0f7",
   "metadata": {},
   "source": [
    "### **1.3 Quantum Feature Engineering Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5512d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Advanced quantum feature engineering for molecular property prediction.\n",
    "    \n",
    "    This class extracts and engineers features that are specifically relevant\n",
    "    for quantum mechanical properties.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_cache = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def extract_molecular_features(self, smiles_list: List[str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Extract comprehensive molecular features for quantum property prediction.\n",
    "        \"\"\"\n",
    "        features = {\n",
    "            'constitutional': [],\n",
    "            'topological': [],\n",
    "            'electronic': [],\n",
    "            'geometric': [],\n",
    "            'quantum_descriptors': []\n",
    "        }\n",
    "        \n",
    "        valid_molecules = []\n",
    "        \n",
    "        for smiles in smiles_list:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "                \n",
    "            valid_molecules.append(smiles)\n",
    "            \n",
    "            # Constitutional descriptors\n",
    "            const_features = self._extract_constitutional_features(mol)\n",
    "            features['constitutional'].append(const_features)\n",
    "            \n",
    "            # Topological descriptors\n",
    "            topo_features = self._extract_topological_features(mol)\n",
    "            features['topological'].append(topo_features)\n",
    "            \n",
    "            # Electronic descriptors\n",
    "            elec_features = self._extract_electronic_features(mol)\n",
    "            features['electronic'].append(elec_features)\n",
    "            \n",
    "            # Geometric descriptors (if 3D coordinates available)\n",
    "            geom_features = self._extract_geometric_features(mol)\n",
    "            features['geometric'].append(geom_features)\n",
    "            \n",
    "            # Quantum-specific descriptors\n",
    "            quantum_features = self._extract_quantum_descriptors(mol)\n",
    "            features['quantum_descriptors'].append(quantum_features)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        for key in features:\n",
    "            if features[key]:\n",
    "                features[key] = np.array(features[key])\n",
    "            else:\n",
    "                features[key] = np.array([]).reshape(0, 0)\n",
    "        \n",
    "        self.valid_molecules = valid_molecules\n",
    "        return features\n",
    "    \n",
    "    def _extract_constitutional_features(self, mol: Chem.Mol) -> List[float]:\n",
    "        \"\"\"\n",
    "        Extract constitutional molecular descriptors.\n",
    "        \"\"\"\n",
    "        features = [\n",
    "            mol.GetNumAtoms(),  # Number of atoms\n",
    "            mol.GetNumBonds(),  # Number of bonds\n",
    "            mol.GetNumHeavyAtoms(),  # Number of heavy atoms\n",
    "            Descriptors.MolWt(mol),  # Molecular weight\n",
    "            Descriptors.NumHeteroatoms(mol),  # Number of heteroatoms\n",
    "            Descriptors.NumRotatableBonds(mol),  # Number of rotatable bonds\n",
    "            Descriptors.NumAromaticRings(mol),  # Number of aromatic rings\n",
    "            Descriptors.NumSaturatedRings(mol),  # Number of saturated rings\n",
    "            Descriptors.RingCount(mol),  # Total ring count\n",
    "            Descriptors.FractionCsp3(mol),  # Fraction of sp3 carbons\n",
    "        ]\n",
    "        \n",
    "        # Atom type counts\n",
    "        atom_counts = {'C': 0, 'N': 0, 'O': 0, 'F': 0, 'P': 0, 'S': 0, 'Cl': 0, 'Br': 0}\n",
    "        for atom in mol.GetAtoms():\n",
    "            symbol = atom.GetSymbol()\n",
    "            if symbol in atom_counts:\n",
    "                atom_counts[symbol] += 1\n",
    "        \n",
    "        features.extend(list(atom_counts.values()))\n",
    "        return features\n",
    "    \n",
    "    def _extract_topological_features(self, mol: Chem.Mol) -> List[float]:\n",
    "        \"\"\"\n",
    "        Extract topological molecular descriptors.\n",
    "        \"\"\"\n",
    "        features = [\n",
    "            Descriptors.Chi0(mol),  # Chi0 connectivity index\n",
    "            Descriptors.Chi1(mol),  # Chi1 connectivity index\n",
    "            Descriptors.BalabanJ(mol),  # Balaban J index\n",
    "            Descriptors.Kappa1(mol),  # Kappa1 shape index\n",
    "            Descriptors.Kappa2(mol),  # Kappa2 shape index\n",
    "            Descriptors.Kappa3(mol),  # Kappa3 shape index\n",
    "            rdMolDescriptors.CalcNumSpiroAtoms(mol),  # Number of spiro atoms\n",
    "            rdMolDescriptors.CalcNumBridgeheadAtoms(mol),  # Number of bridgehead atoms\n",
    "        ]\n",
    "        \n",
    "        # Handle potential None values\n",
    "        features = [f if f is not None else 0.0 for f in features]\n",
    "        return features\n",
    "    \n",
    "    def _extract_electronic_features(self, mol: Chem.Mol) -> List[float]:\n",
    "        \"\"\"\n",
    "        Extract electronic molecular descriptors.\n",
    "        \"\"\"\n",
    "        features = [\n",
    "            Descriptors.NumValenceElectrons(mol),  # Number of valence electrons\n",
    "            Descriptors.MaxPartialCharge(mol),  # Maximum partial charge\n",
    "            Descriptors.MinPartialCharge(mol),  # Minimum partial charge\n",
    "            Descriptors.MaxAbsPartialCharge(mol),  # Maximum absolute partial charge\n",
    "            Descriptors.MinAbsPartialCharge(mol),  # Minimum absolute partial charge\n",
    "        ]\n",
    "        \n",
    "        # Compute partial charges using Gasteiger method\n",
    "        try:\n",
    "            AllChem.ComputeGasteigerCharges(mol)\n",
    "            charges = [float(atom.GetProp('_GasteigerCharge')) for atom in mol.GetAtoms()]\n",
    "            if charges:\n",
    "                features.extend([\n",
    "                    np.mean(charges),  # Mean partial charge\n",
    "                    np.std(charges),   # Std of partial charges\n",
    "                    np.sum(np.abs(charges)),  # Sum of absolute charges\n",
    "                ])\n",
    "            else:\n",
    "                features.extend([0.0, 0.0, 0.0])\n",
    "        except:\n",
    "            features.extend([0.0, 0.0, 0.0])\n",
    "        \n",
    "        # Handle potential None values\n",
    "        features = [f if f is not None else 0.0 for f in features]\n",
    "        return features\n",
    "    \n",
    "    def _extract_geometric_features(self, mol: Chem.Mol) -> List[float]:\n",
    "        \"\"\"\n",
    "        Extract geometric molecular descriptors.\n",
    "        \"\"\"\n",
    "        # Generate 3D coordinates if not present\n",
    "        mol_copy = Chem.AddHs(mol)\n",
    "        \n",
    "        try:\n",
    "            AllChem.EmbedMolecule(mol_copy, randomSeed=42)\n",
    "            AllChem.OptimizeMoleculeConfs(mol_copy)\n",
    "            \n",
    "            features = [\n",
    "                Descriptors.Asphericity(mol_copy),  # Asphericity\n",
    "                Descriptors.Eccentricity(mol_copy),  # Eccentricity\n",
    "                Descriptors.InertialShapeFactor(mol_copy),  # Inertial shape factor\n",
    "                Descriptors.RadiusOfGyration(mol_copy),  # Radius of gyration\n",
    "                Descriptors.SpherocityIndex(mol_copy),  # Spherocity index\n",
    "            ]\n",
    "        except:\n",
    "            # If 3D generation fails, use default values\n",
    "            features = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        \n",
    "        # Handle potential None values\n",
    "        features = [f if f is not None else 0.0 for f in features]\n",
    "        return features\n",
    "    \n",
    "    def _extract_quantum_descriptors(self, mol: Chem.Mol) -> List[float]:\n",
    "        \"\"\"\n",
    "        Extract quantum-chemistry specific descriptors.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Aromaticity and conjugation features\n",
    "        aromatic_atoms = sum(1 for atom in mol.GetAtoms() if atom.GetIsAromatic())\n",
    "        aromatic_bonds = sum(1 for bond in mol.GetBonds() if bond.GetIsAromatic())\n",
    "        conjugated_bonds = sum(1 for bond in mol.GetBonds() if bond.GetIsConjugated())\n",
    "        \n",
    "        features.extend([\n",
    "            aromatic_atoms / mol.GetNumAtoms() if mol.GetNumAtoms() > 0 else 0,\n",
    "            aromatic_bonds / mol.GetNumBonds() if mol.GetNumBonds() > 0 else 0,\n",
    "            conjugated_bonds / mol.GetNumBonds() if mol.GetNumBonds() > 0 else 0,\n",
    "        ])\n",
    "        \n",
    "        # Hybridization state features\n",
    "        hybridization_counts = {'SP': 0, 'SP2': 0, 'SP3': 0}\n",
    "        for atom in mol.GetAtoms():\n",
    "            hyb = str(atom.GetHybridization())\n",
    "            if hyb in hybridization_counts:\n",
    "                hybridization_counts[hyb] += 1\n",
    "        \n",
    "        total_atoms = mol.GetNumAtoms()\n",
    "        if total_atoms > 0:\n",
    "            features.extend([\n",
    "                hybridization_counts['SP'] / total_atoms,\n",
    "                hybridization_counts['SP2'] / total_atoms,\n",
    "                hybridization_counts['SP3'] / total_atoms,\n",
    "            ])\n",
    "        else:\n",
    "            features.extend([0.0, 0.0, 0.0])\n",
    "        \n",
    "        # Formal charge distribution\n",
    "        formal_charges = [atom.GetFormalCharge() for atom in mol.GetAtoms()]\n",
    "        if formal_charges:\n",
    "            features.extend([\n",
    "                np.sum(formal_charges),  # Total formal charge\n",
    "                np.sum(np.abs(formal_charges)),  # Sum of absolute formal charges\n",
    "                len([c for c in formal_charges if c != 0]),  # Number of charged atoms\n",
    "            ])\n",
    "        else:\n",
    "            features.extend([0.0, 0.0, 0.0])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def create_feature_matrix(self, features_dict: Dict[str, np.ndarray]) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Combine all feature types into a single matrix.\n",
    "        \"\"\"\n",
    "        feature_arrays = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for feature_type, features in features_dict.items():\n",
    "            if features.size > 0:\n",
    "                feature_arrays.append(features)\n",
    "                \n",
    "                # Generate feature names\n",
    "                n_features = features.shape[1]\n",
    "                names = [f\"{feature_type}_{i}\" for i in range(n_features)]\n",
    "                feature_names.extend(names)\n",
    "        \n",
    "        if feature_arrays:\n",
    "            combined_features = np.hstack(feature_arrays)\n",
    "        else:\n",
    "            combined_features = np.array([]).reshape(0, 0)\n",
    "        \n",
    "        return combined_features, feature_names\n",
    "    \n",
    "    def scale_features(self, features: np.ndarray, method: str = 'standard') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Scale features using specified method.\n",
    "        \"\"\"\n",
    "        if method not in self.scalers:\n",
    "            if method == 'standard':\n",
    "                self.scalers[method] = StandardScaler()\n",
    "            elif method == 'robust':\n",
    "                self.scalers[method] = RobustScaler()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown scaling method: {method}\")\n",
    "        \n",
    "        scaler = self.scalers[method]\n",
    "        \n",
    "        if not hasattr(scaler, 'mean_'):\n",
    "            # Fit the scaler\n",
    "            scaled_features = scaler.fit_transform(features)\n",
    "        else:\n",
    "            # Transform using existing fit\n",
    "            scaled_features = scaler.transform(features)\n",
    "        \n",
    "        return scaled_features\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = QuantumFeatureEngineer()\n",
    "print(\"\\nðŸŽ¯ Quantum Feature Engineer initialized!\")\n",
    "print(\"ðŸ”¬ Ready to extract quantum-specific molecular features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac9f0f",
   "metadata": {},
   "source": [
    "### **1.4 Extract and Analyze Molecular Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comprehensive molecular features\n",
    "print(\"Extracting molecular features from QM9 dataset...\")\n",
    "smiles_list = qm9_data['smiles'].tolist()\n",
    "\n",
    "# Extract features (this may take a few minutes)\n",
    "start_time = time.time()\n",
    "features_dict = feature_engineer.extract_molecular_features(smiles_list)\n",
    "feature_extraction_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâ±ï¸ Feature extraction completed in {feature_extraction_time:.2f} seconds\")\n",
    "\n",
    "# Display feature statistics\n",
    "for feature_type, features in features_dict.items():\n",
    "    if features.size > 0:\n",
    "        print(f\"   â€¢ {feature_type}: {features.shape[1]} features, {features.shape[0]} molecules\")\n",
    "    else:\n",
    "        print(f\"   â€¢ {feature_type}: No features extracted\")\n",
    "\n",
    "# Create combined feature matrix\n",
    "print(\"\\nCombining features into matrix...\")\n",
    "feature_matrix, feature_names = feature_engineer.create_feature_matrix(features_dict)\n",
    "\n",
    "print(f\"\\nðŸ“Š Combined Feature Matrix:\")\n",
    "print(f\"   â€¢ Shape: {feature_matrix.shape}\")\n",
    "print(f\"   â€¢ Total features: {len(feature_names)}\")\n",
    "print(f\"   â€¢ Valid molecules: {len(feature_engineer.valid_molecules)}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\nScaling features...\")\n",
    "scaled_features = feature_engineer.scale_features(feature_matrix, method='standard')\n",
    "\n",
    "print(f\"âœ… Features extracted and scaled successfully!\")\n",
    "print(f\"   â€¢ Original range: [{feature_matrix.min():.3f}, {feature_matrix.max():.3f}]\")\n",
    "print(f\"   â€¢ Scaled range: [{scaled_features.min():.3f}, {scaled_features.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef9fa3",
   "metadata": {},
   "source": [
    "### **1.5 Feature-Property Correlation Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align QM9 data with valid molecules\n",
    "valid_indices = [i for i, smiles in enumerate(qm9_data['smiles']) \n",
    "                if smiles in feature_engineer.valid_molecules]\n",
    "aligned_qm9_data = qm9_data.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"Aligned dataset: {len(aligned_qm9_data)} molecules\")\n",
    "\n",
    "# Compute correlations between features and properties\n",
    "property_columns = ['mu', 'alpha', 'homo', 'lumo', 'gap', 'cv']\n",
    "available_properties = [p for p in property_columns if p in aligned_qm9_data.columns]\n",
    "\n",
    "correlations = {}\n",
    "for prop in available_properties:\n",
    "    property_values = aligned_qm9_data[prop].values\n",
    "    \n",
    "    # Compute correlations with each feature type\n",
    "    correlations[prop] = {}\n",
    "    \n",
    "    start_idx = 0\n",
    "    for feature_type, features in features_dict.items():\n",
    "        if features.size > 0:\n",
    "            end_idx = start_idx + features.shape[1]\n",
    "            \n",
    "            # Compute correlation for this feature type\n",
    "            feature_subset = scaled_features[:, start_idx:end_idx]\n",
    "            corr_values = []\n",
    "            \n",
    "            for i in range(feature_subset.shape[1]):\n",
    "                corr = np.corrcoef(property_values, feature_subset[:, i])[0, 1]\n",
    "                if not np.isnan(corr):\n",
    "                    corr_values.append(abs(corr))\n",
    "                else:\n",
    "                    corr_values.append(0.0)\n",
    "            \n",
    "            correlations[prop][feature_type] = {\n",
    "                'max': max(corr_values) if corr_values else 0.0,\n",
    "                'mean': np.mean(corr_values) if corr_values else 0.0,\n",
    "                'top_features': sorted(enumerate(corr_values), key=lambda x: x[1], reverse=True)[:3]\n",
    "            }\n",
    "            \n",
    "            start_idx = end_idx\n",
    "\n",
    "# Display correlation results\n",
    "print(\"\\nðŸ” Feature-Property Correlation Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prop in available_properties:\n",
    "    prop_info = qm9_handler.qm9_properties[prop]\n",
    "    print(f\"\\nðŸ“Š {prop_info['name']} ({prop_info['unit']}):\")\n",
    "    \n",
    "    for feature_type, corr_data in correlations[prop].items():\n",
    "        print(f\"   â€¢ {feature_type.title()}: max={corr_data['max']:.3f}, mean={corr_data['mean']:.3f}\")\n",
    "\n",
    "# Create correlation heatmap for top features\n",
    "def create_correlation_heatmap(property_name: str, top_n: int = 20):\n",
    "    \"\"\"Create correlation heatmap for top features.\"\"\"\n",
    "    if property_name not in available_properties:\n",
    "        return None\n",
    "    \n",
    "    property_values = aligned_qm9_data[property_name].values\n",
    "    \n",
    "    # Find top correlating features across all types\n",
    "    all_correlations = []\n",
    "    feature_labels = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    for feature_type, features in features_dict.items():\n",
    "        if features.size > 0:\n",
    "            end_idx = start_idx + features.shape[1]\n",
    "            feature_subset = scaled_features[:, start_idx:end_idx]\n",
    "            \n",
    "            for i in range(feature_subset.shape[1]):\n",
    "                corr = np.corrcoef(property_values, feature_subset[:, i])[0, 1]\n",
    "                if not np.isnan(corr):\n",
    "                    all_correlations.append(abs(corr))\n",
    "                    feature_labels.append(f\"{feature_type}_{i}\")\n",
    "                else:\n",
    "                    all_correlations.append(0.0)\n",
    "                    feature_labels.append(f\"{feature_type}_{i}\")\n",
    "            \n",
    "            start_idx = end_idx\n",
    "    \n",
    "    # Get top correlating features\n",
    "    top_indices = np.argsort(all_correlations)[-top_n:][::-1]\n",
    "    top_features = scaled_features[:, top_indices]\n",
    "    top_labels = [feature_labels[i] for i in top_indices]\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr_matrix = np.corrcoef(np.column_stack([property_values, top_features.T]))\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=corr_matrix[1:, 1:],  # Exclude property vs property correlation\n",
    "        x=top_labels,\n",
    "        y=top_labels,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0,\n",
    "        text=np.round(corr_matrix[1:, 1:], 3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 8},\n",
    "        showscale=True\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Top {top_n} Feature Correlations for {property_name}\",\n",
    "        xaxis_title=\"Features\",\n",
    "        yaxis_title=\"Features\",\n",
    "        height=600,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Show correlation heatmap for HOMO energy\n",
    "print(f\"\\nCreating correlation heatmap for HOMO energy...\")\n",
    "if 'homo' in available_properties:\n",
    "    homo_heatmap = create_correlation_heatmap('homo', top_n=15)\n",
    "    if homo_heatmap:\n",
    "        homo_heatmap.show()\n",
    "\n",
    "print(\"âœ… Feature-property correlation analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e0f298",
   "metadata": {},
   "source": [
    "### **1.6 Baseline ML Models for Quantum Property Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPropertyPredictor:\n",
    "    \"\"\"\n",
    "    Baseline ML models for quantum property prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def train_baseline_models(self, X: np.ndarray, y_dict: Dict[str, np.ndarray], \n",
    "                            test_size: float = 0.2, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Train baseline models for each quantum property.\n",
    "        \"\"\"\n",
    "        self.results = {}\n",
    "        \n",
    "        for property_name, y_values in y_dict.items():\n",
    "            print(f\"\\nðŸŽ¯ Training models for {property_name}...\")\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_values, test_size=test_size, random_state=random_state\n",
    "            )\n",
    "            \n",
    "            # Train Random Forest\n",
    "            rf_model = RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=15,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            rf_model.fit(X_train, y_train)\n",
    "            rf_pred = rf_model.predict(X_test)\n",
    "            \n",
    "            # Compute metrics\n",
    "            rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "            rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "            rf_r2 = r2_score(y_test, rf_pred)\n",
    "            \n",
    "            # Store results\n",
    "            self.models[property_name] = rf_model\n",
    "            self.results[property_name] = {\n",
    "                'mae': rf_mae,\n",
    "                'rmse': rf_rmse,\n",
    "                'r2': rf_r2,\n",
    "                'y_test': y_test,\n",
    "                'y_pred': rf_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"   â€¢ MAE: {rf_mae:.4f}\")\n",
    "            print(f\"   â€¢ RMSE: {rf_rmse:.4f}\")\n",
    "            print(f\"   â€¢ RÂ²: {rf_r2:.4f}\")\n",
    "    \n",
    "    def create_prediction_plots(self):\n",
    "        \"\"\"\n",
    "        Create prediction vs actual plots for all properties.\n",
    "        \"\"\"\n",
    "        n_props = len(self.results)\n",
    "        n_cols = 3\n",
    "        n_rows = (n_props + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=n_rows, cols=n_cols,\n",
    "            subplot_titles=list(self.results.keys()),\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        for i, (prop, results) in enumerate(self.results.items()):\n",
    "            row = i // n_cols + 1\n",
    "            col = i % n_cols + 1\n",
    "            \n",
    "            y_test = results['y_test']\n",
    "            y_pred = results['y_pred']\n",
    "            \n",
    "            # Scatter plot\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=y_test,\n",
    "                    y=y_pred,\n",
    "                    mode='markers',\n",
    "                    name=f'{prop}',\n",
    "                    showlegend=False,\n",
    "                    marker=dict(\n",
    "                        size=4,\n",
    "                        opacity=0.6,\n",
    "                        color=px.colors.qualitative.Set3[i % len(px.colors.qualitative.Set3)]\n",
    "                    )\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            \n",
    "            # Perfect prediction line\n",
    "            min_val = min(y_test.min(), y_pred.min())\n",
    "            max_val = max(y_test.max(), y_pred.max())\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[min_val, max_val],\n",
    "                    y=[min_val, max_val],\n",
    "                    mode='lines',\n",
    "                    line=dict(dash='dash', color='red'),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            \n",
    "            # Update subplot labels\n",
    "            fig.update_xaxes(title_text=\"Actual\", row=row, col=col)\n",
    "            fig.update_yaxes(title_text=\"Predicted\", row=row, col=col)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Baseline Model Performance: Predicted vs Actual\",\n",
    "            height=300 * n_rows,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Initialize predictor and train baseline models\n",
    "predictor = QuantumPropertyPredictor()\n",
    "\n",
    "# Prepare target variables\n",
    "target_dict = {}\n",
    "for prop in available_properties:\n",
    "    target_dict[prop] = aligned_qm9_data[prop].values\n",
    "\n",
    "print(\"ðŸŽ¯ Training baseline Random Forest models...\")\n",
    "predictor.train_baseline_models(scaled_features, target_dict)\n",
    "\n",
    "# Create performance plots\n",
    "print(\"\\nCreating prediction performance plots...\")\n",
    "performance_fig = predictor.create_prediction_plots()\n",
    "performance_fig.show()\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\nðŸ“Š Baseline Model Performance Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for prop, results in predictor.results.items():\n",
    "    prop_info = qm9_handler.qm9_properties[prop]\n",
    "    print(f\"{prop_info['name']:25} | MAE: {results['mae']:.4f} | RÂ²: {results['r2']:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Section 1 Complete: QM9 Dataset Mastery & Quantum Feature Engineering\")\n",
    "print(\"ðŸŽ¯ Ready to move to Section 2: SchNet Implementation & 3D Molecular Understanding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‹ SECTION 1 CHECKPOINT ASSESSMENT: QM9 Dataset Mastery & Quantum Feature Engineering\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ SECTION 1 CHECKPOINT ASSESSMENT: QM9 Dataset Mastery & Quantum Feature Engineering\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if assessment:\n",
    "    # Record section completion\n",
    "    assessment.record_activity(\n",
    "        \"section_1_completion\", \n",
    "        \"completed\",\n",
    "        {\"section\": \"QM9 Dataset Mastery & Quantum Feature Engineering\", \"timestamp\": datetime.now().isoformat()}\n",
    "    )\n",
    "\n",
    "# Create assessment widget for Section 1\n",
    "section1_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 1: QM9 Dataset Mastery & Quantum Feature Engineering\",\n",
    "    concepts=[\n",
    "        \"QM9 dataset structure and properties understanding\",\n",
    "        \"Quantum chemical property analysis and visualization\", \n",
    "        \"3D molecular coordinate handling and preprocessing\",\n",
    "        \"Molecular graph construction from 3D structures\",\n",
    "        \"Quantum feature engineering and scaling techniques\",\n",
    "        \"Data splits and cross-validation for quantum ML\",\n",
    "        \"Performance metrics for quantum property prediction\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Successfully loaded and explored QM9 dataset\",\n",
    "        \"Analyzed quantum chemical properties (HOMO, LUMO, dipole moment)\",\n",
    "        \"Implemented molecular graph construction pipeline\",\n",
    "        \"Created 3D visualization of molecular structures\",\n",
    "        \"Generated quantum-aware molecular features\",\n",
    "        \"Established training/validation splits for QM9 data\",\n",
    "        \"Computed baseline statistics and property distributions\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the interactive assessment\n",
    "section1_widget.display()\n",
    "\n",
    "# Progress tracking\n",
    "if assessment:\n",
    "    print(f\"\\nðŸ“Š Current Progress: {assessment.get_progress_summary()['overall_score']:.1f}%\")\n",
    "    print(\"ðŸŽ¯ Section 1 assessment completed - proceed when ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06ded2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 2: SchNet Implementation & 3D Molecular Understanding** ðŸŒ\n",
    "\n",
    "Now let's implement SchNet, a state-of-the-art deep learning architecture for 3D molecular property prediction that leverages continuous-filter convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2587057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for SchNet implementation\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax\n",
    "from torch_scatter import scatter_add, scatter_mean\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "print(\"ðŸš€ SchNet Implementation Environment Ready!\")\n",
    "print(f\"   â€¢ PyTorch Geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b336f",
   "metadata": {},
   "source": [
    "### **2.1 SchNet Architecture Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fb95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Continuous-filter convolutional layer as used in SchNet.\n",
    "    \n",
    "    This layer uses continuous filters to model interactions between atoms\n",
    "    based on their distances, making it suitable for 3D molecular modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, num_filters: int, \n",
    "                 cutoff: float = 10.0, smooth: bool = True):\n",
    "        super(CFConv, self).__init__(aggr='add')\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_filters = num_filters\n",
    "        self.cutoff = cutoff\n",
    "        self.smooth = smooth\n",
    "        \n",
    "        # Filter-generating network\n",
    "        self.filter_network = nn.Sequential(\n",
    "            nn.Linear(1, num_filters),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(num_filters, num_filters),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(num_filters, in_channels * out_channels)\n",
    "        )\n",
    "        \n",
    "        # Gate network for smooth cutoff\n",
    "        if smooth:\n",
    "            self.gate_network = nn.Sequential(\n",
    "                nn.Linear(1, num_filters),\n",
    "                nn.Softplus(),\n",
    "                nn.Linear(num_filters, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        for layer in self.filter_network:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        \n",
    "        if self.smooth:\n",
    "            for layer in self.gate_network:\n",
    "                if hasattr(layer, 'reset_parameters'):\n",
    "                    layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, \n",
    "                edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of continuous-filter convolution.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            edge_index: Graph connectivity [2, num_edges]\n",
    "            edge_attr: Edge attributes (distances) [num_edges, 1]\n",
    "        \"\"\"\n",
    "        # Generate continuous filters based on distances\n",
    "        edge_filters = self.filter_network(edge_attr)\n",
    "        edge_filters = edge_filters.view(-1, self.in_channels, self.out_channels)\n",
    "        \n",
    "        # Apply smooth cutoff if enabled\n",
    "        if self.smooth:\n",
    "            cutoff_values = self.gate_network(edge_attr)\n",
    "            edge_filters = edge_filters * cutoff_values.unsqueeze(-1)\n",
    "        \n",
    "        # Propagate messages\n",
    "        return self.propagate(edge_index, x=x, edge_filters=edge_filters)\n",
    "    \n",
    "    def message(self, x_j: torch.Tensor, edge_filters: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Message function: apply continuous filters to neighboring node features.\n",
    "        \"\"\"\n",
    "        # x_j: [num_edges, in_channels]\n",
    "        # edge_filters: [num_edges, in_channels, out_channels]\n",
    "        \n",
    "        # Apply filters via batch matrix multiplication\n",
    "        messages = torch.bmm(x_j.unsqueeze(1), edge_filters).squeeze(1)\n",
    "        return messages\n",
    "\n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    SchNet interaction block combining CFConv with residual connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_channels: int, num_filters: int, cutoff: float = 10.0):\n",
    "        super(InteractionBlock, self).__init__()\n",
    "        \n",
    "        self.cfconv = CFConv(hidden_channels, hidden_channels, num_filters, cutoff)\n",
    "        \n",
    "        # Dense layers for feature transformation\n",
    "        self.dense1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.dense2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.activation = nn.Softplus()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, \n",
    "                edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with residual connection.\"\"\"\n",
    "        # Store input for residual connection\n",
    "        residual = x\n",
    "        \n",
    "        # Apply continuous-filter convolution\n",
    "        x = self.cfconv(x, edge_index, edge_attr)\n",
    "        \n",
    "        # Apply dense transformations\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        # Add residual connection\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class SchNet(nn.Module):\n",
    "    \"\"\"\n",
    "    SchNet architecture for molecular property prediction.\n",
    "    \n",
    "    SchNet uses continuous-filter convolutional layers to learn representations\n",
    "    of molecules that respect rotational and translational invariance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_channels: int = 128, num_filters: int = 128,\n",
    "                 num_interactions: int = 6, num_gaussians: int = 50,\n",
    "                 cutoff: float = 10.0, readout: str = 'add'):\n",
    "        super(SchNet, self).__init__()\n",
    "        \n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_filters = num_filters\n",
    "        self.num_interactions = num_interactions\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.cutoff = cutoff\n",
    "        self.readout = readout\n",
    "        \n",
    "        # Atomic number embedding (assuming max atomic number 100)\n",
    "        self.atomic_embedding = nn.Embedding(100, hidden_channels)\n",
    "        \n",
    "        # Distance expansion using Gaussian basis functions\n",
    "        self.distance_expansion = GaussianSmearing(0.0, cutoff, num_gaussians)\n",
    "        \n",
    "        # Interaction blocks\n",
    "        self.interactions = nn.ModuleList([\n",
    "            InteractionBlock(hidden_channels, num_filters, cutoff)\n",
    "            for _ in range(num_interactions)\n",
    "        ])\n",
    "        \n",
    "        # Output network\n",
    "        self.output_network = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize all parameters.\"\"\"\n",
    "        self.atomic_embedding.reset_parameters()\n",
    "        \n",
    "        for interaction in self.interactions:\n",
    "            interaction.dense1.reset_parameters()\n",
    "            interaction.dense2.reset_parameters()\n",
    "            interaction.cfconv.reset_parameters()\n",
    "        \n",
    "        for layer in self.output_network:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, batch: Data) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of SchNet.\n",
    "        \n",
    "        Args:\n",
    "            batch: PyTorch Geometric batch containing:\n",
    "                - x: Atomic numbers [num_atoms]\n",
    "                - pos: 3D coordinates [num_atoms, 3]\n",
    "                - edge_index: Graph connectivity [2, num_edges]\n",
    "                - batch: Batch assignment [num_atoms]\n",
    "        \"\"\"\n",
    "        x, pos, edge_index, batch_idx = batch.x, batch.pos, batch.edge_index, batch.batch\n",
    "        \n",
    "        # Compute edge distances\n",
    "        row, col = edge_index\n",
    "        edge_distances = torch.norm(pos[row] - pos[col], dim=1).unsqueeze(-1)\n",
    "        \n",
    "        # Filter edges by cutoff distance\n",
    "        edge_mask = edge_distances.squeeze() <= self.cutoff\n",
    "        edge_index = edge_index[:, edge_mask]\n",
    "        edge_distances = edge_distances[edge_mask]\n",
    "        \n",
    "        # Expand distances using Gaussian basis\n",
    "        edge_attr = self.distance_expansion(edge_distances)\n",
    "        \n",
    "        # Embed atomic numbers\n",
    "        x = self.atomic_embedding(x.long())\n",
    "        \n",
    "        # Apply interaction blocks\n",
    "        for interaction in self.interactions:\n",
    "            x = interaction(x, edge_index, edge_attr)\n",
    "        \n",
    "        # Global pooling\n",
    "        if self.readout == 'add':\n",
    "            x = global_add_pool(x, batch_idx)\n",
    "        elif self.readout == 'mean':\n",
    "            x = global_mean_pool(x, batch_idx)\n",
    "        \n",
    "        # Final output\n",
    "        return self.output_network(x)\n",
    "\n",
    "\n",
    "class GaussianSmearing(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian smearing of distances for continuous representations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, start: float = 0.0, stop: float = 5.0, num_gaussians: int = 50):\n",
    "        super(GaussianSmearing, self).__init__()\n",
    "        \n",
    "        offset = torch.linspace(start, stop, num_gaussians)\n",
    "        self.coeff = -0.5 / (offset[1] - offset[0]).item() ** 2\n",
    "        self.register_buffer('offset', offset)\n",
    "    \n",
    "    def forward(self, dist: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply Gaussian smearing to distances.\"\"\"\n",
    "        dist = dist.view(-1, 1) - self.offset.view(1, -1)\n",
    "        return torch.exp(self.coeff * torch.pow(dist, 2))\n",
    "\n",
    "print(\"âœ… SchNet architecture implemented!\")\n",
    "print(\"ðŸŽ¯ Ready for 3D molecular property prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8373da4",
   "metadata": {},
   "source": [
    "### **2.2 3D Molecular Graph Construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d37647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularGraphBuilder:\n",
    "    \"\"\"\n",
    "    Build 3D molecular graphs from SMILES strings for SchNet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cutoff: float = 10.0):\n",
    "        self.cutoff = cutoff\n",
    "        \n",
    "    def smiles_to_graph(self, smiles: str, target: Optional[float] = None) -> Optional[Data]:\n",
    "        \"\"\"\n",
    "        Convert SMILES to 3D molecular graph.\n",
    "        \n",
    "        Args:\n",
    "            smiles: SMILES string\n",
    "            target: Optional target property value\n",
    "            \n",
    "        Returns:\n",
    "            PyTorch Geometric Data object or None if conversion fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create molecule object\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return None\n",
    "            \n",
    "            # Add hydrogens and generate 3D coordinates\n",
    "            mol = Chem.AddHs(mol)\n",
    "            \n",
    "            # Generate 3D coordinates using ETKDG\n",
    "            params = AllChem.ETKDGv3()\n",
    "            params.randomSeed = 42\n",
    "            \n",
    "            if AllChem.EmbedMolecule(mol, params) == -1:\n",
    "                return None\n",
    "            \n",
    "            # Optimize molecular geometry\n",
    "            AllChem.OptimizeMoleculeConfs(mol, maxIters=1000)\n",
    "            \n",
    "            # Extract atomic information\n",
    "            atomic_numbers = []\n",
    "            positions = []\n",
    "            \n",
    "            conf = mol.GetConformer()\n",
    "            for atom in mol.GetAtoms():\n",
    "                atomic_numbers.append(atom.GetAtomicNum())\n",
    "                pos = conf.GetAtomPosition(atom.GetIdx())\n",
    "                positions.append([pos.x, pos.y, pos.z])\n",
    "            \n",
    "            # Convert to tensors\n",
    "            x = torch.tensor(atomic_numbers, dtype=torch.long)\n",
    "            pos = torch.tensor(positions, dtype=torch.float)\n",
    "            \n",
    "            # Build edge index based on distance cutoff\n",
    "            edge_index = self._build_edge_index(pos, self.cutoff)\n",
    "            \n",
    "            # Create Data object\n",
    "            data = Data(x=x, pos=pos, edge_index=edge_index)\n",
    "            \n",
    "            if target is not None:\n",
    "                data.y = torch.tensor([target], dtype=torch.float)\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to convert SMILES {smiles}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _build_edge_index(self, pos: torch.Tensor, cutoff: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Build edge index based on distance cutoff.\n",
    "        \"\"\"\n",
    "        num_atoms = pos.size(0)\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        distances = torch.cdist(pos, pos)\n",
    "        \n",
    "        # Create adjacency matrix based on cutoff\n",
    "        adj_matrix = (distances <= cutoff) & (distances > 0)  # Exclude self-loops\n",
    "        \n",
    "        # Convert to edge index format\n",
    "        edge_index = adj_matrix.nonzero().t().contiguous()\n",
    "        \n",
    "        return edge_index\n",
    "    \n",
    "    def create_dataset(self, smiles_list: List[str], targets: Optional[List[float]] = None) -> List[Data]:\n",
    "        \"\"\"\n",
    "        Create a dataset of molecular graphs.\n",
    "        \"\"\"\n",
    "        dataset = []\n",
    "        \n",
    "        for i, smiles in enumerate(smiles_list):\n",
    "            target = targets[i] if targets is not None else None\n",
    "            \n",
    "            graph = self.smiles_to_graph(smiles, target)\n",
    "            if graph is not None:\n",
    "                dataset.append(graph)\n",
    "        \n",
    "        logger.info(f\"Successfully created {len(dataset)} molecular graphs from {len(smiles_list)} SMILES\")\n",
    "        return dataset\n",
    "\n",
    "# Initialize graph builder\n",
    "graph_builder = MolecularGraphBuilder(cutoff=8.0)\n",
    "print(\"ðŸŽ¯ Molecular Graph Builder initialized!\")\n",
    "print(\"ðŸ”¬ Ready to convert SMILES to 3D molecular graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde10daf",
   "metadata": {},
   "source": [
    "### **2.3 Create 3D Molecular Graphs for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D molecular graphs from QM9 dataset\n",
    "print(\"Converting QM9 molecules to 3D graphs...\")\n",
    "\n",
    "# Use a subset for faster processing\n",
    "subset_size = 1000\n",
    "subset_data = aligned_qm9_data.head(subset_size)\n",
    "\n",
    "# Create molecular graphs for HOMO energy prediction\n",
    "homo_values = subset_data['homo'].values\n",
    "smiles_subset = subset_data['smiles'].tolist()\n",
    "\n",
    "print(f\"Building 3D graphs for {len(smiles_subset)} molecules...\")\n",
    "start_time = time.time()\n",
    "\n",
    "molecular_graphs = graph_builder.create_dataset(smiles_subset, homo_values.tolist())\n",
    "\n",
    "build_time = time.time() - start_time\n",
    "print(f\"â±ï¸ Graph construction completed in {build_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nðŸ“Š 3D Molecular Graph Dataset:\")\n",
    "print(f\"   â€¢ Total graphs: {len(molecular_graphs)}\")\n",
    "print(f\"   â€¢ Success rate: {len(molecular_graphs)/len(smiles_subset)*100:.1f}%\")\n",
    "\n",
    "# Analyze graph properties\n",
    "if molecular_graphs:\n",
    "    sample_graph = molecular_graphs[0]\n",
    "    print(f\"\\nðŸ” Sample Graph Properties:\")\n",
    "    print(f\"   â€¢ Number of atoms: {sample_graph.x.size(0)}\")\n",
    "    print(f\"   â€¢ Number of edges: {sample_graph.edge_index.size(1)}\")\n",
    "    print(f\"   â€¢ Node features shape: {sample_graph.x.shape}\")\n",
    "    print(f\"   â€¢ Position features shape: {sample_graph.pos.shape}\")\n",
    "    print(f\"   â€¢ Target value: {sample_graph.y.item():.4f}\")\n",
    "\n",
    "# Analyze dataset statistics\n",
    "num_atoms = [graph.x.size(0) for graph in molecular_graphs]\n",
    "num_edges = [graph.edge_index.size(1) for graph in molecular_graphs]\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Dataset Statistics:\")\n",
    "print(f\"   â€¢ Atoms per molecule: {np.mean(num_atoms):.1f} Â± {np.std(num_atoms):.1f}\")\n",
    "print(f\"   â€¢ Edges per molecule: {np.mean(num_edges):.1f} Â± {np.std(num_edges):.1f}\")\n",
    "print(f\"   â€¢ Min/Max atoms: {np.min(num_atoms)}/{np.max(num_atoms)}\")\n",
    "print(f\"   â€¢ Min/Max edges: {np.min(num_edges)}/{np.max(num_edges)}\")\n",
    "\n",
    "print(\"âœ… 3D molecular graphs successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ffd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ **MID-SECTION EXERCISE CHECKPOINT 2.1: 3D Molecular Graph Construction**\n",
    "\n",
    "print(\"ðŸŽ¯ MID-SECTION EXERCISE CHECKPOINT 2.1: 3D Molecular Graph Construction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Quick hands-on exercise: Analyze 3D molecular graphs\n",
    "exercise_widget_2_1 = create_widget(\n",
    "    assessment, \n",
    "    section=\"2.1\",\n",
    "    concepts=[\n",
    "        \"3D molecular graph representation\",\n",
    "        \"Node and edge feature engineering\",\n",
    "        \"Geometric deep learning principles\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Built 3D molecular graphs from SMILES\",\n",
    "        \"Analyzed graph statistics and properties\", \n",
    "        \"Prepared data for SchNet training\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if assessment:\n",
    "    assessment.record_activity(\n",
    "        activity=\"3d_graph_construction\",\n",
    "        result=\"completed\",\n",
    "        metadata={\n",
    "            \"graphs_created\": len(molecular_graphs),\n",
    "            \"success_rate\": len(molecular_graphs)/len(smiles_subset)*100,\n",
    "            \"section\": \"2.1_mid_checkpoint\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "exercise_widget_2_1.display()\n",
    "print(\"ðŸŽ“ Mid-section checkpoint 2.1 completed! Continue with SchNet training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aede98",
   "metadata": {},
   "source": [
    "### **2.4 SchNet Training Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchNetTrainer:\n",
    "    \"\"\"\n",
    "    Training pipeline for SchNet molecular property prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: SchNet, device: str = 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_mae': [],\n",
    "            'val_mae': []\n",
    "        }\n",
    "        \n",
    "    def prepare_data(self, dataset: List[Data], train_ratio: float = 0.8, \n",
    "                    batch_size: int = 32) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Prepare training and validation data loaders.\n",
    "        \"\"\"\n",
    "        # Split dataset\n",
    "        train_size = int(len(dataset) * train_ratio)\n",
    "        train_dataset = dataset[:train_size]\n",
    "        val_dataset = dataset[train_size:]\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader, optimizer: torch.optim.Optimizer, \n",
    "                   criterion: nn.Module) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Train for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_mae = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            predictions = self.model(batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions.squeeze(), batch.y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mae = torch.mean(torch.abs(predictions.squeeze() - batch.y))\n",
    "                total_mae += mae.item()\n",
    "            \n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_mae = total_mae / num_batches\n",
    "        \n",
    "        return avg_loss, avg_mae\n",
    "    \n",
    "    def validate(self, val_loader: DataLoader, criterion: nn.Module) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Validate model on validation set.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_mae = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                \n",
    "                predictions = self.model(batch)\n",
    "                loss = criterion(predictions.squeeze(), batch.y)\n",
    "                mae = torch.mean(torch.abs(predictions.squeeze() - batch.y))\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_mae += mae.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_mae = total_mae / num_batches\n",
    "        \n",
    "        return avg_loss, avg_mae\n",
    "    \n",
    "    def train(self, dataset: List[Data], num_epochs: int = 100, \n",
    "              learning_rate: float = 1e-3, batch_size: int = 32,\n",
    "              patience: int = 10) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Full training pipeline with early stopping.\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ Starting SchNet training...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        train_loader, val_loader = self.prepare_data(dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Setup optimizer and criterion\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Train\n",
    "            train_loss, train_mae = self.train_epoch(train_loader, optimizer, criterion)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_mae = self.validate(val_loader, criterion)\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Store history\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['train_mae'].append(train_mae)\n",
    "            self.training_history['val_mae'].append(val_mae)\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                      f\"Train MAE: {train_mae:.4f} | Val MAE: {val_mae:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model state\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    # Load best model\n",
    "                    self.model.load_state_dict(self.best_model_state)\n",
    "                    break\n",
    "        \n",
    "        print(f\"âœ… Training completed!\")\n",
    "        print(f\"   â€¢ Best validation loss: {best_val_loss:.4f}\")\n",
    "        print(f\"   â€¢ Final validation MAE: {self.training_history['val_mae'][-1]:.4f}\")\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"\n",
    "        Plot training and validation metrics.\n",
    "        \"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=('Loss', 'Mean Absolute Error'),\n",
    "            x_title=\"Epoch\"\n",
    "        )\n",
    "        \n",
    "        epochs = list(range(len(self.training_history['train_loss'])))\n",
    "        \n",
    "        # Loss plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=self.training_history['train_loss'], \n",
    "                      name='Train Loss', line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=self.training_history['val_loss'], \n",
    "                      name='Val Loss', line=dict(color='red')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # MAE plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=self.training_history['train_mae'], \n",
    "                      name='Train MAE', line=dict(color='blue'), showlegend=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=self.training_history['val_mae'], \n",
    "                      name='Val MAE', line=dict(color='red'), showlegend=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"SchNet Training Progress\",\n",
    "            height=400,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Initialize SchNet model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸŽ¯ Using device: {device}\")\n",
    "\n",
    "schnet_model = SchNet(\n",
    "    hidden_channels=64,\n",
    "    num_filters=64,\n",
    "    num_interactions=3,\n",
    "    num_gaussians=25,\n",
    "    cutoff=8.0,\n",
    "    readout='add'\n",
    ")\n",
    "\n",
    "print(f\"ðŸ§  SchNet Model Summary:\")\n",
    "total_params = sum(p.numel() for p in schnet_model.parameters())\n",
    "print(f\"   â€¢ Total parameters: {total_params:,}\")\n",
    "print(f\"   â€¢ Hidden channels: {schnet_model.hidden_channels}\")\n",
    "print(f\"   â€¢ Number of interactions: {schnet_model.num_interactions}\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SchNetTrainer(schnet_model, device)\n",
    "print(\"âœ… SchNet trainer initialized and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SchNet model\n",
    "print(\"ðŸš€ Training SchNet on HOMO energy prediction...\")\n",
    "\n",
    "if len(molecular_graphs) > 0:\n",
    "    # Train the model\n",
    "    training_history = trainer.train(\n",
    "        dataset=molecular_graphs,\n",
    "        num_epochs=50,  # Reduced for demo\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=16,\n",
    "        patience=10\n",
    "    )\n",
    "    \n",
    "    # Plot training progress\n",
    "    training_plot = trainer.plot_training_history()\n",
    "    training_plot.show()\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ SchNet Training Results:\")\n",
    "    print(f\"   â€¢ Final Train Loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"   â€¢ Final Val Loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"   â€¢ Final Train MAE: {training_history['train_mae'][-1]:.4f}\")\n",
    "    print(f\"   â€¢ Final Val MAE: {training_history['val_mae'][-1]:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No molecular graphs available for training\")\n",
    "\n",
    "print(\"\\nâœ… Section 2 Complete: SchNet Implementation & 3D Molecular Understanding\")\n",
    "print(\"ðŸŽ¯ Ready to move to Section 3: Delta Learning Framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4dfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‹ SECTION 2 CHECKPOINT ASSESSMENT: SchNet Implementation & 3D Molecular Understanding\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ SECTION 2 CHECKPOINT ASSESSMENT: SchNet Implementation & 3D Molecular Understanding\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if assessment:\n",
    "    # Record section completion\n",
    "    assessment.record_activity(\n",
    "        \"section_2_completion\", \n",
    "        \"completed\",\n",
    "        {\"section\": \"SchNet Implementation & 3D Molecular Understanding\", \"timestamp\": datetime.now().isoformat()}\n",
    "    )\n",
    "\n",
    "# Create assessment widget for Section 2\n",
    "section2_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 2: SchNet Implementation & 3D Molecular Understanding\",\n",
    "    concepts=[\n",
    "        \"SchNet architecture understanding and implementation\",\n",
    "        \"3D molecular representation and coordinate handling\", \n",
    "        \"Continuous-filter convolutional layers\",\n",
    "        \"Message passing neural networks for molecules\",\n",
    "        \"Radial basis functions and smooth cutoff functions\",\n",
    "        \"3D invariant and equivariant features\",\n",
    "        \"Property prediction with geometric deep learning\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Successfully implemented SchNet architecture\",\n",
    "        \"Built 3D molecular data preprocessing pipeline\",\n",
    "        \"Created continuous-filter convolutional layers\",\n",
    "        \"Implemented message passing framework\",\n",
    "        \"Applied SchNet to QM9 property prediction\",\n",
    "        \"Analyzed 3D molecular geometric features\",\n",
    "        \"Evaluated model performance on quantum properties\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the interactive assessment\n",
    "section2_widget.display()\n",
    "\n",
    "# Progress tracking\n",
    "if assessment:\n",
    "    print(f\"\\nðŸ“Š Current Progress: {assessment.get_progress_summary()['overall_score']:.1f}%\")\n",
    "    print(\"ðŸŽ¯ Section 2 assessment completed - proceed when ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37219d71",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 3: Delta Learning Framework for QM/ML Hybrid Models** âš—ï¸\n",
    "\n",
    "Delta learning combines quantum mechanical calculations with machine learning to achieve chemical accuracy by learning corrections to lower-level quantum methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for delta learning\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "\n",
    "print(\"ðŸŽ¯ Delta Learning Framework Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7613b31",
   "metadata": {},
   "source": [
    "### **3.1 Delta Learning Framework Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumMethodSimulator:\n",
    "    \"\"\"\n",
    "    Simulate different levels of quantum chemical calculations.\n",
    "    \n",
    "    In practice, these would interface with real quantum chemistry codes\n",
    "    like Psi4, Gaussian, or ORCA. Here we simulate the behavior.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.method_accuracies = {\n",
    "            'HF': 0.85,      # Hartree-Fock: fast but less accurate\n",
    "            'DFT': 0.92,     # DFT: good balance of speed and accuracy\n",
    "            'MP2': 0.96,     # MP2: more accurate but slower\n",
    "            'CCSD': 0.98,    # CCSD: high accuracy, very slow\n",
    "            'experiment': 1.0  # Experimental reference\n",
    "        }\n",
    "        \n",
    "        self.method_costs = {\n",
    "            'HF': 1,         # Relative computational cost\n",
    "            'DFT': 3,\n",
    "            'MP2': 15,\n",
    "            'CCSD': 100\n",
    "        }\n",
    "    \n",
    "    def calculate_property(self, smiles: str, method: str = 'DFT', \n",
    "                         property_name: str = 'homo') -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Simulate quantum chemical calculation for a given method.\n",
    "        \"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return {'value': 0.0, 'uncertainty': 1.0, 'cost': 1.0}\n",
    "        \n",
    "        # Base calculation using molecular descriptors\n",
    "        base_value = self._compute_base_property(mol, property_name)\n",
    "        \n",
    "        # Add method-specific corrections and noise\n",
    "        accuracy = self.method_accuracies[method]\n",
    "        cost = self.method_costs[method]\n",
    "        \n",
    "        # Simulate method-specific corrections\n",
    "        if method == 'HF':\n",
    "            # HF typically underestimates correlation effects\n",
    "            correction = -0.1 + 0.02 * np.random.randn()\n",
    "        elif method == 'DFT':\n",
    "            # DFT is generally good but has some systematic errors\n",
    "            correction = 0.05 + 0.01 * np.random.randn()\n",
    "        elif method == 'MP2':\n",
    "            # MP2 overcorrects correlation in some cases\n",
    "            correction = 0.02 + 0.005 * np.random.randn()\n",
    "        elif method == 'CCSD':\n",
    "            # CCSD is very accurate\n",
    "            correction = 0.0 + 0.002 * np.random.randn()\n",
    "        \n",
    "        final_value = base_value + correction\n",
    "        uncertainty = (1.0 - accuracy) * abs(base_value) + 0.001\n",
    "        \n",
    "        return {\n",
    "            'value': final_value,\n",
    "            'uncertainty': uncertainty,\n",
    "            'cost': cost,\n",
    "            'method': method\n",
    "        }\n",
    "    \n",
    "    def _compute_base_property(self, mol: Chem.Mol, property_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute base property value using simple molecular descriptors.\n",
    "        \"\"\"\n",
    "        # Simple correlation based on molecular properties\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        natoms = mol.GetNumAtoms()\n",
    "        aromatic_atoms = sum(1 for atom in mol.GetAtoms() if atom.GetIsAromatic())\n",
    "        \n",
    "        if property_name == 'homo':\n",
    "            # HOMO energy correlation (rough approximation)\n",
    "            base_value = -0.25 - 0.001 * mw + 0.02 * aromatic_atoms / natoms\n",
    "        elif property_name == 'lumo':\n",
    "            # LUMO energy correlation\n",
    "            base_value = -0.05 - 0.0005 * mw - 0.01 * aromatic_atoms / natoms\n",
    "        elif property_name == 'gap':\n",
    "            # HOMO-LUMO gap\n",
    "            homo = self._compute_base_property(mol, 'homo')\n",
    "            lumo = self._compute_base_property(mol, 'lumo')\n",
    "            base_value = lumo - homo\n",
    "        else:\n",
    "            base_value = np.random.normal(0, 0.1)\n",
    "        \n",
    "        return base_value\n",
    "\n",
    "\n",
    "class DeltaLearningModel:\n",
    "    \"\"\"\n",
    "    Delta learning model that learns corrections between different QM levels.\n",
    "    \n",
    "    Î”(molecule) = E_high_level - E_low_level\n",
    "    \n",
    "    The ML model predicts this delta, allowing us to get high-level accuracy\n",
    "    at low-level computational cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, low_level_method: str = 'DFT', high_level_method: str = 'CCSD'):\n",
    "        self.low_level_method = low_level_method\n",
    "        self.high_level_method = high_level_method\n",
    "        self.model = None\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.target_scaler = StandardScaler()\n",
    "        self.qm_simulator = QuantumMethodSimulator()\n",
    "        \n",
    "    def generate_training_data(self, smiles_list: List[str], \n",
    "                             property_name: str = 'homo') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate training data with low-level and high-level calculations.\n",
    "        \"\"\"\n",
    "        print(f\"Generating delta learning data for {len(smiles_list)} molecules...\")\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        for i, smiles in enumerate(smiles_list):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processing molecule {i+1}/{len(smiles_list)}\")\n",
    "            \n",
    "            # Low-level calculation (fast)\n",
    "            low_result = self.qm_simulator.calculate_property(\n",
    "                smiles, self.low_level_method, property_name\n",
    "            )\n",
    "            \n",
    "            # High-level calculation (expensive)\n",
    "            high_result = self.qm_simulator.calculate_property(\n",
    "                smiles, self.high_level_method, property_name\n",
    "            )\n",
    "            \n",
    "            # Compute delta\n",
    "            delta = high_result['value'] - low_result['value']\n",
    "            \n",
    "            training_data.append({\n",
    "                'smiles': smiles,\n",
    "                'low_level': low_result['value'],\n",
    "                'high_level': high_result['value'],\n",
    "                'delta': delta,\n",
    "                'low_uncertainty': low_result['uncertainty'],\n",
    "                'high_uncertainty': high_result['uncertainty'],\n",
    "                'total_cost': low_result['cost'] + high_result['cost']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(training_data)\n",
    "    \n",
    "    def train_delta_model(self, training_data: pd.DataFrame, \n",
    "                         features: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Train the delta learning model.\n",
    "        \"\"\"\n",
    "        print(f\"Training delta model ({self.low_level_method} â†’ {self.high_level_method})...\")\n",
    "        \n",
    "        # Prepare features and targets\n",
    "        X = features\n",
    "        y = training_data['delta'].values\n",
    "        \n",
    "        # Scale features and targets\n",
    "        X_scaled = self.feature_scaler.fit_transform(X)\n",
    "        y_scaled = self.target_scaler.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        self.model = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        y_test_orig = self.target_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "        y_pred_orig = self.target_scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Compute metrics\n",
    "        mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))\n",
    "        r2 = r2_score(y_test_orig, y_pred_orig)\n",
    "        \n",
    "        results = {'mae': mae, 'rmse': rmse, 'r2': r2}\n",
    "        \n",
    "        print(f\"Delta model performance:\")\n",
    "        print(f\"   â€¢ MAE: {mae:.6f}\")\n",
    "        print(f\"   â€¢ RMSE: {rmse:.6f}\")\n",
    "        print(f\"   â€¢ RÂ²: {r2:.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict_high_level(self, smiles: str, features: np.ndarray, \n",
    "                          property_name: str = 'homo') -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Predict high-level property using delta learning.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train_delta_model first.\")\n",
    "        \n",
    "        # Low-level calculation\n",
    "        low_result = self.qm_simulator.calculate_property(\n",
    "            smiles, self.low_level_method, property_name\n",
    "        )\n",
    "        \n",
    "        # Predict delta using ML model\n",
    "        features_scaled = self.feature_scaler.transform(features.reshape(1, -1))\n",
    "        delta_scaled = self.model.predict(features_scaled)[0]\n",
    "        delta = self.target_scaler.inverse_transform([[delta_scaled]])[0][0]\n",
    "        \n",
    "        # Combine for high-level prediction\n",
    "        high_level_pred = low_result['value'] + delta\n",
    "        \n",
    "        # Estimate uncertainty (simplified)\n",
    "        delta_uncertainty = abs(delta) * 0.1  # 10% uncertainty on delta\n",
    "        total_uncertainty = np.sqrt(low_result['uncertainty']**2 + delta_uncertainty**2)\n",
    "        \n",
    "        return {\n",
    "            'low_level': low_result['value'],\n",
    "            'delta': delta,\n",
    "            'high_level_pred': high_level_pred,\n",
    "            'uncertainty': total_uncertainty,\n",
    "            'cost_savings': self.qm_simulator.method_costs[self.high_level_method] - \n",
    "                          self.qm_simulator.method_costs[self.low_level_method]\n",
    "        }\n",
    "\n",
    "\n",
    "class ActiveLearningDelta:\n",
    "    \"\"\"\n",
    "    Active learning for optimal selection of molecules for expensive high-level calculations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, delta_model: DeltaLearningModel):\n",
    "        self.delta_model = delta_model\n",
    "        self.uncertainty_threshold = 0.05\n",
    "        \n",
    "    def select_molecules_for_calculation(self, candidate_smiles: List[str], \n",
    "                                       candidate_features: np.ndarray,\n",
    "                                       n_select: int = 10) -> List[int]:\n",
    "        \"\"\"\n",
    "        Select molecules for high-level calculations using uncertainty sampling.\n",
    "        \"\"\"\n",
    "        if self.delta_model.model is None:\n",
    "            # If no model trained yet, select randomly\n",
    "            return np.random.choice(len(candidate_smiles), n_select, replace=False).tolist()\n",
    "        \n",
    "        uncertainties = []\n",
    "        \n",
    "        for i, (smiles, features) in enumerate(zip(candidate_smiles, candidate_features)):\n",
    "            try:\n",
    "                result = self.delta_model.predict_high_level(smiles, features)\n",
    "                uncertainties.append(result['uncertainty'])\n",
    "            except:\n",
    "                uncertainties.append(1.0)  # High uncertainty for failed predictions\n",
    "        \n",
    "        # Select molecules with highest uncertainty\n",
    "        uncertainty_indices = np.argsort(uncertainties)[-n_select:]\n",
    "        \n",
    "        return uncertainty_indices.tolist()\n",
    "    \n",
    "    def adaptive_training_loop(self, all_smiles: List[str], all_features: np.ndarray,\n",
    "                             property_name: str = 'homo', max_iterations: int = 5,\n",
    "                             molecules_per_iteration: int = 50) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Adaptive training loop that iteratively selects molecules and improves the model.\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ Starting adaptive delta learning...\")\n",
    "        \n",
    "        results = {\n",
    "            'iterations': [],\n",
    "            'model_performance': [],\n",
    "            'total_molecules': 0,\n",
    "            'total_cost': 0\n",
    "        }\n",
    "        \n",
    "        # Start with random selection\n",
    "        current_indices = np.random.choice(\n",
    "            len(all_smiles), molecules_per_iteration, replace=False\n",
    "        ).tolist()\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            print(f\"\\nðŸ“Š Iteration {iteration + 1}/{max_iterations}\")\n",
    "            \n",
    "            # Get current molecules and features\n",
    "            current_smiles = [all_smiles[i] for i in current_indices]\n",
    "            current_features = all_features[current_indices]\n",
    "            \n",
    "            # Generate training data\n",
    "            training_data = self.delta_model.generate_training_data(\n",
    "                current_smiles, property_name\n",
    "            )\n",
    "            \n",
    "            # Train delta model\n",
    "            performance = self.delta_model.train_delta_model(training_data, current_features)\n",
    "            \n",
    "            # Update results\n",
    "            results['iterations'].append(iteration + 1)\n",
    "            results['model_performance'].append(performance)\n",
    "            results['total_molecules'] += len(current_smiles)\n",
    "            results['total_cost'] += training_data['total_cost'].sum()\n",
    "            \n",
    "            print(f\"   â€¢ Molecules processed: {len(current_smiles)}\")\n",
    "            print(f\"   â€¢ Cumulative molecules: {results['total_molecules']}\")\n",
    "            print(f\"   â€¢ Model RÂ²: {performance['r2']:.3f}\")\n",
    "            \n",
    "            # Select next batch (if not last iteration)\n",
    "            if iteration < max_iterations - 1:\n",
    "                remaining_indices = [i for i in range(len(all_smiles)) if i not in current_indices]\n",
    "                remaining_smiles = [all_smiles[i] for i in remaining_indices]\n",
    "                remaining_features = all_features[remaining_indices]\n",
    "                \n",
    "                next_indices = self.select_molecules_for_calculation(\n",
    "                    remaining_smiles, remaining_features, molecules_per_iteration\n",
    "                )\n",
    "                \n",
    "                # Convert back to global indices\n",
    "                next_global_indices = [remaining_indices[i] for i in next_indices]\n",
    "                current_indices.extend(next_global_indices)\n",
    "        \n",
    "        print(f\"\\nâœ… Adaptive learning completed!\")\n",
    "        print(f\"   â€¢ Total molecules: {results['total_molecules']}\")\n",
    "        print(f\"   â€¢ Final RÂ²: {results['model_performance'][-1]['r2']:.3f}\")\n",
    "        \n",
    "        return results\n",
    "print(\"âœ… Delta Learning Framework implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3ed80",
   "metadata": {},
   "source": [
    "### **3.2 Delta Learning Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize delta learning system\n",
    "print(\"ðŸš€ Setting up Delta Learning demonstration...\")\n",
    "\n",
    "# Create delta learning model (DFT â†’ CCSD)\n",
    "delta_model = DeltaLearningModel(low_level_method='DFT', high_level_method='CCSD')\n",
    "\n",
    "# Use subset of molecules for demonstration\n",
    "demo_size = 200\n",
    "demo_indices = np.random.choice(len(aligned_qm9_data), demo_size, replace=False)\n",
    "demo_smiles = [aligned_qm9_data['smiles'].iloc[i] for i in demo_indices]\n",
    "demo_features = scaled_features[demo_indices]\n",
    "\n",
    "print(f\"ðŸ“Š Demo setup:\")\n",
    "print(f\"   â€¢ Molecules: {len(demo_smiles)}\")\n",
    "print(f\"   â€¢ Features: {demo_features.shape}\")\n",
    "\n",
    "# Generate training data for delta learning\n",
    "print(\"\\nGenerating delta learning training data...\")\n",
    "delta_training_data = delta_model.generate_training_data(demo_smiles, 'homo')\n",
    "\n",
    "print(f\"âœ… Training data generated:\")\n",
    "print(f\"   â€¢ Data shape: {delta_training_data.shape}\")\n",
    "display(delta_training_data.head())\n",
    "\n",
    "# Analyze delta statistics\n",
    "print(f\"\\nðŸ“ˆ Delta Statistics:\")\n",
    "print(f\"   â€¢ Mean delta: {delta_training_data['delta'].mean():.6f}\")\n",
    "print(f\"   â€¢ Std delta: {delta_training_data['delta'].std():.6f}\")\n",
    "print(f\"   â€¢ Min delta: {delta_training_data['delta'].min():.6f}\")\n",
    "print(f\"   â€¢ Max delta: {delta_training_data['delta'].max():.6f}\")\n",
    "\n",
    "# Train delta model\n",
    "delta_performance = delta_model.train_delta_model(delta_training_data, demo_features)\n",
    "\n",
    "# Visualize delta learning results\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Low vs High Level Energies', 'Delta Distribution'),\n",
    ")\n",
    "\n",
    "# Scatter plot: Low vs High level\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=delta_training_data['low_level'],\n",
    "        y=delta_training_data['high_level'],\n",
    "        mode='markers',\n",
    "        name='Calculations',\n",
    "        marker=dict(size=4, opacity=0.7, color='blue')\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Perfect correlation line\n",
    "min_val = min(delta_training_data['low_level'].min(), delta_training_data['high_level'].min())\n",
    "max_val = max(delta_training_data['low_level'].max(), delta_training_data['high_level'].max())\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[min_val, max_val],\n",
    "        y=[min_val, max_val],\n",
    "        mode='lines',\n",
    "        line=dict(dash='dash', color='red'),\n",
    "        name='Perfect correlation',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Delta histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=delta_training_data['delta'],\n",
    "        nbinsx=30,\n",
    "        name='Delta values',\n",
    "        showlegend=False,\n",
    "        marker_color='green'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"DFT Energy\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"CCSD Energy\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Delta (CCSD - DFT)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Delta Learning Analysis\",\n",
    "    height=400,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"âœ… Delta learning model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ **MID-SECTION EXERCISE CHECKPOINT 3.1: Delta Learning Implementation**\n",
    "\n",
    "print(\"ðŸŽ¯ MID-SECTION EXERCISE CHECKPOINT 3.1: Delta Learning Implementation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Quick hands-on exercise: Analyze delta learning performance\n",
    "exercise_widget_3_1 = create_widget(\n",
    "    assessment, \n",
    "    section=\"3.1\",\n",
    "    concepts=[\n",
    "        \"Delta learning methodology and principles\",\n",
    "        \"QM/ML hybrid model architecture\",\n",
    "        \"Cost-accuracy trade-off optimization\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Implemented quantum method simulator\",\n",
    "        \"Built delta learning correction model\", \n",
    "        \"Validated cost-accuracy improvements\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if assessment:\n",
    "    assessment.record_activity(\n",
    "        activity=\"delta_learning_implementation\",\n",
    "        result=\"completed\",\n",
    "        metadata={\n",
    "            \"model_performance\": \"implemented\",\n",
    "            \"qm_simulator\": \"functional\",\n",
    "            \"section\": \"3.1_mid_checkpoint\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "exercise_widget_3_1.display()\n",
    "print(\"ðŸŽ“ Mid-section checkpoint 3.1 completed! Continue with active learning...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7aa01",
   "metadata": {},
   "source": [
    "### **3.3 Active Learning Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize active learning system\n",
    "print(\"ðŸŽ¯ Setting up Active Learning demonstration...\")\n",
    "\n",
    "active_learner = ActiveLearningDelta(delta_model)\n",
    "\n",
    "# Run adaptive training loop\n",
    "print(\"Starting adaptive training loop...\")\n",
    "adaptive_results = active_learner.adaptive_training_loop(\n",
    "    all_smiles=demo_smiles,\n",
    "    all_features=demo_features,\n",
    "    property_name='homo',\n",
    "    max_iterations: int = 3,  # Reduced for demo\n",
    "    molecules_per_iteration: int = 30\n",
    ")\n",
    "\n",
    "# Visualize active learning progress\n",
    "iterations = adaptive_results['iterations']\n",
    "r2_scores = [perf['r2'] for perf in adaptive_results['model_performance']]\n",
    "mae_scores = [perf['mae'] for perf in adaptive_results['model_performance']]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Model Accuracy (RÂ²)', 'Prediction Error (MAE)'),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=iterations,\n",
    "        y=r2_scores,\n",
    "        mode='lines+markers',\n",
    "        name='RÂ² Score',\n",
    "        line=dict(color='blue', width=3),\n",
    "        marker=dict(size=8)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=iterations,\n",
    "        y=mae_scores,\n",
    "        mode='lines+markers',\n",
    "        name='MAE',\n",
    "        line=dict(color='red', width=3),\n",
    "        marker=dict(size=8)\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Iteration\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"RÂ² Score\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Iteration\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"MAE\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Active Learning Progress\",\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Active Learning Results:\")\n",
    "print(f\"   â€¢ Initial RÂ²: {r2_scores[0]:.3f}\")\n",
    "print(f\"   â€¢ Final RÂ²: {r2_scores[-1]:.3f}\")\n",
    "print(f\"   â€¢ Improvement: {r2_scores[-1] - r2_scores[0]:.3f}\")\n",
    "print(f\"   â€¢ Total molecules: {adaptive_results['total_molecules']}\")\n",
    "print(f\"   â€¢ Total cost: {adaptive_results['total_cost']:.0f}\")\n",
    "\n",
    "# Demonstrate cost savings\n",
    "print(f\"\\nðŸ’° Cost Analysis:\")\n",
    "traditional_cost = len(demo_smiles) * delta_model.qm_simulator.method_costs['CCSD']\n",
    "delta_cost = adaptive_results['total_cost']\n",
    "savings = traditional_cost - delta_cost\n",
    "savings_percent = (savings / traditional_cost) * 100\n",
    "\n",
    "print(f\"   â€¢ Traditional approach cost: {traditional_cost:.0f}\")\n",
    "print(f\"   â€¢ Delta learning cost: {delta_cost:.0f}\")\n",
    "print(f\"   â€¢ Cost savings: {savings:.0f} ({savings_percent:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Section 3 Complete: Delta Learning Framework\")\n",
    "print(\"ðŸŽ¯ Ready to move to Section 4: Advanced Quantum ML Architectures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4efb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‹ SECTION 3 CHECKPOINT ASSESSMENT: Delta Learning Framework for QM/ML Hybrid Models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ SECTION 3 CHECKPOINT ASSESSMENT: Delta Learning Framework for QM/ML Hybrid Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if assessment:\n",
    "    # Record section completion\n",
    "    assessment.record_activity(\n",
    "        \"section_3_completion\", \n",
    "        \"completed\",\n",
    "        {\"section\": \"Delta Learning Framework for QM/ML Hybrid Models\", \"timestamp\": datetime.now().isoformat()}\n",
    "    )\n",
    "\n",
    "# Create assessment widget for Section 3\n",
    "section3_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 3: Delta Learning Framework for QM/ML Hybrid Models\",\n",
    "    concepts=[\n",
    "        \"Delta learning methodology and theoretical framework\",\n",
    "        \"Multi-level quantum chemistry method integration\", \n",
    "        \"Hybrid QM/ML model architectures\",\n",
    "        \"Uncertainty quantification in quantum ML\",\n",
    "        \"Cost-accuracy trade-offs in quantum calculations\",\n",
    "        \"Error correction and systematic bias handling\",\n",
    "        \"Production deployment of delta learning systems\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Implemented quantum method simulator framework\",\n",
    "        \"Built delta learning correction models\",\n",
    "        \"Created multi-level QM/ML hybrid pipeline\",\n",
    "        \"Developed uncertainty quantification metrics\",\n",
    "        \"Optimized cost-accuracy trade-offs\",\n",
    "        \"Validated delta learning performance\",\n",
    "        \"Deployed production-ready delta learning system\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the interactive assessment\n",
    "section3_widget.display()\n",
    "\n",
    "# Progress tracking\n",
    "if assessment:\n",
    "    print(f\"\\nðŸ“Š Current Progress: {assessment.get_progress_summary()['overall_score']:.1f}%\")\n",
    "    print(\"ðŸŽ¯ Section 3 assessment completed - proceed when ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d67eeb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 4: Advanced Quantum ML Architectures** ðŸ§ \n",
    "\n",
    "Explore cutting-edge architectures that combine quantum mechanical insights with deep learning innovations, including attention mechanisms and transformer architectures for molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb78507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for advanced architectures\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention, TransformerEncoder, TransformerEncoderLayer\n",
    "from typing import Optional, Callable\n",
    "import math\n",
    "\n",
    "print(\"ðŸš€ Advanced Quantum ML Architectures Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18eea2e",
   "metadata": {},
   "source": [
    "### **4.1 Quantum-Aware Attention Mechanisms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumAwareAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum-aware attention mechanism that incorporates physical principles\n",
    "    into the attention computation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int = 8, \n",
    "                 quantum_features: bool = True, dropout: float = 0.1):\n",
    "        super(QuantumAwareAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.quantum_features = quantum_features\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Standard attention projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Quantum-aware components\n",
    "        if quantum_features:\n",
    "            # Distance-based attention weights\n",
    "            self.distance_embedding = nn.Sequential(\n",
    "                nn.Linear(1, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, num_heads),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            \n",
    "            # Orbital overlap modeling\n",
    "            self.orbital_interaction = nn.Sequential(\n",
    "                nn.Linear(d_model * 2, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, num_heads),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, edge_index: Optional[torch.Tensor] = None,\n",
    "                distances: Optional[torch.Tensor] = None, \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with quantum-aware attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [batch_size, seq_len, d_model]\n",
    "            edge_index: Graph connectivity [2, num_edges]\n",
    "            distances: Pairwise distances [num_edges] or [batch_size, seq_len, seq_len]\n",
    "            mask: Attention mask [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute standard attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Add quantum-aware modifications\n",
    "        if self.quantum_features and distances is not None:\n",
    "            # Distance-based attention modification\n",
    "            if len(distances.shape) == 1:\n",
    "                # Convert edge distances to attention matrix\n",
    "                distance_matrix = self._edge_to_matrix(edge_index, distances, seq_len)\n",
    "            else:\n",
    "                distance_matrix = distances\n",
    "            \n",
    "            distance_weights = self.distance_embedding(distance_matrix.unsqueeze(-1))\n",
    "            distance_weights = distance_weights.permute(0, 3, 1, 2)  # [batch, heads, seq, seq]\n",
    "            \n",
    "            # Apply distance-based modulation\n",
    "            scores = scores * distance_weights\n",
    "            \n",
    "            # Orbital interaction terms\n",
    "            x_pairs = self._create_pairwise_features(x)  # [batch, seq, seq, 2*d_model]\n",
    "            orbital_weights = self.orbital_interaction(x_pairs)\n",
    "            orbital_weights = orbital_weights.permute(0, 3, 1, 2)  # [batch, heads, seq, seq]\n",
    "            \n",
    "            scores = scores + orbital_weights\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(1).unsqueeze(1) == 0, -1e9)\n",
    "        \n",
    "        # Softmax attention\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_weights, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "    def _edge_to_matrix(self, edge_index: torch.Tensor, edge_distances: torch.Tensor, \n",
    "                       seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"Convert edge list to distance matrix.\"\"\"\n",
    "        batch_size = 1  # Simplified for single batch\n",
    "        distance_matrix = torch.zeros(batch_size, seq_len, seq_len, device=edge_distances.device)\n",
    "        \n",
    "        if edge_index is not None:\n",
    "            row, col = edge_index\n",
    "            distance_matrix[0, row, col] = edge_distances\n",
    "            distance_matrix[0, col, row] = edge_distances  # Symmetric\n",
    "        \n",
    "        return distance_matrix\n",
    "    \n",
    "    def _create_pairwise_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create pairwise features for orbital interaction modeling.\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Expand features for all pairs\n",
    "        x_i = x.unsqueeze(2).expand(-1, -1, seq_len, -1)  # [batch, seq, seq, d_model]\n",
    "        x_j = x.unsqueeze(1).expand(-1, seq_len, -1, -1)  # [batch, seq, seq, d_model]\n",
    "        \n",
    "        # Concatenate pairwise features\n",
    "        pairwise_features = torch.cat([x_i, x_j], dim=-1)  # [batch, seq, seq, 2*d_model]\n",
    "        \n",
    "        return pairwise_features\n",
    "\n",
    "\n",
    "class MolecularTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer architecture specifically designed for molecular property prediction\n",
    "    with quantum-aware attention mechanisms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 256, num_heads: int = 8, \n",
    "                 num_layers: int = 6, max_atoms: int = 100,\n",
    "                 num_atom_types: int = 100, dropout: float = 0.1):\n",
    "        super(MolecularTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_atoms = max_atoms\n",
    "        \n",
    "        # Atom type embedding\n",
    "        self.atom_embedding = nn.Embedding(num_atom_types, d_model)\n",
    "        \n",
    "        # Positional encoding (3D-aware)\n",
    "        self.position_embedding = nn.Linear(3, d_model)\n",
    "        \n",
    "        # Quantum-aware transformer layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            self._make_layer(d_model, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output head for property prediction\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def _make_layer(self, d_model: int, num_heads: int, dropout: float) -> nn.Module:\n",
    "        \"\"\"Create a single transformer layer with quantum-aware attention.\"\"\"\n",
    "        return nn.ModuleDict({\n",
    "            'attention': QuantumAwareAttention(d_model, num_heads, quantum_features=True, dropout=dropout),\n",
    "            'norm1': nn.LayerNorm(d_model),\n",
    "            'ffn': nn.Sequential(\n",
    "                nn.Linear(d_model, d_model * 4),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model * 4, d_model),\n",
    "                nn.Dropout(dropout)\n",
    "            ),\n",
    "            'norm2': nn.LayerNorm(d_model)\n",
    "        })\n",
    "    \n",
    "    def forward(self, atom_types: torch.Tensor, positions: torch.Tensor,\n",
    "                edge_index: Optional[torch.Tensor] = None,\n",
    "                batch_idx: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of molecular transformer.\n",
    "        \n",
    "        Args:\n",
    "            atom_types: Atomic numbers [batch_size, max_atoms]\n",
    "            positions: 3D coordinates [batch_size, max_atoms, 3]\n",
    "            edge_index: Graph connectivity [2, num_edges]\n",
    "            batch_idx: Batch assignment for graph data\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = atom_types.shape\n",
    "        \n",
    "        # Create attention mask for padding\n",
    "        mask = (atom_types != 0).float()  # Assume 0 is padding\n",
    "        \n",
    "        # Embed atoms and positions\n",
    "        atom_embed = self.atom_embedding(atom_types)\n",
    "        pos_embed = self.position_embedding(positions)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = atom_embed + pos_embed\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        distances = self._compute_distances(positions, mask)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            # Self-attention with residual connection\n",
    "            attn_out = layer['attention'](x, edge_index, distances, mask.unsqueeze(-1))\n",
    "            x = layer['norm1'](x + attn_out)\n",
    "            \n",
    "            # Feed-forward with residual connection\n",
    "            ffn_out = layer['ffn'](x)\n",
    "            x = layer['norm2'](x + ffn_out)\n",
    "        \n",
    "        # Global pooling (mean over valid atoms)\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(x)\n",
    "        x_masked = x * mask_expanded\n",
    "        global_features = x_masked.sum(dim=1) / mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "        \n",
    "        # Predict property\n",
    "        return self.output_head(global_features)\n",
    "    \n",
    "    def _compute_distances(self, positions: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute pairwise distances between atoms.\"\"\"\n",
    "        batch_size, seq_len, _ = positions.shape\n",
    "        \n",
    "        # Expand positions for pairwise computation\n",
    "        pos_i = positions.unsqueeze(2).expand(-1, -1, seq_len, -1)\n",
    "        pos_j = positions.unsqueeze(1).expand(-1, seq_len, -1, -1)\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = torch.norm(pos_i - pos_j, dim=-1)\n",
    "        \n",
    "        # Apply mask to ignore padding atoms\n",
    "        mask_matrix = mask.unsqueeze(-1) * mask.unsqueeze(1)\n",
    "        distances = distances * mask_matrix\n",
    "        \n",
    "        return distances\n",
    "\n",
    "print(\"âœ… Quantum-Aware Attention and Molecular Transformer implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814269b0",
   "metadata": {},
   "source": [
    "### **4.2 Multi-Task Quantum Property Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b183566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskQuantumPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task learning model for simultaneous prediction of multiple quantum properties.\n",
    "    \n",
    "    This model leverages shared representations to improve prediction of correlated\n",
    "    quantum mechanical properties like HOMO, LUMO, gap, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [512, 256, 128],\n",
    "                 property_dims: Dict[str, int] = None, dropout: float = 0.1):\n",
    "        super(MultiTaskQuantumPredictor, self).__init__()\n",
    "        \n",
    "        if property_dims is None:\n",
    "            property_dims = {\n",
    "                'homo': 1, 'lumo': 1, 'gap': 1, 'mu': 1, \n",
    "                'alpha': 1, 'cv': 1, 'zpve': 1, 'u0': 1\n",
    "            }\n",
    "        \n",
    "        self.property_names = list(property_dims.keys())\n",
    "        self.num_properties = len(self.property_names)\n",
    "        \n",
    "        # Shared encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.shared_encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Property-specific heads\n",
    "        self.property_heads = nn.ModuleDict()\n",
    "        for prop_name, output_dim in property_dims.items():\n",
    "            self.property_heads[prop_name] = nn.Sequential(\n",
    "                nn.Linear(prev_dim, prev_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(prev_dim // 2, output_dim)\n",
    "            )\n",
    "        \n",
    "        # Uncertainty estimation heads\n",
    "        self.uncertainty_heads = nn.ModuleDict()\n",
    "        for prop_name in property_dims.keys():\n",
    "            self.uncertainty_heads[prop_name] = nn.Sequential(\n",
    "                nn.Linear(prev_dim, prev_dim // 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(prev_dim // 4, 1),\n",
    "                nn.Softplus()  # Ensure positive uncertainty\n",
    "            )\n",
    "        \n",
    "        # Cross-property correlation learning\n",
    "        self.correlation_matrix = nn.Parameter(\n",
    "            torch.eye(self.num_properties) * 0.1 + torch.randn(self.num_properties, self.num_properties) * 0.01\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass with multi-task prediction.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with predictions and uncertainties for each property\n",
    "        \"\"\"\n",
    "        # Shared feature extraction\n",
    "        shared_features = self.shared_encoder(x)\n",
    "        \n",
    "        # Property-specific predictions\n",
    "        predictions = {}\n",
    "        uncertainties = {}\n",
    "        \n",
    "        for prop_name in self.property_names:\n",
    "            predictions[prop_name] = self.property_heads[prop_name](shared_features)\n",
    "            uncertainties[prop_name] = self.uncertainty_heads[prop_name](shared_features)\n",
    "        \n",
    "        # Apply correlation constraints\n",
    "        pred_tensor = torch.stack([predictions[prop] for prop in self.property_names], dim=-1)\n",
    "        corr_adjusted = torch.matmul(pred_tensor, self.correlation_matrix)\n",
    "        \n",
    "        # Update predictions with correlations\n",
    "        for i, prop_name in enumerate(self.property_names):\n",
    "            predictions[prop_name] = corr_adjusted[..., i:i+1]\n",
    "        \n",
    "        return {\n",
    "            'predictions': predictions,\n",
    "            'uncertainties': uncertainties,\n",
    "            'correlations': self.correlation_matrix\n",
    "        }\n",
    "    \n",
    "    def compute_multi_task_loss(self, outputs: Dict[str, torch.Tensor], \n",
    "                               targets: Dict[str, torch.Tensor],\n",
    "                               weights: Optional[Dict[str, float]] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute multi-task loss with uncertainty weighting.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = {prop: 1.0 for prop in self.property_names}\n",
    "        \n",
    "        total_loss = 0\n",
    "        predictions = outputs['predictions']\n",
    "        uncertainties = outputs['uncertainties']\n",
    "        \n",
    "        for prop_name in self.property_names:\n",
    "            if prop_name in targets:\n",
    "                pred = predictions[prop_name].squeeze()\n",
    "                target = targets[prop_name]\n",
    "                uncertainty = uncertainties[prop_name].squeeze()\n",
    "                \n",
    "                # Uncertainty-weighted loss (heteroscedastic)\n",
    "                mse_loss = F.mse_loss(pred, target, reduction='none')\n",
    "                weighted_loss = mse_loss / (2 * uncertainty**2) + 0.5 * torch.log(uncertainty**2)\n",
    "                \n",
    "                total_loss += weights[prop_name] * weighted_loss.mean()\n",
    "        \n",
    "        # Add correlation regularization\n",
    "        corr_reg = torch.norm(self.correlation_matrix - torch.eye(self.num_properties, device=self.correlation_matrix.device))\n",
    "        total_loss += 0.01 * corr_reg\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "class EnsembleQuantumPredictor:\n",
    "    \"\"\"\n",
    "    Ensemble of quantum property predictors for improved uncertainty quantification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_class, model_configs: List[Dict], num_models: int = 5):\n",
    "        self.models = []\n",
    "        self.num_models = num_models\n",
    "        \n",
    "        # Create ensemble of models with different configurations\n",
    "        for i in range(num_models):\n",
    "            config = model_configs[i % len(model_configs)]\n",
    "            model = base_model_class(**config)\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def train_ensemble(self, train_loader: DataLoader, val_loader: DataLoader,\n",
    "                      num_epochs: int = 100, device: str = 'cpu') -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Train ensemble of models with different random initializations.\n",
    "        \"\"\"\n",
    "        ensemble_history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            print(f\"Training model {i+1}/{self.num_models}...\")\n",
    "            \n",
    "            model = model.to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "            \n",
    "            model_history = {'train_loss': [], 'val_loss': []}\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                # Training\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                for batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Prepare batch data\n",
    "                    features = batch['features'].to(device)\n",
    "                    targets = {k: v.to(device) for k, v in batch['targets'].items()}\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(features)\n",
    "                    loss = model.compute_multi_task_loss(outputs, targets)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                \n",
    "                # Validation\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        features = batch['features'].to(device)\n",
    "                        targets = {k: v.to(device) for k, v in batch['targets'].items()}\n",
    "                        \n",
    "                        outputs = model(features)\n",
    "                        loss = model.compute_multi_task_loss(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                \n",
    "                model_history['train_loss'].append(train_loss / len(train_loader))\n",
    "                model_history['val_loss'].append(val_loss / len(val_loader))\n",
    "                \n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"   Epoch {epoch}: Train Loss: {model_history['train_loss'][-1]:.4f}, \"\n",
    "                          f\"Val Loss: {model_history['val_loss'][-1]:.4f}\")\n",
    "            \n",
    "            ensemble_history['train_loss'].append(model_history['train_loss'])\n",
    "            ensemble_history['val_loss'].append(model_history['val_loss'])\n",
    "        \n",
    "        return ensemble_history\n",
    "    \n",
    "    def predict_with_uncertainty(self, x: torch.Tensor, device: str = 'cpu') -> Dict[str, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Make predictions with ensemble uncertainty quantification.\n",
    "        \"\"\"\n",
    "        all_predictions = {prop: [] for prop in self.models[0].property_names}\n",
    "        all_uncertainties = {prop: [] for prop in self.models[0].property_names}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model in self.models:\n",
    "                model.eval()\n",
    "                model = model.to(device)\n",
    "                x = x.to(device)\n",
    "                \n",
    "                outputs = model(x)\n",
    "                \n",
    "                for prop in self.models[0].property_names:\n",
    "                    all_predictions[prop].append(outputs['predictions'][prop])\n",
    "                    all_uncertainties[prop].append(outputs['uncertainties'][prop])\n",
    "        \n",
    "        # Compute ensemble statistics\n",
    "        ensemble_results = {}\n",
    "        \n",
    "        for prop in self.models[0].property_names:\n",
    "            pred_stack = torch.stack(all_predictions[prop], dim=0)\n",
    "            uncert_stack = torch.stack(all_uncertainties[prop], dim=0)\n",
    "            \n",
    "            # Ensemble mean and variance\n",
    "            ensemble_mean = torch.mean(pred_stack, dim=0)\n",
    "            ensemble_var = torch.var(pred_stack, dim=0)\n",
    "            \n",
    "            # Total uncertainty (epistemic + aleatoric)\n",
    "            aleatoric_uncertainty = torch.mean(uncert_stack**2, dim=0)\n",
    "            epistemic_uncertainty = ensemble_var\n",
    "            total_uncertainty = torch.sqrt(aleatoric_uncertainty + epistemic_uncertainty)\n",
    "            \n",
    "            ensemble_results[prop] = {\n",
    "                'mean': ensemble_mean,\n",
    "                'total_uncertainty': total_uncertainty,\n",
    "                'epistemic_uncertainty': torch.sqrt(epistemic_uncertainty),\n",
    "                'aleatoric_uncertainty': torch.sqrt(aleatoric_uncertainty)\n",
    "            }\n",
    "        \n",
    "        return ensemble_results\n",
    "\n",
    "print(\"âœ… Multi-Task and Ensemble Learning frameworks implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659af43",
   "metadata": {},
   "source": [
    "### **4.3 Advanced Architecture Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e881f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multi-task learning\n",
    "print(\"ðŸš€ Setting up Multi-Task Quantum Property Prediction...\")\n",
    "\n",
    "# Prepare multi-task dataset\n",
    "available_props = ['homo', 'lumo', 'gap', 'mu', 'alpha', 'cv']\n",
    "existing_props = [prop for prop in available_props if prop in aligned_qm9_data.columns]\n",
    "\n",
    "print(f\"Available properties for multi-task learning: {existing_props}\")\n",
    "\n",
    "# Create multi-task model\n",
    "input_dim = scaled_features.shape[1]\n",
    "property_dims = {prop: 1 for prop in existing_props}\n",
    "\n",
    "multi_task_model = MultiTaskQuantumPredictor(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=[256, 128, 64],\n",
    "    property_dims=property_dims,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"ðŸ§  Multi-Task Model Summary:\")\n",
    "total_params = sum(p.numel() for p in multi_task_model.parameters())\n",
    "print(f\"   â€¢ Total parameters: {total_params:,}\")\n",
    "print(f\"   â€¢ Properties: {len(existing_props)}\")\n",
    "print(f\"   â€¢ Shared encoder layers: 3\")\n",
    "\n",
    "# Prepare data for multi-task learning\n",
    "demo_size = 500\n",
    "demo_indices = np.random.choice(len(aligned_qm9_data), demo_size, replace=False)\n",
    "\n",
    "# Features and targets\n",
    "X_demo = scaled_features[demo_indices]\n",
    "targets_demo = {}\n",
    "for prop in existing_props:\n",
    "    targets_demo[prop] = aligned_qm9_data[prop].iloc[demo_indices].values\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.FloatTensor(X_demo)\n",
    "target_tensors = {prop: torch.FloatTensor(targets) for prop, targets in targets_demo.items()}\n",
    "\n",
    "print(f\"\\nðŸ“Š Demo Dataset:\")\n",
    "print(f\"   â€¢ Samples: {X_tensor.shape[0]}\")\n",
    "print(f\"   â€¢ Features: {X_tensor.shape[1]}\")\n",
    "print(f\"   â€¢ Properties: {len(target_tensors)}\")\n",
    "\n",
    "# Single forward pass demonstration\n",
    "print(\"\\nTesting multi-task model...\")\n",
    "with torch.no_grad():\n",
    "    sample_batch = X_tensor[:10]  # Small batch\n",
    "    outputs = multi_task_model(sample_batch)\n",
    "    \n",
    "    print(f\"\\nðŸ” Sample Predictions:\")\n",
    "    for prop in existing_props[:3]:  # Show first 3 properties\n",
    "        pred = outputs['predictions'][prop]\n",
    "        uncert = outputs['uncertainties'][prop]\n",
    "        print(f\"   â€¢ {prop}: {pred[0].item():.4f} Â± {uncert[0].item():.4f}\")\n",
    "\n",
    "# Analyze correlation matrix\n",
    "correlation_matrix = multi_task_model.correlation_matrix.detach().numpy()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix,\n",
    "    x=existing_props,\n",
    "    y=existing_props,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0,\n",
    "    text=np.round(correlation_matrix, 3),\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 10},\n",
    "    showscale=True\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Learned Property Correlations\",\n",
    "    xaxis_title=\"Properties\",\n",
    "    yaxis_title=\"Properties\",\n",
    "    height=500,\n",
    "    width=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"âœ… Multi-task model demonstration completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d85552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ **MID-SECTION EXERCISE CHECKPOINT 4.1: Advanced Architecture Implementation**\n",
    "\n",
    "print(\"ðŸŽ¯ MID-SECTION EXERCISE CHECKPOINT 4.1: Advanced Architecture Implementation\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "# Quick hands-on exercise: Analyze advanced architectures\n",
    "exercise_widget_4_1 = create_widget(\n",
    "    assessment, \n",
    "    section=\"4.1\",\n",
    "    concepts=[\n",
    "        \"Multi-task learning for quantum properties\",\n",
    "        \"Ensemble uncertainty quantification\",\n",
    "        \"Property correlation modeling\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Implemented multi-task quantum predictor\",\n",
    "        \"Built ensemble learning framework\", \n",
    "        \"Analyzed property correlations and uncertainties\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if assessment:\n",
    "    assessment.record_activity(\n",
    "        activity=\"advanced_architectures_implementation\",\n",
    "        result=\"completed\",\n",
    "        metadata={\n",
    "            \"multi_task_model\": \"implemented\",\n",
    "            \"ensemble_framework\": \"built\",\n",
    "            \"section\": \"4.1_mid_checkpoint\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "exercise_widget_4_1.display()\n",
    "print(\"ðŸŽ“ Mid-section checkpoint 4.1 completed! Continue with molecular transformers...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca7581",
   "metadata": {},
   "source": [
    "### **4.4 Molecular Transformer Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Molecular Transformer\n",
    "print(\"ðŸš€ Setting up Molecular Transformer demonstration...\")\n",
    "\n",
    "# Create synthetic molecular data for transformer\n",
    "def create_transformer_batch(smiles_list: List[str], max_atoms: int = 50) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Create a batch of molecular data for transformer.\"\"\"\n",
    "    batch_data = {\n",
    "        'atom_types': [],\n",
    "        'positions': [],\n",
    "        'targets': []\n",
    "    }\n",
    "    \n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        \n",
    "        # Add hydrogens and generate 3D coordinates\n",
    "        mol = Chem.AddHs(mol)\n",
    "        \n",
    "        # Try to embed molecule\n",
    "        try:\n",
    "            AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "            AllChem.OptimizeMoleculeConfs(mol)\n",
    "            \n",
    "            # Extract atomic information\n",
    "            atom_types = []\n",
    "            positions = []\n",
    "            \n",
    "            conf = mol.GetConformer()\n",
    "            for atom in mol.GetAtoms():\n",
    "                atom_types.append(atom.GetAtomicNum())\n",
    "                pos = conf.GetAtomPosition(atom.GetIdx())\n",
    "                positions.append([pos.x, pos.y, pos.z])\n",
    "            \n",
    "            # Pad or truncate to max_atoms\n",
    "            while len(atom_types) < max_atoms:\n",
    "                atom_types.append(0)  # Padding token\n",
    "                positions.append([0.0, 0.0, 0.0])\n",
    "            \n",
    "            if len(atom_types) > max_atoms:\n",
    "                atom_types = atom_types[:max_atoms]\n",
    "                positions = positions[:max_atoms]\n",
    "            \n",
    "            batch_data['atom_types'].append(atom_types)\n",
    "            batch_data['positions'].append(positions)\n",
    "            batch_data['targets'].append(np.random.normal(0, 0.1))  # Dummy target\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Convert to tensors\n",
    "    if batch_data['atom_types']:\n",
    "        return {\n",
    "            'atom_types': torch.LongTensor(batch_data['atom_types']),\n",
    "            'positions': torch.FloatTensor(batch_data['positions']),\n",
    "            'targets': torch.FloatTensor(batch_data['targets'])\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create molecular transformer\n",
    "transformer_model = MolecularTransformer(\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    max_atoms=30,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"ðŸ§  Molecular Transformer Summary:\")\n",
    "transformer_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "print(f\"   â€¢ Total parameters: {transformer_params:,}\")\n",
    "print(f\"   â€¢ Model dimension: {transformer_model.d_model}\")\n",
    "print(f\"   â€¢ Number of layers: 4\")\n",
    "print(f\"   â€¢ Number of heads: 8\")\n",
    "\n",
    "# Create demo batch\n",
    "demo_smiles = aligned_qm9_data['smiles'].head(20).tolist()\n",
    "transformer_batch = create_transformer_batch(demo_smiles, max_atoms=25)\n",
    "\n",
    "if transformer_batch is not None:\n",
    "    print(f\"\\nðŸ“Š Transformer Batch:\")\n",
    "    print(f\"   â€¢ Batch size: {transformer_batch['atom_types'].shape[0]}\")\n",
    "    print(f\"   â€¢ Max atoms: {transformer_batch['atom_types'].shape[1]}\")\n",
    "    print(f\"   â€¢ Position dims: {transformer_batch['positions'].shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    print(\"\\nTesting molecular transformer...\")\n",
    "    with torch.no_grad():\n",
    "        transformer_model.eval()\n",
    "        predictions = transformer_model(\n",
    "            transformer_batch['atom_types'],\n",
    "            transformer_batch['positions']\n",
    "        )\n",
    "        \n",
    "        print(f\"   â€¢ Output shape: {predictions.shape}\")\n",
    "        print(f\"   â€¢ Sample predictions: {predictions[:3].squeeze().tolist()}\")\n",
    "    \n",
    "    print(\"âœ… Molecular transformer demonstration completed!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Could not create transformer batch - skipping demonstration\")\n",
    "\n",
    "print(\"\\nâœ… Section 4 Complete: Advanced Quantum ML Architectures\")\n",
    "print(\"ðŸŽ¯ Ready to move to Section 5: Production Pipeline & Integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‹ SECTION 4 CHECKPOINT ASSESSMENT: Advanced Quantum ML Architectures\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ SECTION 4 CHECKPOINT ASSESSMENT: Advanced Quantum ML Architectures\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if assessment:\n",
    "    # Record section completion\n",
    "    assessment.record_activity(\n",
    "        \"section_4_completion\", \n",
    "        \"completed\",\n",
    "        {\"section\": \"Advanced Quantum ML Architectures\", \"timestamp\": datetime.now().isoformat()}\n",
    "    )\n",
    "\n",
    "# Create assessment widget for Section 4\n",
    "section4_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 4: Advanced Quantum ML Architectures\",\n",
    "    concepts=[\n",
    "        \"Quantum attention mechanisms and self-attention\",\n",
    "        \"Quantum transformer architectures for molecules\", \n",
    "        \"Variational quantum neural networks (VQNNs)\",\n",
    "        \"Quantum graph neural networks (QGNNs)\",\n",
    "        \"Hybrid classical-quantum architectures\",\n",
    "        \"Quantum advantage in molecular modeling\",\n",
    "        \"Scalability and noise considerations in quantum ML\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Implemented quantum attention mechanisms\",\n",
    "        \"Built quantum transformer for molecular sequences\",\n",
    "        \"Created variational quantum neural networks\",\n",
    "        \"Developed quantum graph neural networks\",\n",
    "        \"Designed hybrid classical-quantum models\",\n",
    "        \"Analyzed quantum advantage in specific tasks\",\n",
    "        \"Evaluated noise resilience and scalability\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the interactive assessment\n",
    "section4_widget.display()\n",
    "\n",
    "# Progress tracking\n",
    "if assessment:\n",
    "    print(f\"\\nðŸ“Š Current Progress: {assessment.get_progress_summary()['overall_score']:.1f}%\")\n",
    "    print(\"ðŸŽ¯ Section 4 assessment completed - proceed when ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c86b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 5: Production Pipeline & Integration Toolkit** ðŸš€\n",
    "\n",
    "Build a complete production-ready toolkit that integrates all quantum ML components into a unified pipeline for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b300d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for production pipeline\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from dataclasses import dataclass, asdict\n",
    "import yaml\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "from typing import Protocol, runtime_checkable\n",
    "\n",
    "print(\"ðŸš€ Production Pipeline Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a692012",
   "metadata": {},
   "source": [
    "### **5.1 Configuration Management & Model Registry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4211ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for quantum ML models.\"\"\"\n",
    "    model_type: str\n",
    "    input_dim: int\n",
    "    hidden_dims: List[int]\n",
    "    dropout: float\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    num_epochs: int\n",
    "    early_stopping_patience: int\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict) -> 'ModelConfig':\n",
    "        return cls(**config_dict)\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save configuration to YAML file.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(self.to_dict(), f, default_flow_style=False)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath: str) -> 'ModelConfig':\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            config_dict = yaml.safe_load(f)\n",
    "        return cls.from_dict(config_dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for complete experiments.\"\"\"\n",
    "    experiment_name: str\n",
    "    model_config: ModelConfig\n",
    "    data_config: Dict\n",
    "    training_config: Dict\n",
    "    evaluation_config: Dict\n",
    "    \n",
    "    def get_experiment_hash(self) -> str:\n",
    "        \"\"\"Generate unique hash for experiment configuration.\"\"\"\n",
    "        config_str = str(sorted(self.to_dict().items()))\n",
    "        return hashlib.md5(config_str.encode()).hexdigest()[:8]\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'experiment_name': self.experiment_name,\n",
    "            'model_config': self.model_config.to_dict(),\n",
    "            'data_config': self.data_config,\n",
    "            'training_config': self.training_config,\n",
    "            'evaluation_config': self.evaluation_config\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Model registry for managing trained quantum ML models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, registry_path: str = \"./model_registry\"):\n",
    "        self.registry_path = Path(registry_path)\n",
    "        self.registry_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize MLflow\n",
    "        mlflow.set_tracking_uri(f\"file://{self.registry_path}/mlruns\")\n",
    "        \n",
    "    def register_model(self, model: nn.Module, config: ExperimentConfig,\n",
    "                      metrics: Dict[str, float], artifacts: Dict[str, str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Register a trained model with configuration and metrics.\n",
    "        \"\"\"\n",
    "        experiment_name = config.experiment_name\n",
    "        experiment_hash = config.get_experiment_hash()\n",
    "        \n",
    "        # Set or create experiment\n",
    "        experiment = mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            # Log configuration\n",
    "            mlflow.log_params(config.model_config.to_dict())\n",
    "            mlflow.log_params(config.data_config)\n",
    "            mlflow.log_params(config.training_config)\n",
    "            \n",
    "            # Log metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "            \n",
    "            # Log model\n",
    "            mlflow.pytorch.log_model(\n",
    "                model, \n",
    "                \"model\",\n",
    "                registered_model_name=f\"{experiment_name}_model\"\n",
    "            )\n",
    "            \n",
    "            # Log configuration file\n",
    "            config_path = self.registry_path / f\"config_{experiment_hash}.yaml\"\n",
    "            config.model_config.save(config_path)\n",
    "            mlflow.log_artifact(str(config_path))\n",
    "            \n",
    "            # Log additional artifacts\n",
    "            if artifacts:\n",
    "                for artifact_name, artifact_path in artifacts.items():\n",
    "                    mlflow.log_artifact(artifact_path, artifact_name)\n",
    "            \n",
    "            run_id = mlflow.active_run().info.run_id\n",
    "            \n",
    "        print(f\"âœ… Model registered with run ID: {run_id}\")\n",
    "        return run_id\n",
    "    \n",
    "    def load_model(self, run_id: str) -> Tuple[nn.Module, ExperimentConfig]:\n",
    "        \"\"\"\n",
    "        Load a registered model and its configuration.\n",
    "        \"\"\"\n",
    "        model_uri = f\"runs:/{run_id}/model\"\n",
    "        model = mlflow.pytorch.load_model(model_uri)\n",
    "        \n",
    "        # Load configuration\n",
    "        run = mlflow.get_run(run_id)\n",
    "        config_dict = run.data.params\n",
    "        \n",
    "        # Reconstruct config (simplified)\n",
    "        model_config = ModelConfig(\n",
    "            model_type=config_dict.get('model_type', 'unknown'),\n",
    "            input_dim=int(config_dict.get('input_dim', 0)),\n",
    "            hidden_dims=eval(config_dict.get('hidden_dims', '[128]')),\n",
    "            dropout=float(config_dict.get('dropout', 0.1)),\n",
    "            learning_rate=float(config_dict.get('learning_rate', 1e-3)),\n",
    "            batch_size=int(config_dict.get('batch_size', 32)),\n",
    "            num_epochs=int(config_dict.get('num_epochs', 100)),\n",
    "            early_stopping_patience=int(config_dict.get('early_stopping_patience', 10))\n",
    "        )\n",
    "        \n",
    "        experiment_config = ExperimentConfig(\n",
    "            experiment_name=run.info.experiment_id,\n",
    "            model_config=model_config,\n",
    "            data_config={},\n",
    "            training_config={},\n",
    "            evaluation_config={}\n",
    "        )\n",
    "        \n",
    "        return model, experiment_config\n",
    "\n",
    "print(\"âœ… Configuration management and model registry implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6cd7c",
   "metadata": {},
   "source": [
    "### **5.2 Production Quantum ML Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@runtime_checkable\n",
    "class QuantumMLPredictor(Protocol):\n",
    "    \"\"\"Protocol for quantum ML predictors.\"\"\"\n",
    "    \n",
    "    def predict(self, smiles: str) -> Dict[str, float]:\n",
    "        \"\"\"Make prediction for a single molecule.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def predict_batch(self, smiles_list: List[str]) -> List[Dict[str, float]]:\n",
    "        \"\"\"Make predictions for a batch of molecules.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class ProductionQuantumMLPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready pipeline for quantum ML predictions.\n",
    "    \n",
    "    Integrates feature engineering, model prediction, uncertainty quantification,\n",
    "    and result validation in a single streamlined interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_registry: ModelRegistry):\n",
    "        self.model_registry = model_registry\n",
    "        self.feature_engineer = QuantumFeatureEngineer()\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.cache = {}\n",
    "        \n",
    "    def load_model(self, model_name: str, run_id: str):\n",
    "        \"\"\"Load a model from the registry.\"\"\"\n",
    "        model, config = self.model_registry.load_model(run_id)\n",
    "        self.models[model_name] = {\n",
    "            'model': model,\n",
    "            'config': config\n",
    "        }\n",
    "        print(f\"âœ… Loaded model: {model_name}\")\n",
    "    \n",
    "    def register_feature_scaler(self, model_name: str, scaler: StandardScaler):\n",
    "        \"\"\"Register feature scaler for a model.\"\"\"\n",
    "        self.scalers[model_name] = scaler\n",
    "    \n",
    "    def predict_molecular_properties(self, smiles: str, model_name: str = None,\n",
    "                                   include_uncertainty: bool = True,\n",
    "                                   use_cache: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete pipeline for molecular property prediction.\n",
    "        \n",
    "        Args:\n",
    "            smiles: SMILES string\n",
    "            model_name: Name of model to use (if None, uses best available)\n",
    "            include_uncertainty: Whether to include uncertainty estimates\n",
    "            use_cache: Whether to use cached results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with predictions, uncertainties, and metadata\n",
    "        \"\"\"\n",
    "        # Check cache\n",
    "        cache_key = f\"{smiles}_{model_name}_{include_uncertainty}\"\n",
    "        if use_cache and cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Validate SMILES\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                raise ValueError(f\"Invalid SMILES: {smiles}\")\n",
    "            \n",
    "            # Extract features\n",
    "            features_dict = self.feature_engineer.extract_molecular_features([smiles])\n",
    "            if not features_dict or not any(f.size > 0 for f in features_dict.values()):\n",
    "                raise ValueError(f\"Feature extraction failed for: {smiles}\")\n",
    "            \n",
    "            # Create feature matrix\n",
    "            feature_matrix, feature_names = self.feature_engineer.create_feature_matrix(features_dict)\n",
    "            \n",
    "            # Select model\n",
    "            if model_name is None:\n",
    "                model_name = list(self.models.keys())[0] if self.models else None\n",
    "            \n",
    "            if model_name not in self.models:\n",
    "                raise ValueError(f\"Model '{model_name}' not loaded\")\n",
    "            \n",
    "            model_info = self.models[model_name]\n",
    "            model = model_info['model']\n",
    "            \n",
    "            # Scale features\n",
    "            if model_name in self.scalers:\n",
    "                scaled_features = self.scalers[model_name].transform(feature_matrix)\n",
    "            else:\n",
    "                scaled_features = feature_matrix\n",
    "            \n",
    "            # Make prediction\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                features_tensor = torch.FloatTensor(scaled_features)\n",
    "                \n",
    "                if hasattr(model, 'forward') and 'MultiTask' in str(type(model)):\n",
    "                    # Multi-task model\n",
    "                    outputs = model(features_tensor)\n",
    "                    predictions = {\n",
    "                        prop: pred.item() for prop, pred in outputs['predictions'].items()\n",
    "                    }\n",
    "                    \n",
    "                    if include_uncertainty:\n",
    "                        uncertainties = {\n",
    "                            prop: uncert.item() for prop, uncert in outputs['uncertainties'].items()\n",
    "                        }\n",
    "                    else:\n",
    "                        uncertainties = {}\n",
    "                else:\n",
    "                    # Single-task model\n",
    "                    prediction = model(features_tensor).item()\n",
    "                    predictions = {'predicted_property': prediction}\n",
    "                    uncertainties = {'predicted_property': 0.1 * abs(prediction)} if include_uncertainty else {}\n",
    "            \n",
    "            # Prepare result\n",
    "            result = {\n",
    "                'smiles': smiles,\n",
    "                'predictions': predictions,\n",
    "                'uncertainties': uncertainties,\n",
    "                'model_name': model_name,\n",
    "                'features': {\n",
    "                    'count': len(feature_names),\n",
    "                    'names': feature_names[:10]  # First 10 feature names\n",
    "                },\n",
    "                'metadata': {\n",
    "                    'prediction_time': time.time() - start_time,\n",
    "                    'model_type': model_info['config'].model_config.model_type,\n",
    "                    'confidence': 'high' if max(uncertainties.values()) < 0.1 else 'medium' if max(uncertainties.values()) < 0.2 else 'low' if uncertainties else 'unknown'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Cache result\n",
    "            if use_cache:\n",
    "                self.cache[cache_key] = result\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Return error result\n",
    "            return {\n",
    "                'smiles': smiles,\n",
    "                'error': str(e),\n",
    "                'predictions': {},\n",
    "                'uncertainties': {},\n",
    "                'metadata': {\n",
    "                    'prediction_time': time.time() - start_time,\n",
    "                    'status': 'failed'\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def predict_batch(self, smiles_list: List[str], model_name: str = None,\n",
    "                     include_uncertainty: bool = True, max_workers: int = 4) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Batch prediction with parallel processing.\n",
    "        \"\"\"\n",
    "        print(f\"ðŸš€ Processing batch of {len(smiles_list)} molecules...\")\n",
    "        \n",
    "        # Use ThreadPoolExecutor for I/O bound tasks\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            prediction_fn = partial(\n",
    "                self.predict_molecular_properties,\n",
    "                model_name=model_name,\n",
    "                include_uncertainty=include_uncertainty\n",
    "            )\n",
    "            \n",
    "            results = list(executor.map(prediction_fn, smiles_list))\n",
    "        \n",
    "        # Compute batch statistics\n",
    "        successful_predictions = [r for r in results if 'error' not in r]\n",
    "        failed_predictions = [r for r in results if 'error' in r]\n",
    "        \n",
    "        print(f\"âœ… Batch processing completed:\")\n",
    "        print(f\"   â€¢ Successful: {len(successful_predictions)}\")\n",
    "        print(f\"   â€¢ Failed: {len(failed_predictions)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def validate_predictions(self, results: List[Dict[str, Any]], \n",
    "                           validation_rules: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate prediction results against chemical knowledge and rules.\n",
    "        \"\"\"\n",
    "        if validation_rules is None:\n",
    "            validation_rules = {\n",
    "                'homo_range': (-1.0, 0.0),\n",
    "                'lumo_range': (-0.5, 0.5),\n",
    "                'gap_min': 0.01,\n",
    "                'max_uncertainty': 0.5\n",
    "            }\n",
    "        \n",
    "        validation_report = {\n",
    "            'total_predictions': len(results),\n",
    "            'valid_predictions': 0,\n",
    "            'invalid_predictions': 0,\n",
    "            'warnings': [],\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        for result in results:\n",
    "            if 'error' in result:\n",
    "                validation_report['errors'].append(f\"Prediction failed for {result['smiles']}: {result['error']}\")\n",
    "                validation_report['invalid_predictions'] += 1\n",
    "                continue\n",
    "            \n",
    "            predictions = result['predictions']\n",
    "            uncertainties = result['uncertainties']\n",
    "            \n",
    "            # Validate HOMO energy\n",
    "            if 'homo' in predictions:\n",
    "                homo_val = predictions['homo']\n",
    "                homo_range = validation_rules['homo_range']\n",
    "                if not (homo_range[0] <= homo_val <= homo_range[1]):\n",
    "                    validation_report['warnings'].append(\n",
    "                        f\"HOMO energy out of range for {result['smiles']}: {homo_val:.3f}\"\n",
    "                    )\n",
    "            \n",
    "            # Validate uncertainties\n",
    "            for prop, uncertainty in uncertainties.items():\n",
    "                if uncertainty > validation_rules['max_uncertainty']:\n",
    "                    validation_report['warnings'].append(\n",
    "                        f\"High uncertainty for {prop} in {result['smiles']}: {uncertainty:.3f}\"\n",
    "                    )\n",
    "            \n",
    "            validation_report['valid_predictions'] += 1\n",
    "        \n",
    "        validation_report['success_rate'] = validation_report['valid_predictions'] / validation_report['total_predictions']\n",
    "        \n",
    "        return validation_report\n",
    "    \n",
    "    def export_predictions(self, results: List[Dict[str, Any]], \n",
    "                          output_format: str = 'csv', filepath: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Export prediction results to various formats.\n",
    "        \"\"\"\n",
    "        if filepath is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"quantum_ml_predictions_{timestamp}.{output_format}\"\n",
    "        \n",
    "        # Flatten results for export\n",
    "        export_data = []\n",
    "        for result in results:\n",
    "            if 'error' in result:\n",
    "                continue\n",
    "            \n",
    "            row = {'smiles': result['smiles']}\n",
    "            row.update(result['predictions'])\n",
    "            row.update({f\"{k}_uncertainty\": v for k, v in result['uncertainties'].items()})\n",
    "            row.update({f\"meta_{k}\": v for k, v in result['metadata'].items() if isinstance(v, (int, float, str))})\n",
    "            \n",
    "            export_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(export_data)\n",
    "        \n",
    "        if output_format.lower() == 'csv':\n",
    "            df.to_csv(filepath, index=False)\n",
    "        elif output_format.lower() == 'json':\n",
    "            df.to_json(filepath, orient='records', indent=2)\n",
    "        elif output_format.lower() == 'excel':\n",
    "            df.to_excel(filepath, index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {output_format}\")\n",
    "        \n",
    "        print(f\"âœ… Predictions exported to: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "print(\"âœ… Production pipeline implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141c27b",
   "metadata": {},
   "source": [
    "### **5.3 Complete Integration Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc84a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize production pipeline\n",
    "print(\"ðŸš€ Setting up Production Quantum ML Pipeline...\")\n",
    "\n",
    "# Initialize model registry\n",
    "model_registry = ModelRegistry(\"./demo_model_registry\")\n",
    "\n",
    "# Initialize production pipeline\n",
    "pipeline = ProductionQuantumMLPipeline(model_registry)\n",
    "\n",
    "# Create and register a demo model\n",
    "print(\"\\nCreating demo model for registration...\")\n",
    "\n",
    "# Create a simple demo model\n",
    "demo_model = MultiTaskQuantumPredictor(\n",
    "    input_dim=scaled_features.shape[1],\n",
    "    hidden_dims=[128, 64],\n",
    "    property_dims={'homo': 1, 'lumo': 1, 'gap': 1},\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Create demo configuration\n",
    "demo_config = ExperimentConfig(\n",
    "    experiment_name=\"quantum_ml_demo\",\n",
    "    model_config=ModelConfig(\n",
    "        model_type=\"MultiTaskQuantumPredictor\",\n",
    "        input_dim=scaled_features.shape[1],\n",
    "        hidden_dims=[128, 64],\n",
    "        dropout=0.1,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=32,\n",
    "        num_epochs=50,\n",
    "        early_stopping_patience=10\n",
    "    ),\n",
    "    data_config={\n",
    "        \"dataset\": \"QM9_subset\",\n",
    "        \"num_molecules\": len(aligned_qm9_data),\n",
    "        \"features\": \"quantum_engineered\"\n",
    "    },\n",
    "    training_config={\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"loss\": \"multi_task_mse\"\n",
    "    },\n",
    "    evaluation_config={\n",
    "        \"metrics\": [\"mae\", \"rmse\", \"r2\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Register the model\n",
    "demo_metrics = {\n",
    "    \"homo_mae\": 0.012,\n",
    "    \"lumo_mae\": 0.015,\n",
    "    \"gap_mae\": 0.008,\n",
    "    \"overall_r2\": 0.94\n",
    "}\n",
    "\n",
    "run_id = model_registry.register_model(\n",
    "    model=demo_model,\n",
    "    config=demo_config,\n",
    "    metrics=demo_metrics\n",
    ")\n",
    "\n",
    "print(f\"âœ… Demo model registered with run ID: {run_id}\")\n",
    "\n",
    "# Load model into pipeline\n",
    "pipeline.load_model(\"quantum_predictor\", run_id)\n",
    "\n",
    "# Register feature scaler\n",
    "pipeline.register_feature_scaler(\"quantum_predictor\", feature_engineer.scalers.get('standard', StandardScaler()))\n",
    "\n",
    "print(\"âœ… Production pipeline setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete pipeline\n",
    "print(\"ðŸ§ª Testing Production Pipeline...\")\n",
    "\n",
    "# Test molecules\n",
    "test_smiles = [\n",
    "    \"CCO\",           # Ethanol\n",
    "    \"c1ccccc1\",      # Benzene\n",
    "    \"CC(=O)O\",       # Acetic acid\n",
    "    \"CCN(CC)CC\",     # Triethylamine\n",
    "    \"c1ccc2ccccc2c1\" # Naphthalene\n",
    "]\n",
    "\n",
    "print(f\"\\nTesting with {len(test_smiles)} molecules:\")\n",
    "for i, smiles in enumerate(test_smiles):\n",
    "    print(f\"   {i+1}. {smiles}\")\n",
    "\n",
    "# Single prediction test\n",
    "print(\"\\nðŸ” Single Prediction Test:\")\n",
    "single_result = pipeline.predict_molecular_properties(\n",
    "    smiles=test_smiles[0],\n",
    "    model_name=\"quantum_predictor\",\n",
    "    include_uncertainty=True\n",
    ")\n",
    "\n",
    "print(f\"   â€¢ SMILES: {single_result['smiles']}\")\n",
    "if 'predictions' in single_result:\n",
    "    for prop, value in single_result['predictions'].items():\n",
    "        uncertainty = single_result['uncertainties'].get(prop, 0)\n",
    "        print(f\"   â€¢ {prop}: {value:.4f} Â± {uncertainty:.4f}\")\n",
    "    print(f\"   â€¢ Confidence: {single_result['metadata']['confidence']}\")\n",
    "    print(f\"   â€¢ Prediction time: {single_result['metadata']['prediction_time']:.3f}s\")\n",
    "\n",
    "# Batch prediction test\n",
    "print(\"\\nðŸ“Š Batch Prediction Test:\")\n",
    "batch_results = pipeline.predict_batch(\n",
    "    smiles_list=test_smiles,\n",
    "    model_name=\"quantum_predictor\",\n",
    "    include_uncertainty=True,\n",
    "    max_workers=2\n",
    ")\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for result in batch_results:\n",
    "    if 'error' not in result:\n",
    "        row = {'SMILES': result['smiles']}\n",
    "        for prop, value in result['predictions'].items():\n",
    "            uncertainty = result['uncertainties'].get(prop, 0)\n",
    "            row[f'{prop}'] = f\"{value:.4f}\"\n",
    "            row[f'{prop}_uncertainty'] = f\"{uncertainty:.4f}\"\n",
    "        row['confidence'] = result['metadata']['confidence']\n",
    "        summary_data.append(row)\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\nðŸ“‹ Prediction Summary:\")\n",
    "    display(summary_df)\n",
    "\n",
    "# Validate predictions\n",
    "print(\"\\nâœ… Validation Test:\")\n",
    "validation_report = pipeline.validate_predictions(batch_results)\n",
    "\n",
    "print(f\"   â€¢ Total predictions: {validation_report['total_predictions']}\")\n",
    "print(f\"   â€¢ Success rate: {validation_report['success_rate']:.1%}\")\n",
    "print(f\"   â€¢ Warnings: {len(validation_report['warnings'])}\")\n",
    "print(f\"   â€¢ Errors: {len(validation_report['errors'])}\")\n",
    "\n",
    "if validation_report['warnings']:\n",
    "    print(\"   â€¢ Sample warnings:\")\n",
    "    for warning in validation_report['warnings'][:3]:\n",
    "        print(f\"     - {warning}\")\n",
    "\n",
    "# Export results\n",
    "print(\"\\nðŸ’¾ Export Test:\")\n",
    "export_file = pipeline.export_predictions(batch_results, output_format='csv')\n",
    "\n",
    "print(\"âœ… Production pipeline testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ **MID-SECTION EXERCISE CHECKPOINT 5.1: Production Pipeline Implementation**\n",
    "\n",
    "print(\"ðŸŽ¯ MID-SECTION EXERCISE CHECKPOINT 5.1: Production Pipeline Implementation\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "# Quick hands-on exercise: Analyze production pipeline\n",
    "exercise_widget_5_1 = create_widget(\n",
    "    assessment, \n",
    "    section=\"5.1\",\n",
    "    concepts=[\n",
    "        \"Production ML pipeline architecture\",\n",
    "        \"Model registry and configuration management\",\n",
    "        \"Batch processing and validation systems\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Built production quantum ML pipeline\",\n",
    "        \"Implemented model registry and configs\", \n",
    "        \"Tested batch prediction and validation\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if assessment:\n",
    "    assessment.record_activity(\n",
    "        activity=\"production_pipeline_implementation\",\n",
    "        result=\"completed\",\n",
    "        metadata={\n",
    "            \"pipeline_functional\": True,\n",
    "            \"batch_processing\": \"tested\",\n",
    "            \"section\": \"5.1_mid_checkpoint\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "exercise_widget_5_1.display()\n",
    "print(\"ðŸŽ“ Mid-section checkpoint 5.1 completed! Continue with monitoring systems...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c48d6",
   "metadata": {},
   "source": [
    "### **5.4 Model Monitoring & Maintenance System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd922533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumMLMonitor:\n",
    "    \"\"\"\n",
    "    Monitoring system for production quantum ML models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline: ProductionQuantumMLPipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.prediction_log = []\n",
    "        self.performance_metrics = {}\n",
    "        self.drift_detectors = {}\n",
    "        \n",
    "    def log_prediction(self, result: Dict[str, Any]):\n",
    "        \"\"\"Log a prediction for monitoring.\"\"\"\n",
    "        self.prediction_log.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'smiles': result['smiles'],\n",
    "            'predictions': result.get('predictions', {}),\n",
    "            'uncertainties': result.get('uncertainties', {}),\n",
    "            'prediction_time': result['metadata'].get('prediction_time', 0),\n",
    "            'confidence': result['metadata'].get('confidence', 'unknown')\n",
    "        })\n",
    "    \n",
    "    def compute_monitoring_metrics(self, window_size: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"Compute monitoring metrics over recent predictions.\"\"\"\n",
    "        if len(self.prediction_log) < window_size:\n",
    "            recent_predictions = self.prediction_log\n",
    "        else:\n",
    "            recent_predictions = self.prediction_log[-window_size:]\n",
    "        \n",
    "        if not recent_predictions:\n",
    "            return {}\n",
    "        \n",
    "        # Performance metrics\n",
    "        prediction_times = [p['prediction_time'] for p in recent_predictions]\n",
    "        \n",
    "        # Uncertainty metrics\n",
    "        all_uncertainties = []\n",
    "        for p in recent_predictions:\n",
    "            all_uncertainties.extend(p['uncertainties'].values())\n",
    "        \n",
    "        # Confidence distribution\n",
    "        confidence_counts = {}\n",
    "        for p in recent_predictions:\n",
    "            conf = p['confidence']\n",
    "            confidence_counts[conf] = confidence_counts.get(conf, 0) + 1\n",
    "        \n",
    "        metrics = {\n",
    "            'num_predictions': len(recent_predictions),\n",
    "            'avg_prediction_time': np.mean(prediction_times),\n",
    "            'max_prediction_time': np.max(prediction_times),\n",
    "            'avg_uncertainty': np.mean(all_uncertainties) if all_uncertainties else 0,\n",
    "            'high_uncertainty_rate': sum(1 for u in all_uncertainties if u > 0.2) / len(all_uncertainties) if all_uncertainties else 0,\n",
    "            'confidence_distribution': confidence_counts,\n",
    "            'predictions_per_hour': len(recent_predictions) / max(1, (datetime.now() - recent_predictions[0]['timestamp']).total_seconds() / 3600)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def detect_model_drift(self, reference_predictions: List[Dict], \n",
    "                          current_predictions: List[Dict],\n",
    "                          threshold: float = 0.1) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Detect potential model drift by comparing prediction distributions.\n",
    "        \"\"\"\n",
    "        drift_report = {\n",
    "            'drift_detected': False,\n",
    "            'drift_score': 0.0,\n",
    "            'affected_properties': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Compare prediction distributions for each property\n",
    "        ref_props = {}\n",
    "        curr_props = {}\n",
    "        \n",
    "        # Collect predictions by property\n",
    "        for pred in reference_predictions:\n",
    "            for prop, value in pred.get('predictions', {}).items():\n",
    "                if prop not in ref_props:\n",
    "                    ref_props[prop] = []\n",
    "                ref_props[prop].append(value)\n",
    "        \n",
    "        for pred in current_predictions:\n",
    "            for prop, value in pred.get('predictions', {}).items():\n",
    "                if prop not in curr_props:\n",
    "                    curr_props[prop] = []\n",
    "                curr_props[prop].append(value)\n",
    "        \n",
    "        # Compare distributions using KL divergence approximation\n",
    "        for prop in ref_props:\n",
    "            if prop in curr_props and len(ref_props[prop]) > 10 and len(curr_props[prop]) > 10:\n",
    "                # Simple distribution comparison using means and stds\n",
    "                ref_mean, ref_std = np.mean(ref_props[prop]), np.std(ref_props[prop])\n",
    "                curr_mean, curr_std = np.mean(curr_props[prop]), np.std(currProps[prop])\n",
    "                \n",
    "                # Drift score based on standardized difference\n",
    "                mean_diff = abs(curr_mean - ref_mean) / (ref_std + 1e-8)\n",
    "                std_diff = abs(curr_std - ref_std) / (ref_std + 1e-8)\n",
    "                \n",
    "                property_drift = max(mean_diff, std_diff)\n",
    "                \n",
    "                if property_drift > threshold:\n",
    "                    drift_report['drift_detected'] = True\n",
    "                    drift_report['affected_properties'].append({\n",
    "                        'property': prop,\n",
    "                        'drift_score': property_drift,\n",
    "                        'ref_mean': ref_mean,\n",
    "                        'curr_mean': curr_mean,\n",
    "                        'mean_shift': curr_mean - ref_mean\n",
    "                    })\n",
    "        \n",
    "        # Overall drift score\n",
    "        if drift_report['affected_properties']:\n",
    "            drift_report['drift_score'] = max(p['drift_score'] for p in drift_report['affected_properties'])\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if drift_report['drift_detected']:\n",
    "            drift_report['recommendations'] = [\n",
    "                \"Consider retraining the model with recent data\",\n",
    "                \"Investigate changes in input data distribution\",\n",
    "                \"Review feature engineering pipeline\",\n",
    "                \"Implement model ensemble to improve robustness\"\n",
    "            ]\n",
    "        \n",
    "        return drift_report\n",
    "    \n",
    "    def generate_monitoring_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive monitoring report.\"\"\"\n",
    "        metrics = self.compute_monitoring_metrics()\n",
    "        \n",
    "        report = {\n",
    "            'report_timestamp': datetime.now(),\n",
    "            'monitoring_period': '24 hours',\n",
    "            'model_performance': metrics,\n",
    "            'system_health': {\n",
    "                'status': 'healthy' if metrics.get('avg_prediction_time', 0) < 1.0 else 'degraded',\n",
    "                'uptime': '99.9%',  # Mock uptime\n",
    "                'total_predictions': len(self.prediction_log)\n",
    "            },\n",
    "            'alerts': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Generate alerts based on metrics\n",
    "        if metrics.get('avg_prediction_time', 0) > 2.0:\n",
    "            report['alerts'].append(\"High average prediction time detected\")\n",
    "        \n",
    "        if metrics.get('high_uncertainty_rate', 0) > 0.3:\n",
    "            report['alerts'].append(\"High rate of uncertain predictions\")\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if report['alerts']:\n",
    "            report['recommendations'] = [\n",
    "                \"Monitor system resources\",\n",
    "                \"Consider model optimization\",\n",
    "                \"Review input data quality\"\n",
    "            ]\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize monitoring system\n",
    "monitor = QuantumMLMonitor(pipeline)\n",
    "\n",
    "# Simulate monitoring by logging the previous predictions\n",
    "print(\"ðŸ” Initializing Model Monitoring System...\")\n",
    "\n",
    "# Log batch predictions for monitoring\n",
    "for result in batch_results:\n",
    "    monitor.log_prediction(result)\n",
    "\n",
    "# Generate monitoring metrics\n",
    "monitoring_metrics = monitor.compute_monitoring_metrics()\n",
    "\n",
    "print(f\"\\nðŸ“Š Monitoring Metrics:\")\n",
    "print(f\"   â€¢ Total predictions logged: {monitoring_metrics['num_predictions']}\")\n",
    "print(f\"   â€¢ Average prediction time: {monitoring_metrics['avg_prediction_time']:.3f}s\")\n",
    "print(f\"   â€¢ Average uncertainty: {monitoring_metrics['avg_uncertainty']:.4f}\")\n",
    "print(f\"   â€¢ High uncertainty rate: {monitoring_metrics['high_uncertainty_rate']:.1%}\")\n",
    "print(f\"   â€¢ Confidence distribution: {monitoring_metrics['confidence_distribution']}\")\n",
    "\n",
    "# Generate monitoring report\n",
    "monitoring_report = monitor.generate_monitoring_report()\n",
    "\n",
    "print(f\"\\nðŸ“‹ System Health Report:\")\n",
    "print(f\"   â€¢ Status: {monitoring_report['system_health']['status']}\")\n",
    "print(f\"   â€¢ Total predictions: {monitoring_report['system_health']['total_predictions']}\")\n",
    "print(f\"   â€¢ Alerts: {len(monitoring_report['alerts'])}\")\n",
    "\n",
    "if monitoring_report['alerts']:\n",
    "    print(\"   â€¢ Active alerts:\")\n",
    "    for alert in monitoring_report['alerts']:\n",
    "        print(f\"     - {alert}\")\n",
    "\n",
    "print(\"âœ… Monitoring system demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12d050",
   "metadata": {},
   "source": [
    "### **5.5 Day 5 Project Summary & Portfolio Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive project summary\n",
    "print(\"ðŸŽ¯ Day 5 Quantum ML Integration Project Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "project_summary = {\n",
    "    \"project_title\": \"Quantum ML Integration with Advanced Architectures\",\n",
    "    \"completion_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"sections_completed\": [\n",
    "        {\n",
    "            \"section\": \"1. QM9 Dataset Mastery & Quantum Feature Engineering\",\n",
    "            \"key_achievements\": [\n",
    "                \"Implemented professional QM9 dataset handler\",\n",
    "                \"Created comprehensive quantum feature engineering framework\", \n",
    "                \"Built baseline ML models with correlation analysis\",\n",
    "                \"Achieved feature-property correlation analysis\"\n",
    "            ],\n",
    "            \"technologies\": [\"RDKit\", \"DeepChem\", \"Scikit-learn\", \"Pandas\", \"NumPy\"]\n",
    "        },\n",
    "        {\n",
    "            \"section\": \"2. SchNet Implementation & 3D Molecular Understanding\",\n",
    "            \"key_achievements\": [\n",
    "                \"Implemented complete SchNet architecture from scratch\",\n",
    "                \"Built continuous-filter convolutional layers\",\n",
    "                \"Created 3D molecular graph construction pipeline\",\n",
    "                \"Developed SchNet training framework with early stopping\"\n",
    "            ],\n",
    "            \"technologies\": [\"PyTorch\", \"PyTorch Geometric\", \"RDKit\", \"ASE\"]\n",
    "        },\n",
    "        {\n",
    "            \"section\": \"3. Delta Learning Framework for QM/ML Hybrid Models\",\n",
    "            \"key_achievements\": [\n",
    "                \"Built quantum method simulation framework\",\n",
    "                \"Implemented delta learning for QM/ML corrections\",\n",
    "                \"Created active learning for optimal molecule selection\",\n",
    "                \"Demonstrated cost-effective high-accuracy predictions\"\n",
    "            ],\n",
    "            \"technologies\": [\"Scikit-learn\", \"Multiprocessing\", \"Optimization\"]\n",
    "        },\n",
    "        {\n",
    "            \"section\": \"4. Advanced Quantum ML Architectures\",\n",
    "            \"key_achievements\": [\n",
    "                \"Implemented quantum-aware attention mechanisms\",\n",
    "                \"Built molecular transformer architecture\",\n",
    "                \"Created multi-task learning framework\",\n",
    "                \"Developed ensemble uncertainty quantification\"\n",
    "            ],\n",
    "            \"technologies\": [\"PyTorch\", \"Transformers\", \"Attention Mechanisms\"]\n",
    "        },\n",
    "        {\n",
    "            \"section\": \"5. Production Pipeline & Integration Toolkit\",\n",
    "            \"key_achievements\": [\n",
    "                \"Built complete production ML pipeline\",\n",
    "                \"Implemented model registry with MLflow\",\n",
    "                \"Created monitoring and drift detection system\",\n",
    "                \"Developed batch processing and validation framework\"\n",
    "            ],\n",
    "            \"technologies\": [\"MLflow\", \"Production Systems\", \"Monitoring\", \"YAML\"]\n",
    "        }\n",
    "    ],\n",
    "    \"key_metrics\": {\n",
    "        \"models_implemented\": 6,\n",
    "        \"architectures_covered\": [\"SchNet\", \"Transformers\", \"Multi-task\", \"Ensemble\"],\n",
    "        \"datasets_processed\": [\"QM9\", \"Custom molecular sets\"],\n",
    "        \"prediction_accuracy\": \"RÂ² > 0.9 for major quantum properties\",\n",
    "        \"code_quality\": \"Production-ready with error handling and monitoring\"\n",
    "    },\n",
    "    \"deliverables\": [\n",
    "        \"Complete quantum ML toolkit\",\n",
    "        \"Production-ready prediction pipeline\", \n",
    "        \"Model registry and monitoring system\",\n",
    "        \"Comprehensive documentation and examples\",\n",
    "        \"Integration framework for real applications\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nðŸš€ Project: {project_summary['project_title']}\")\n",
    "print(f\"ðŸ“… Completed: {project_summary['completion_date']}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Key Metrics:\")\n",
    "for metric, value in project_summary['key_metrics'].items():\n",
    "    print(f\"   â€¢ {metric.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Major Deliverables:\")\n",
    "for deliverable in project_summary['deliverables']:\n",
    "    print(f\"   â€¢ {deliverable}\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Technologies Mastered:\")\n",
    "all_technologies = set()\n",
    "for section in project_summary['sections_completed']:\n",
    "    all_technologies.update(section['technologies'])\n",
    "print(f\"   â€¢ {', '.join(sorted(all_technologies))}\")\n",
    "\n",
    "# Save project summary\n",
    "summary_file = f\"day_05_quantum_ml_summary_{datetime.now().strftime('%Y%m%d')}.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(project_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Project summary saved to: {summary_file}\")\n",
    "\n",
    "# Create portfolio entry\n",
    "portfolio_entry = {\n",
    "    \"day\": 5,\n",
    "    \"title\": \"Quantum ML Integration & Advanced Architectures\",\n",
    "    \"duration\": \"6-8 hours intensive coding\",\n",
    "    \"difficulty\": \"Advanced\",\n",
    "    \"skills_developed\": [\n",
    "        \"Quantum-aware deep learning architectures\",\n",
    "        \"SchNet and continuous-filter convolutions\", \n",
    "        \"Delta learning for QM/ML hybrid models\",\n",
    "        \"Molecular transformers and attention mechanisms\",\n",
    "        \"Production ML pipeline development\",\n",
    "        \"Model monitoring and drift detection\",\n",
    "        \"Multi-task learning for quantum properties\",\n",
    "        \"Uncertainty quantification and ensemble methods\"\n",
    "    ],\n",
    "    \"real_world_applications\": [\n",
    "        \"Drug discovery and molecular design\",\n",
    "        \"Materials science and catalyst development\",\n",
    "        \"Quantum chemistry acceleration\",\n",
    "        \"Chemical property prediction at scale\",\n",
    "        \"AI-driven molecular optimization\"\n",
    "    ],\n",
    "    \"portfolio_value\": \"Demonstrates cutting-edge quantum ML expertise and production systems development\"\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Portfolio Entry for Day 5:\")\n",
    "print(f\"   â€¢ Title: {portfolio_entry['title']}\")\n",
    "print(f\"   â€¢ Difficulty: {portfolio_entry['difficulty']}\")\n",
    "print(f\"   â€¢ Duration: {portfolio_entry['duration']}\")\n",
    "print(f\"   â€¢ Skills: {len(portfolio_entry['skills_developed'])} advanced skills\")\n",
    "print(f\"   â€¢ Applications: {len(portfolio_entry['real_world_applications'])} real-world use cases\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… DAY 5 COMPLETE: Quantum ML Integration & Advanced Architectures\")\n",
    "print(\"ðŸŽ¯ READY FOR DAY 6: Quantum Computing Algorithms & VQE\")\n",
    "print(\"ðŸš€ You've built a complete quantum ML production system!\")\n",
    "print(\"ðŸ“ˆ Your skills now span from basic ML to cutting-edge quantum architectures\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ“ **FINAL DAY 5 COMPREHENSIVE ASSESSMENT**\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ“ FINAL DAY 5 COMPREHENSIVE ASSESSMENT: Quantum ML Integration\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if assessment:\n",
    "    # Record day completion\n",
    "    assessment.record_activity(\n",
    "        \"day_5_completion\", \n",
    "        \"completed\",\n",
    "        {\"day\": 5, \"total_sections\": 5, \"timestamp\": datetime.now().isoformat()}\n",
    "    )\n",
    "    \n",
    "    # End the day assessment\n",
    "    assessment.end_section(\"day_5_quantum_ml\")\n",
    "\n",
    "# Create comprehensive final assessment widget\n",
    "final_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Day 5 Final Assessment: Quantum ML Integration & Production Systems\",\n",
    "    concepts=[\n",
    "        \"QM9 dataset mastery and quantum property analysis\",\n",
    "        \"SchNet implementation for 3D molecular understanding\",\n",
    "        \"Delta learning frameworks for QM/ML hybrid systems\",\n",
    "        \"Advanced quantum ML architectures (QGNNs, QTransformers)\",\n",
    "        \"Production quantum ML pipeline development\",\n",
    "        \"Quantum advantage analysis and practical applications\",\n",
    "        \"Integration of classical and quantum ML approaches\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Mastered QM9 dataset and quantum chemical properties\",\n",
    "        \"Implemented end-to-end SchNet architecture\",\n",
    "        \"Built delta learning correction systems\",\n",
    "        \"Created advanced quantum ML architectures\",\n",
    "        \"Developed production-ready quantum ML pipelines\",\n",
    "        \"Analyzed quantum advantage in molecular modeling\",\n",
    "        \"Integrated multiple quantum ML approaches into cohesive system\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the comprehensive assessment\n",
    "print(\"\\nðŸŽ¯ Please complete your final Day 5 assessment:\")\n",
    "final_widget.display()\n",
    "\n",
    "# Generate final progress report\n",
    "if assessment:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š DAY 5 PROGRESS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    progress = assessment.get_progress_summary()\n",
    "    print(f\"Overall Score: {progress['overall_score']:.1f}%\")\n",
    "    print(f\"Sections Completed: {len(progress.get('section_scores', {}))}/5\")\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    report = assessment.get_comprehensive_report()\n",
    "    print(f\"Total Activities: {len(report['activities'])}\")\n",
    "    print(f\"Track: {track_selected}\")\n",
    "    \n",
    "    # Save final report\n",
    "    try:\n",
    "        assessment.save_final_report(f\"day_5_assessment_{student_id}\")\n",
    "        print(f\"âœ… Assessment report saved for student: {student_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Report saving warning: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ“ Congratulations on completing Day 5: Quantum ML Integration!\")\n",
    "print(\"ðŸš€ You're now ready for Day 6: Quantum Computing Algorithms & VQE\")\n",
    "print(\"ðŸ“ˆ Your quantum ML skills are now at production level!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16138902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š **DAY 5 PROGRESS DASHBOARD**\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š DAY 5 PROGRESS DASHBOARD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if assessment:\n",
    "    # Create and display progress dashboard\n",
    "    try:\n",
    "        dashboard = create_dashboard(assessment)\n",
    "        dashboard.display()\n",
    "        print(\"âœ… Progress dashboard generated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Dashboard generation warning: {e}\")\n",
    "        print(\"ðŸ“Š Basic progress summary available above\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Day 5 Assessment Framework Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‹ Section 4 Completion Assessment: Advanced Quantum ML Architectures\n",
    "\n",
    "print(\"ðŸ“‹ SECTION 4 COMPLETION: Advanced Quantum ML Architectures\")\n",
    "\n",
    "# Initialize assessment for Section 4\n",
    "assessment.start_section(\n",
    "    section=\"Section 4 Completion: Advanced Quantum ML Architectures\",\n",
    "    learning_objectives=[\n",
    "        \"Understanding advanced quantum ML architectures (QGNNs, QTransformers)\",\n",
    "        \"Delta learning frameworks for QM/ML hybrid systems\",\n",
    "        \"Quantum molecular graph neural networks implementation\",\n",
    "        \"Production-ready quantum ML pipeline development\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Assess Section 4 learning objectives\n",
    "section4_concepts = {\n",
    "    \"quantum_architectures\": {\n",
    "        \"question\": \"What is the main advantage of Quantum Graph Neural Networks (QGNNs) for molecular modeling?\",\n",
    "        \"options\": [\n",
    "            \"a) They only work with classical computers\",\n",
    "            \"b) They combine quantum superposition with graph structure to capture molecular quantum states\",\n",
    "            \"c) They require fewer training samples\",\n",
    "            \"d) They are always faster than classical methods\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"QGNNs leverage quantum superposition to represent molecular quantum states while using graph structures to capture molecular topology and chemical bonds.\"\n",
    "    },\n",
    "    \"delta_learning\": {\n",
    "        \"question\": \"In delta learning for quantum chemistry, what does the delta correction represent?\",\n",
    "        \"options\": [\n",
    "            \"a) The difference between quantum and classical computation time\",\n",
    "            \"b) The correction between approximate and accurate quantum chemical calculations\",\n",
    "            \"c) The number of qubits used\",\n",
    "            \"d) The molecular weight difference\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"Delta learning predicts the correction needed to go from fast approximate methods (like DFT) to more accurate but expensive methods (like CCSD(T)).\"\n",
    "    },\n",
    "    \"hybrid_systems\": {\n",
    "        \"question\": \"Why are QM/ML hybrid systems particularly powerful for molecular property prediction?\",\n",
    "        \"options\": [\n",
    "            \"a) They only use quantum mechanics\",\n",
    "            \"b) They combine quantum mechanical accuracy with machine learning scalability and speed\",\n",
    "            \"c) They eliminate the need for experimental data\",\n",
    "            \"d) They work only for small molecules\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"Hybrid systems leverage quantum mechanical insights for accuracy while using ML to scale to larger molecular systems and enable fast predictions.\"\n",
    "    },\n",
    "    \"production_pipelines\": {\n",
    "        \"question\": \"What is the most critical consideration when deploying quantum ML models in production?\",\n",
    "        \"options\": [\n",
    "            \"a) Using the largest possible model\",\n",
    "            \"b) Balancing model accuracy, computational efficiency, and uncertainty quantification\",\n",
    "            \"c) Only using quantum hardware\",\n",
    "            \"d) Maximizing the number of features\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"Production systems must balance accuracy with computational constraints while providing reliable uncertainty estimates for decision-making.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Present Section 4 concept assessment\n",
    "for concept, data in section4_concepts.items():\n",
    "    print(f\"\\nðŸ“š {concept.replace('_', ' ').title()}:\")\n",
    "    print(f\"Q: {data['question']}\")\n",
    "    for option in data['options']:\n",
    "        print(f\"   {option}\")\n",
    "    \n",
    "    user_answer = input(\"\\nYour answer (a/b/c/d): \").lower().strip()\n",
    "    \n",
    "    if user_answer == data['correct']:\n",
    "        print(f\"âœ… Correct! {data['explanation']}\")\n",
    "        assessment.record_activity(concept, \"correct\", {\"score\": 1.0})\n",
    "    else:\n",
    "        print(f\"âŒ Incorrect. {data['explanation']}\")\n",
    "        assessment.record_activity(concept, \"incorrect\", {\"score\": 0.0})\n",
    "\n",
    "# Practical Quantum ML Architecture Assessment\n",
    "print(f\"\\nðŸ› ï¸ Hands-On: Advanced Quantum ML Implementation\")\n",
    "\n",
    "# Assess quantum ML implementations\n",
    "quantum_ml_models = 0\n",
    "architecture_quality = 0.0\n",
    "\n",
    "# Check if advanced quantum ML models were created\n",
    "if 'qgnn_model' in locals() or 'quantum_transformer' in locals():\n",
    "    quantum_ml_models = 1\n",
    "    architecture_quality = 0.8  # Simulated quality score\n",
    "    \n",
    "if 'delta_learning_system' in locals():\n",
    "    quantum_ml_models += 1\n",
    "    architecture_quality = max(architecture_quality, 0.85)\n",
    "\n",
    "if 'production_pipeline' in locals():\n",
    "    quantum_ml_models += 1\n",
    "    architecture_quality = max(architecture_quality, 0.9)\n",
    "\n",
    "print(f\"Advanced quantum ML models implemented: {quantum_ml_models}\")\n",
    "print(f\"Architecture quality score: {architecture_quality:.3f}\")\n",
    "\n",
    "# Quantum ML workflow assessment\n",
    "workflow_steps_completed = 0\n",
    "if quantum_ml_models > 0:\n",
    "    workflow_steps_completed = min(quantum_ml_models + 1, 4)  # Cap at 4\n",
    "\n",
    "print(f\"Quantum ML workflow steps completed: {workflow_steps_completed}/4\")\n",
    "\n",
    "# Performance evaluation\n",
    "if quantum_ml_models >= 3 and architecture_quality > 0.85:\n",
    "    print(\"ðŸŒŸ Outstanding quantum ML mastery! Multiple advanced architectures with high quality.\")\n",
    "    assessment.record_activity(\"quantum_ml_architecture\", \"outstanding\", {\n",
    "        \"score\": 1.0,\n",
    "        \"models_implemented\": quantum_ml_models,\n",
    "        \"architecture_quality\": architecture_quality,\n",
    "        \"workflow_completion\": workflow_steps_completed\n",
    "    })\n",
    "elif quantum_ml_models >= 2 and architecture_quality > 0.7:\n",
    "    print(\"ðŸ‘ Excellent quantum ML implementation! Strong advanced architecture understanding.\")\n",
    "    assessment.record_activity(\"quantum_ml_architecture\", \"excellent\", {\n",
    "        \"score\": 0.9,\n",
    "        \"models_implemented\": quantum_ml_models,\n",
    "        \"architecture_quality\": architecture_quality,\n",
    "        \"workflow_completion\": workflow_steps_completed\n",
    "    })\n",
    "elif quantum_ml_models >= 1 and architecture_quality > 0.6:\n",
    "    print(\"ðŸ“ˆ Good quantum ML progress! Solid foundation in advanced architectures.\")\n",
    "    assessment.record_activity(\"quantum_ml_architecture\", \"good\", {\n",
    "        \"score\": 0.8,\n",
    "        \"models_implemented\": quantum_ml_models,\n",
    "        \"architecture_quality\": architecture_quality,\n",
    "        \"workflow_completion\": workflow_steps_completed\n",
    "    })\n",
    "else:\n",
    "    print(\"ðŸ“Š Basic quantum ML concepts mastered. Consider deeper exploration of advanced architectures.\")\n",
    "    assessment.record_activity(\"quantum_ml_architecture\", \"basic\", {\n",
    "        \"score\": 0.6,\n",
    "        \"models_implemented\": quantum_ml_models,\n",
    "        \"architecture_quality\": architecture_quality,\n",
    "        \"workflow_completion\": workflow_steps_completed\n",
    "    })\n",
    "\n",
    "# Production readiness assessment\n",
    "production_readiness = min((quantum_ml_models * architecture_quality) / 2.0, 1.0)\n",
    "\n",
    "if production_readiness >= 0.8:\n",
    "    print(\"ðŸš€ Production-ready quantum ML systems achieved!\")\n",
    "    assessment.record_activity(\"production_readiness\", \"ready\", {\"score\": 1.0})\n",
    "elif production_readiness >= 0.6:\n",
    "    print(\"ðŸ”§ Good progress toward production-ready quantum ML systems!\")\n",
    "    assessment.record_activity(\"production_readiness\", \"developing\", {\"score\": 0.8})\n",
    "else:\n",
    "    print(\"ðŸ“š Foundation established for production quantum ML development.\")\n",
    "    assessment.record_activity(\"production_readiness\", \"foundation\", {\"score\": 0.6})\n",
    "\n",
    "assessment.end_section(\"Section 4 Completion: Advanced Quantum ML Architectures\")\n",
    "\n",
    "print(\"\\nâœ… Section 4 Complete: Advanced Quantum ML Architectures Mastery\")\n",
    "print(\"ðŸš€ Ready to advance to Section 5: Production Pipeline & Integration Toolkit!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
