{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5101a1",
   "metadata": {},
   "source": [
    "# Week 4 Checkpoint: Advanced Molecular Descriptors and Feature Engineering\n",
    "\n",
    "## Learning Objectives Verification\n",
    "By the end of this week, you should be able to:\n",
    "- Calculate and interpret advanced molecular descriptors\n",
    "- Implement feature selection and dimensionality reduction techniques\n",
    "- Apply feature engineering strategies for molecular data\n",
    "- Evaluate descriptor importance and correlation patterns\n",
    "\n",
    "## Progress Tracking Dashboard\n",
    "**Week:** 4/12  \n",
    "**Module:** Advanced Molecular Descriptors and Feature Engineering  \n",
    "**Estimated Time:** 8-12 hours  \n",
    "**Prerequisites:** Weeks 1-3 completed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Lipinski, Crippen, rdMolDescriptors\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Progress tracking\n",
    "progress_tracker = {\n",
    "    'week': 4,\n",
    "    'completed_tasks': [],\n",
    "    'scores': {},\n",
    "    'time_spent': 0,\n",
    "    'challenges_faced': [],\n",
    "    'next_steps': []\n",
    "}\n",
    "\n",
    "print(\"Week 4 Checkpoint: Advanced Molecular Descriptors and Feature Engineering\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02f177",
   "metadata": {},
   "source": [
    "## Task 1: Advanced Descriptor Calculation (25 points)\n",
    "\n",
    "Calculate a comprehensive set of molecular descriptors for a dataset of drug molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample drug molecules (SMILES)\n",
    "drug_smiles = [\n",
    "    'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O',  # Ibuprofen\n",
    "    'CC1=CC=C(C=C1)C(=O)C2=CC=C(C=C2)N(C)C',  # Michler's ketone\n",
    "    'CN1CCC[C@H]1C2=CN=CC=C2',  # Nicotine\n",
    "    'CC(=O)OC1=CC=CC=C1C(=O)O',  # Aspirin\n",
    "    'CN(C)CCN1C2=CC=CC=C2SC3=C1C=C(C=C3)Cl',  # Chlorpromazine\n",
    "    'CCN(CC)CCCC(C)NC1=C2C=CC(=CC2=NC=C1)CF3',  # Mefloquine\n",
    "    'CN1C=NC2=C1C(=O)N(C(=O)N2C)C',  # Caffeine\n",
    "    'CC(C)(C)NCC(C1=CC(=C(C=C1)O)CO)O',  # Salbutamol\n",
    "    'CN(C)C(=N)NC(=N)N',  # Metformin\n",
    "    'CC1=C(C=C(C=C1)N2CCNCC2)C'  # Methylpiperazine derivative\n",
    "]\n",
    "\n",
    "def calculate_advanced_descriptors(smiles_list):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive molecular descriptors\n",
    "    \"\"\"\n",
    "    descriptors_data = []\n",
    "    \n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        # Basic descriptors\n",
    "        desc_dict = {\n",
    "            'SMILES': smiles,\n",
    "            'MW': Descriptors.MolWt(mol),\n",
    "            'LogP': Descriptors.MolLogP(mol),\n",
    "            'HBD': Descriptors.NumHDonors(mol),\n",
    "            'HBA': Descriptors.NumHAcceptors(mol),\n",
    "            'RotBonds': Descriptors.NumRotatableBonds(mol),\n",
    "            'TPSA': Descriptors.TPSA(mol),\n",
    "            'AromaticRings': Descriptors.NumAromaticRings(mol),\n",
    "            'SaturatedRings': Descriptors.NumSaturatedRings(mol),\n",
    "            'HeteroAtoms': Descriptors.NumHeteroatoms(mol),\n",
    "            'FractionCsp3': Descriptors.FractionCsp3(mol),\n",
    "            'MolecularComplexity': Descriptors.BertzCT(mol),\n",
    "            'Flexibility': Descriptors.Kappa3(mol)\n",
    "        }\n",
    "        \n",
    "        # Lipinski's Rule of Five compliance\n",
    "        desc_dict['Lipinski_Violations'] = sum([\n",
    "            desc_dict['MW'] > 500,\n",
    "            desc_dict['LogP'] > 5,\n",
    "            desc_dict['HBD'] > 5,\n",
    "            desc_dict['HBA'] > 10\n",
    "        ])\n",
    "        \n",
    "        # Drug-likeness indicators\n",
    "        desc_dict['QED'] = Descriptors.qed(mol)  # Quantitative Estimate of Drug-likeness\n",
    "        \n",
    "        descriptors_data.append(desc_dict)\n",
    "    \n",
    "    return pd.DataFrame(descriptors_data)\n",
    "\n",
    "# Task 1: Calculate descriptors\n",
    "print(\"Task 1: Calculating advanced molecular descriptors...\")\n",
    "descriptor_df = calculate_advanced_descriptors(drug_smiles)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nCalculated {len(descriptor_df.columns)-1} descriptors for {len(descriptor_df)} molecules\")\n",
    "print(\"\\nDescriptor Summary:\")\n",
    "print(descriptor_df.describe().round(3))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "descriptor_df.hist(column=['MW', 'LogP', 'TPSA', 'QED'], bins=10, ax=axes, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.title('Distribution of Key Molecular Descriptors')\n",
    "plt.show()\n",
    "\n",
    "progress_tracker['completed_tasks'].append('Task 1: Advanced Descriptor Calculation')\n",
    "progress_tracker['scores']['task_1'] = 25  # Full points for completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c212cec",
   "metadata": {},
   "source": [
    "## Task 2: Feature Selection and Correlation Analysis (25 points)\n",
    "\n",
    "Analyze descriptor correlations and implement feature selection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Feature selection and correlation analysis\n",
    "print(\"Task 2: Feature selection and correlation analysis...\")\n",
    "\n",
    "# Prepare data for analysis (excluding SMILES)\n",
    "feature_df = descriptor_df.drop('SMILES', axis=1)\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = feature_df.corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('Molecular Descriptor Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features\n",
    "def find_correlated_features(corr_matrix, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Find pairs of features with correlation above threshold\n",
    "    \"\"\"\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr_pairs.append((\n",
    "                    corr_matrix.columns[i], \n",
    "                    corr_matrix.columns[j], \n",
    "                    corr_matrix.iloc[i, j]\n",
    "                ))\n",
    "    return high_corr_pairs\n",
    "\n",
    "high_corr = find_correlated_features(correlation_matrix)\n",
    "print(f\"\\nFound {len(high_corr)} highly correlated feature pairs (|r| > 0.8):\")\n",
    "for feat1, feat2, corr in high_corr:\n",
    "    print(f\"  {feat1} - {feat2}: {corr:.3f}\")\n",
    "\n",
    "# Feature importance using Random Forest\n",
    "# Use QED as target variable for demonstration\n",
    "X = feature_df.drop('QED', axis=1)\n",
    "y = feature_df['QED']\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Feature importance plot\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance for QED Prediction')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 most important features for QED prediction:\")\n",
    "for i, row in feature_importance.head().iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "progress_tracker['completed_tasks'].append('Task 2: Feature Selection and Correlation Analysis')\n",
    "progress_tracker['scores']['task_2'] = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62434242",
   "metadata": {},
   "source": [
    "## Task 3: Dimensionality Reduction with PCA (25 points)\n",
    "\n",
    "Apply Principal Component Analysis to reduce descriptor dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Dimensionality reduction with PCA\n",
    "print(\"Task 3: Dimensionality reduction with PCA...\")\n",
    "\n",
    "# Standardize features before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_var_ratio = pca.explained_variance_ratio_\n",
    "cumulative_var_ratio = np.cumsum(explained_var_ratio)\n",
    "\n",
    "# Plot explained variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Individual explained variance\n",
    "ax1.bar(range(1, len(explained_var_ratio)+1), explained_var_ratio)\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Individual Explained Variance by PC')\n",
    "ax1.set_xticks(range(1, len(explained_var_ratio)+1))\n",
    "\n",
    "# Cumulative explained variance\n",
    "ax2.plot(range(1, len(cumulative_var_ratio)+1), cumulative_var_ratio, 'bo-')\n",
    "ax2.axhline(y=0.9, color='r', linestyle='--', label='90% Variance')\n",
    "ax2.axhline(y=0.95, color='g', linestyle='--', label='95% Variance')\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance Ratio')\n",
    "ax2.set_title('Cumulative Explained Variance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(range(1, len(cumulative_var_ratio)+1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 90% and 95% variance\n",
    "n_components_90 = np.argmax(cumulative_var_ratio >= 0.90) + 1\n",
    "n_components_95 = np.argmax(cumulative_var_ratio >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nPCA Analysis Results:\")\n",
    "print(f\"  Original features: {X.shape[1]}\")\n",
    "print(f\"  Components for 90% variance: {n_components_90}\")\n",
    "print(f\"  Components for 95% variance: {n_components_95}\")\n",
    "print(f\"  Variance captured by first 3 PCs: {cumulative_var_ratio[2]:.1%}\")\n",
    "\n",
    "# Feature contributions to first 3 PCs\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_[:3].T,\n",
    "    columns=['PC1', 'PC2', 'PC3'],\n",
    "    index=X.columns\n",
    ")\n",
    "\n",
    "# Plot feature loadings\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, pc in enumerate(['PC1', 'PC2', 'PC3']):\n",
    "    loadings = components_df[pc].abs().sort_values(ascending=True)\n",
    "    loadings.plot(kind='barh', ax=axes[i])\n",
    "    axes[i].set_title(f'{pc} Feature Loadings')\n",
    "    axes[i].set_xlabel('Absolute Loading')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 3 features contributing to each PC:\")\n",
    "for pc in ['PC1', 'PC2', 'PC3']:\n",
    "    top_features = components_df[pc].abs().nlargest(3)\n",
    "    print(f\"  {pc}: {', '.join(top_features.index)}\")\n",
    "\n",
    "progress_tracker['completed_tasks'].append('Task 3: Dimensionality Reduction with PCA')\n",
    "progress_tracker['scores']['task_3'] = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bca8ca",
   "metadata": {},
   "source": [
    "## Task 4: Feature Engineering Challenge (25 points)\n",
    "\n",
    "Create new features and evaluate their impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3833a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Feature engineering challenge\n",
    "print(\"Task 4: Feature engineering challenge...\")\n",
    "\n",
    "def engineer_new_features(df):\n",
    "    \"\"\"\n",
    "    Create new engineered features from existing descriptors\n",
    "    \"\"\"\n",
    "    engineered_df = df.copy()\n",
    "    \n",
    "    # Ratio features\n",
    "    engineered_df['HBA_HBD_ratio'] = engineered_df['HBA'] / (engineered_df['HBD'] + 1e-6)\n",
    "    engineered_df['MW_TPSA_ratio'] = engineered_df['MW'] / (engineered_df['TPSA'] + 1e-6)\n",
    "    engineered_df['Aromatic_Total_rings'] = engineered_df['AromaticRings'] / (engineered_df['AromaticRings'] + engineered_df['SaturatedRings'] + 1e-6)\n",
    "    \n",
    "    # Interaction features\n",
    "    engineered_df['LogP_MW_interaction'] = engineered_df['LogP'] * engineered_df['MW']\n",
    "    engineered_df['TPSA_RotBonds_interaction'] = engineered_df['TPSA'] * engineered_df['RotBonds']\n",
    "    \n",
    "    # Binned features\n",
    "    engineered_df['MW_category'] = pd.cut(engineered_df['MW'], \n",
    "                                         bins=[0, 200, 350, 500, float('inf')], \n",
    "                                         labels=['Small', 'Medium', 'Large', 'VeryLarge'])\n",
    "    \n",
    "    # Drug-likeness score\n",
    "    engineered_df['Custom_Druglikeness'] = (\n",
    "        (engineered_df['MW'] <= 500) * 0.25 +\n",
    "        (engineered_df['LogP'] <= 5) * 0.25 +\n",
    "        (engineered_df['HBD'] <= 5) * 0.25 +\n",
    "        (engineered_df['TPSA'] <= 140) * 0.25\n",
    "    )\n",
    "    \n",
    "    return engineered_df\n",
    "\n",
    "# Apply feature engineering\n",
    "engineered_features = engineer_new_features(feature_df)\n",
    "\n",
    "print(f\"\\nOriginal features: {len(feature_df.columns)}\")\n",
    "print(f\"Features after engineering: {len(engineered_features.columns)}\")\n",
    "print(f\"New features created: {len(engineered_features.columns) - len(feature_df.columns)}\")\n",
    "\n",
    "# Evaluate impact of new features on QED prediction\n",
    "def evaluate_feature_impact(original_df, engineered_df, target_col='QED'):\n",
    "    \"\"\"\n",
    "    Compare model performance with original vs engineered features\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, df in [('Original', original_df), ('Engineered', engineered_df)]:\n",
    "        # Prepare data\n",
    "        X = df.drop([target_col], axis=1)\n",
    "        if 'MW_category' in X.columns:\n",
    "            X = pd.get_dummies(X, columns=['MW_category'])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Train model\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        # Calculate R¬≤ score\n",
    "        score = rf.score(X, y)\n",
    "        results[name] = {\n",
    "            'n_features': X.shape[1],\n",
    "            'r2_score': score\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate feature impact\n",
    "impact_results = evaluate_feature_impact(feature_df, engineered_features)\n",
    "\n",
    "print(\"\\nFeature Engineering Impact Assessment:\")\n",
    "for name, results in impact_results.items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Features: {results['n_features']}\")\n",
    "    print(f\"    R¬≤ Score: {results['r2_score']:.4f}\")\n",
    "\n",
    "improvement = impact_results['Engineered']['r2_score'] - impact_results['Original']['r2_score']\n",
    "print(f\"\\nImprovement: {improvement:.4f} ({improvement/impact_results['Original']['r2_score']*100:.1f}%)\")\n",
    "\n",
    "# Visualize new feature distributions\n",
    "new_features = ['HBA_HBD_ratio', 'MW_TPSA_ratio', 'Custom_Druglikeness']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, feature in enumerate(new_features):\n",
    "    engineered_features[feature].hist(bins=10, ax=axes[i], alpha=0.7, color='skyblue')\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "progress_tracker['completed_tasks'].append('Task 4: Feature Engineering Challenge')\n",
    "progress_tracker['scores']['task_4'] = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1ba74",
   "metadata": {},
   "source": [
    "## Self-Assessment and Reflection\n",
    "\n",
    "Complete this self-assessment to evaluate your understanding of advanced molecular descriptors and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e874aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Assessment Questions\n",
    "print(\"SELF-ASSESSMENT: Advanced Molecular Descriptors and Feature Engineering\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "assessment_questions = [\n",
    "    {\n",
    "        'question': 'What is the primary purpose of molecular descriptors in drug discovery?',\n",
    "        'options': ['A) To visualize molecules', 'B) To quantify molecular properties for analysis', \n",
    "                   'C) To store chemical structures', 'D) To name compounds'],\n",
    "        'correct': 'B',\n",
    "        'explanation': 'Molecular descriptors quantify chemical and physical properties of molecules for computational analysis.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which descriptor is most important for assessing membrane permeability?',\n",
    "        'options': ['A) Molecular weight', 'B) Number of rotatable bonds', \n",
    "                   'C) Topological polar surface area (TPSA)', 'D) Number of rings'],\n",
    "        'correct': 'C',\n",
    "        'explanation': 'TPSA is strongly correlated with membrane permeability and oral bioavailability.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What does a high correlation (>0.8) between two descriptors indicate?',\n",
    "        'options': ['A) They are independent', 'B) They provide redundant information', \n",
    "                   'C) One is more important', 'D) They should always be used together'],\n",
    "        'correct': 'B',\n",
    "        'explanation': 'High correlation indicates redundancy, suggesting one descriptor could be removed.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the main advantage of using PCA for descriptor reduction?',\n",
    "        'options': ['A) It removes noise', 'B) It identifies the most important features', \n",
    "                   'C) It creates uncorrelated components', 'D) It improves model accuracy'],\n",
    "        'correct': 'C',\n",
    "        'explanation': 'PCA creates orthogonal (uncorrelated) components while retaining maximum variance.'\n",
    "    },\n",
    "    {\n",
    "        'question': 'When engineering ratio features, why add a small epsilon (1e-6) to the denominator?',\n",
    "        'options': ['A) To improve accuracy', 'B) To prevent division by zero', \n",
    "                   'C) To normalize the data', 'D) To add noise'],\n",
    "        'correct': 'B',\n",
    "        'explanation': 'Adding epsilon prevents division by zero errors when the denominator is zero.'\n",
    "    }\n",
    "]\n",
    "\n",
    "score = 0\n",
    "for i, q in enumerate(assessment_questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {q['question']}\")\n",
    "    for option in q['options']:\n",
    "        print(f\"  {option}\")\n",
    "    \n",
    "    # For demonstration, we'll show the correct answer\n",
    "    print(f\"\\nCorrect Answer: {q['correct']}\")\n",
    "    print(f\"Explanation: {q['explanation']}\")\n",
    "    score += 1  # Assuming correct for progress tracking\n",
    "\n",
    "assessment_score = (score / len(assessment_questions)) * 100\n",
    "progress_tracker['scores']['self_assessment'] = assessment_score\n",
    "\n",
    "print(f\"\\nSelf-Assessment Score: {assessment_score:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dca1f7",
   "metadata": {},
   "source": [
    "## Week 4 Progress Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66214fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall progress\n",
    "total_score = sum(progress_tracker['scores'].values())\n",
    "max_score = 125  # 4 tasks √ó 25 points + 25 points assessment\n",
    "overall_percentage = (total_score / max_score) * 100\n",
    "\n",
    "progress_tracker['overall_score'] = overall_percentage\n",
    "progress_tracker['time_spent'] = 10  # Estimated hours\n",
    "\n",
    "print(\"WEEK 4 PROGRESS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Overall Score: {overall_percentage:.1f}%\")\n",
    "print(f\"Time Spent: {progress_tracker['time_spent']} hours\")\n",
    "print(f\"Tasks Completed: {len(progress_tracker['completed_tasks'])}/4\")\n",
    "\n",
    "print(\"\\nTask Breakdown:\")\n",
    "for task, score in progress_tracker['scores'].items():\n",
    "    print(f\"  {task}: {score}/25 points\")\n",
    "\n",
    "print(\"\\nKey Learning Outcomes Achieved:\")\n",
    "outcomes = [\n",
    "    \"‚úì Calculated comprehensive molecular descriptors\",\n",
    "    \"‚úì Analyzed feature correlations and redundancy\",\n",
    "    \"‚úì Applied PCA for dimensionality reduction\",\n",
    "    \"‚úì Engineered new features and evaluated impact\",\n",
    "    \"‚úì Understood descriptor importance for drug-likeness\"\n",
    "]\n",
    "for outcome in outcomes:\n",
    "    print(f\"  {outcome}\")\n",
    "\n",
    "print(\"\\nNext Week (Week 5) Preview:\")\n",
    "print(\"  üìö Topic: Machine Learning Fundamentals for Drug Discovery\")\n",
    "print(\"  üéØ Focus: Regression and classification models\")\n",
    "print(\"  üí° Skills: Model selection, validation, hyperparameter tuning\")\n",
    "print(\"  üî¨ Practice: Build QSAR models for bioactivity prediction\")\n",
    "\n",
    "# Portfolio development checkpoint\n",
    "print(\"\\nPortfolio Development Checkpoint:\")\n",
    "print(\"  üìä Add descriptor analysis to your multi-target project\")\n",
    "  print(\"  üîß Implement feature engineering pipeline\")\n",
    "print(\"  üìà Document feature selection rationale\")\n",
    "print(\"  üîç Compare descriptor sets across targets\")\n",
    "\n",
    "# Save progress\n",
    "import json\n",
    "with open('week_04_progress.json', 'w') as f:\n",
    "    json.dump(progress_tracker, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Week 4 checkpoint completed! Progress saved to week_04_progress.json\")\n",
    "print(\"üìù Remember to update your learning journal with key insights\")\n",
    "print(\"üöÄ Ready to move on to Week 5: Machine Learning Fundamentals!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
