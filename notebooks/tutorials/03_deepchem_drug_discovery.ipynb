{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5992f022",
   "metadata": {},
   "source": [
    "# Deep Learning for Drug Discovery with DeepChem\n",
    "\n",
    "This notebook demonstrates how to use DeepChem for molecular property prediction and drug discovery tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load and preprocess molecular datasets\n",
    "- Create molecular featurizations\n",
    "- Build and train deep learning models\n",
    "- Evaluate model performance for drug discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aedc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress various warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Suppress DeepChem normalization warnings\n",
    "import sys\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "# Enhanced Weights & Biases Setup\n",
    "print(\"üöÄ Setting up Weights & Biases tracking...\")\n",
    "\n",
    "# Login to wandb with your API key\n",
    "wandb.login(key=\"b4f102d87161194b68baa7395d5862aa3f93b2b7\", relogin=True)\n",
    "\n",
    "# Initialize experiment tracking\n",
    "experiment_config = {\n",
    "    \"framework\": \"deepchem\",\n",
    "    \"task\": \"molecular_property_prediction\",\n",
    "    \"model_type\": \"deep_learning\",\n",
    "    \"dataset\": \"drug_discovery_demo\",\n",
    "    \"notebook\": \"03_deepchem_drug_discovery\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"environment\": \"jupyter\",\n",
    "    \"libraries\": {\n",
    "        \"deepchem\": dc.__version__,\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pandas\": pd.__version__\n",
    "    }\n",
    "}\n",
    "\n",
    "# Start wandb experiment\n",
    "run = wandb.init(\n",
    "    project=\"chemml-experiments\",\n",
    "    name=f\"deepchem_drug_discovery_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    config=experiment_config,\n",
    "    tags=[\"deepchem\", \"drug_discovery\", \"molecular_ml\", \"tutorial\"],\n",
    "    notes=\"Deep learning for drug discovery using DeepChem - comprehensive tutorial with wandb tracking\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DeepChem version: {dc.__version__}\")\n",
    "print(f\"‚úÖ Weights & Biases experiment started: {run.name}\")\n",
    "print(f\"üìä View experiment at: {run.url}\")\n",
    "print(\"üß™ Ready for molecular drug discovery experiments!\")\n",
    "\n",
    "# Helper function to log molecular data to wandb\n",
    "def log_molecular_metrics(y_true, y_pred, dataset_name=\"\", step=None):\n",
    "    \"\"\"Log comprehensive molecular prediction metrics to wandb.\"\"\"\n",
    "    try:\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # Create metrics dictionary\n",
    "        metrics = {\n",
    "            f\"{dataset_name}mse\": mse,\n",
    "            f\"{dataset_name}mae\": mae,\n",
    "            f\"{dataset_name}rmse\": rmse,\n",
    "            f\"{dataset_name}r2_score\": r2,\n",
    "        }\n",
    "        \n",
    "        # Log to wandb\n",
    "        if step is not None:\n",
    "            wandb.log(metrics, step=step)\n",
    "        else:\n",
    "            wandb.log(metrics)\n",
    "        \n",
    "        # Create and log prediction plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(y_true, y_pred, alpha=0.6, s=20)\n",
    "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title(f'{dataset_name}Predictions vs Actual (R¬≤ = {r2:.3f})')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Log plot to wandb\n",
    "        wandb.log({f\"{dataset_name}prediction_plot\": wandb.Image(plt)}, step=step)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üìà Logged metrics for {dataset_name}: R¬≤ = {r2:.3f}, RMSE = {rmse:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error logging metrics: {e}\")\n",
    "\n",
    "print(\"üéØ Helper functions loaded and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58847f",
   "metadata": {},
   "source": [
    "## 1. Loading and Exploring Molecular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c193f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset with SMILES and properties\n",
    "# In practice, you would load this from a file or database\n",
    "sample_data = {\n",
    "    'smiles': [\n",
    "        'CCO',  # Ethanol\n",
    "        'CC(=O)O',  # Acetic acid\n",
    "        'c1ccccc1',  # Benzene\n",
    "        'CC(=O)Nc1ccc(O)cc1',  # Paracetamol\n",
    "        'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O',  # Ibuprofen\n",
    "        'CN1C=NC2=C1C(=O)N(C(=O)N2C)C',  # Caffeine\n",
    "        'CC(C)NCC(C1=CC(=C(C=C1)O)CO)O',  # Salbutamol\n",
    "        'CC1=CC=C(C=C1)C2=CC=C(C=C2)C',  # 4,4'-Dimethylbiphenyl\n",
    "        'C1=CC=C(C=C1)C(=O)O',  # Benzoic acid\n",
    "        'CC(C)(C)C1=CC=C(C=C1)O'  # 4-tert-Butylphenol\n",
    "    ],\n",
    "    'logp': [‚àí0.31, ‚àí0.17, 2.13, 0.46, 3.97, ‚àí0.07, 0.1, 4.79, 1.87, 3.31],  # LogP values\n",
    "    'solubility': [0.0, 0.0, -2.13, -1.46, -3.97, -0.8, -1.5, -4.79, -1.87, -3.31],  # Log solubility\n",
    "    'mw': [46.07, 60.05, 78.11, 151.16, 206.28, 194.19, 239.31, 182.26, 122.12, 150.22]  # Molecular weight\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(\"Sample molecular dataset:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3015f6",
   "metadata": {},
   "source": [
    "## 2. Molecular Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different featurization methods in DeepChem\n",
    "\n",
    "# 1. Circular Fingerprints (ECFP)\n",
    "ecfp_featurizer = dc.feat.CircularFingerprint(size=1024, radius=2)\n",
    "ecfp_features = ecfp_featurizer.featurize(df['smiles'])\n",
    "\n",
    "print(f\"ECFP features shape: {ecfp_features.shape}\")\n",
    "print(f\"ECFP features for first molecule: {ecfp_features[0][:10]}...\")  # First 10 features\n",
    "\n",
    "# 2. RDKit Descriptors\n",
    "rdkit_featurizer = dc.feat.RDKitDescriptors()\n",
    "rdkit_features = rdkit_featurizer.featurize(df['smiles'])\n",
    "\n",
    "print(f\"\\nRDKit descriptors shape: {rdkit_features.shape}\")\n",
    "print(f\"RDKit descriptors for first molecule: {rdkit_features[0][:5]}...\")  # First 5 descriptors\n",
    "\n",
    "# 3. Coulomb Matrix (for small molecules)\n",
    "try:\n",
    "    coulomb_featurizer = dc.feat.CoulombMatrix(max_atoms=50)\n",
    "    coulomb_features = coulomb_featurizer.featurize(df['smiles'])\n",
    "    print(f\"\\nCoulomb Matrix features shape: {coulomb_features.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCoulomb Matrix featurization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e273d8",
   "metadata": {},
   "source": [
    "## 3. Creating DeepChem Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DeepChem datasets for different properties\n",
    "\n",
    "# Dataset for LogP prediction\n",
    "logp_dataset = dc.data.NumpyDataset(\n",
    "    X=ecfp_features,\n",
    "    y=np.array(df['logp']).reshape(-1, 1),\n",
    "    ids=df['smiles']\n",
    ")\n",
    "\n",
    "# Dataset for solubility prediction\n",
    "solubility_dataset = dc.data.NumpyDataset(\n",
    "    X=ecfp_features,\n",
    "    y=np.array(df['solubility']).reshape(-1, 1),\n",
    "    ids=df['smiles']\n",
    ")\n",
    "\n",
    "print(f\"LogP dataset: {logp_dataset}\")\n",
    "print(f\"Features shape: {logp_dataset.X.shape}\")\n",
    "print(f\"Labels shape: {logp_dataset.y.shape}\")\n",
    "print(f\"Number of tasks: {logp_dataset.n_tasks}\")\n",
    "print(f\"Number of features: {logp_dataset.n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035e3a2",
   "metadata": {},
   "source": [
    "## 4. Data Splitting and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train/validation/test\n",
    "# For small datasets, we'll use a simple random split\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(\n",
    "    logp_dataset, \n",
    "    train_dir=None,\n",
    "    valid_dir=None,\n",
    "    test_dir=None,\n",
    "    frac_train=0.6,\n",
    "    frac_valid=0.2,\n",
    "    frac_test=0.2\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(valid_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# Apply normalization\n",
    "normalizer = dc.trans.NormalizationTransformer(\n",
    "    transform_y=True, dataset=train_dataset\n",
    ")\n",
    "\n",
    "train_dataset_norm = normalizer.transform(train_dataset)\n",
    "valid_dataset_norm = normalizer.transform(valid_dataset)\n",
    "test_dataset_norm = normalizer.transform(test_dataset)\n",
    "\n",
    "print(\"\\nDatasets normalized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cebf8b",
   "metadata": {},
   "source": [
    "## 5. Building and Training Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e09bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different types of models\n",
    "\n",
    "# 1. Multi-layer Perceptron (MLP)\n",
    "mlp_model = dc.models.MultitaskRegressor(\n",
    "    n_tasks=1,\n",
    "    n_features=1024,  # ECFP size\n",
    "    layer_sizes=[512, 256, 128],\n",
    "    dropouts=0.2,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"Training MLP model...\")\n",
    "mlp_model.fit(train_dataset_norm, nb_epoch=50)\n",
    "print(\"MLP training completed!\")\n",
    "\n",
    "# 2. Random Forest (for comparison)\n",
    "rf_model = dc.models.SklearnModel(\n",
    "    model_instance=dc.models.sklearn_models.RandomForestRegressor(\n",
    "        n_estimators=100, random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_model.fit(train_dataset)\n",
    "print(\"Random Forest training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da621f",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dddeeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on test set\n",
    "metric = dc.metrics.Metric(dc.metrics.r2_score)\n",
    "\n",
    "# MLP evaluation\n",
    "mlp_train_score = mlp_model.evaluate(train_dataset_norm, [metric])\n",
    "mlp_valid_score = mlp_model.evaluate(valid_dataset_norm, [metric])\n",
    "mlp_test_score = mlp_model.evaluate(test_dataset_norm, [metric])\n",
    "\n",
    "# Random Forest evaluation\n",
    "rf_train_score = rf_model.evaluate(train_dataset, [metric])\n",
    "rf_valid_score = rf_model.evaluate(valid_dataset, [metric])\n",
    "rf_test_score = rf_model.evaluate(test_dataset, [metric])\n",
    "\n",
    "print(\"Model Performance (R¬≤ Score):\")\n",
    "print(\"\\nMLP Model:\")\n",
    "print(f\"  Train R¬≤: {mlp_train_score['r2_score']:.3f}\")\n",
    "print(f\"  Valid R¬≤: {mlp_valid_score['r2_score']:.3f}\")\n",
    "print(f\"  Test R¬≤:  {mlp_test_score['r2_score']:.3f}\")\n",
    "\n",
    "print(\"\\nRandom Forest Model:\")\n",
    "print(f\"  Train R¬≤: {rf_train_score['r2_score']:.3f}\")\n",
    "print(f\"  Valid R¬≤: {rf_valid_score['r2_score']:.3f}\")\n",
    "print(f\"  Test R¬≤:  {rf_test_score['r2_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09090f",
   "metadata": {},
   "source": [
    "## 7. Making Predictions and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "mlp_predictions = mlp_model.predict(test_dataset_norm)\n",
    "rf_predictions = rf_model.predict(test_dataset)\n",
    "\n",
    "# Get actual values\n",
    "actual_values = test_dataset.y.flatten()\n",
    "\n",
    "# Denormalize MLP predictions\n",
    "mlp_predictions_denorm = normalizer.untransform(test_dataset_norm)\n",
    "mlp_pred_values = mlp_predictions_denorm.y.flatten()\n",
    "rf_pred_values = rf_predictions.flatten()\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# MLP predictions\n",
    "ax1.scatter(actual_values, mlp_pred_values, alpha=0.7, color='blue', s=100)\n",
    "ax1.plot([min(actual_values), max(actual_values)], \n",
    "         [min(actual_values), max(actual_values)], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual LogP')\n",
    "ax1.set_ylabel('Predicted LogP')\n",
    "ax1.set_title('MLP Model Predictions')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest predictions\n",
    "ax2.scatter(actual_values, rf_pred_values, alpha=0.7, color='green', s=100)\n",
    "ax2.plot([min(actual_values), max(actual_values)], \n",
    "         [min(actual_values), max(actual_values)], 'r--', lw=2)\n",
    "ax2.set_xlabel('Actual LogP')\n",
    "ax2.set_ylabel('Predicted LogP')\n",
    "ax2.set_title('Random Forest Predictions')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print prediction details\n",
    "print(\"Test Set Predictions:\")\n",
    "for i, smiles in enumerate(test_dataset.ids):\n",
    "    print(f\"{smiles}: Actual={actual_values[i]:.2f}, \"\n",
    "          f\"MLP={mlp_pred_values[i]:.2f}, RF={rf_pred_values[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65f155",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478908db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Random Forest, we can get feature importance\n",
    "if hasattr(rf_model.model_instance, 'feature_importances_'):\n",
    "    feature_importance = rf_model.model_instance.feature_importances_\n",
    "    \n",
    "    # Get top 20 most important features\n",
    "    top_features_idx = np.argsort(feature_importance)[-20:]\n",
    "    top_features_importance = feature_importance[top_features_idx]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(top_features_importance)), top_features_importance)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('ECFP Bit Index')\n",
    "    plt.title('Top 20 Most Important ECFP Features (Random Forest)')\n",
    "    plt.yticks(range(len(top_features_importance)), top_features_idx)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Most important feature index: {top_features_idx[-1]}\")\n",
    "    print(f\"Highest importance value: {top_features_importance[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80902aee",
   "metadata": {},
   "source": [
    "## 9. Virtual Screening Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6594cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of virtual screening with new molecules\n",
    "new_molecules = [\n",
    "    'CC(C)C1=CC=C(C=C1)C(=O)O',  # Similar to ibuprofen\n",
    "    'CC1=CC=CC=C1C(=O)O',  # Toluic acid\n",
    "    'C1=CC=C(C=C1)CCO',  # Phenethyl alcohol\n",
    "    'CC(C)(C)C1=CC=CC=C1'  # tert-Butylbenzene\n",
    "]\n",
    "\n",
    "print(\"Virtual Screening Results:\")\n",
    "print(\"Predicting LogP for new molecules...\\n\")\n",
    "\n",
    "# Featurize new molecules\n",
    "new_features = ecfp_featurizer.featurize(new_molecules)\n",
    "new_dataset = dc.data.NumpyDataset(X=new_features, ids=new_molecules)\n",
    "\n",
    "# Make predictions\n",
    "rf_new_predictions = rf_model.predict(new_dataset)\n",
    "\n",
    "# Display results\n",
    "for i, smiles in enumerate(new_molecules):\n",
    "    predicted_logp = rf_new_predictions[i, 0]\n",
    "    \n",
    "    # Classify based on LogP\n",
    "    if predicted_logp < 1:\n",
    "        classification = \"Hydrophilic\"\n",
    "    elif predicted_logp < 3:\n",
    "        classification = \"Moderate\"\n",
    "    else:\n",
    "        classification = \"Lipophilic\"\n",
    "    \n",
    "    print(f\"Molecule: {smiles}\")\n",
    "    print(f\"  Predicted LogP: {predicted_logp:.2f}\")\n",
    "    print(f\"  Classification: {classification}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa69d4",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try the following:\n",
    "1. Load a real dataset from DeepChem (e.g., Tox21, BACE)\n",
    "2. Try different featurization methods (GraphConv, MPNN)\n",
    "3. Implement cross-validation for more robust evaluation\n",
    "4. Explore multi-task learning for predicting multiple properties\n",
    "5. Use graph neural networks for molecular property prediction\n",
    "\n",
    "## Next Steps\n",
    "- Explore DeepChem's built-in datasets\n",
    "- Learn about graph neural networks for molecules\n",
    "- Study transfer learning in drug discovery\n",
    "- Investigate uncertainty quantification in molecular predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
