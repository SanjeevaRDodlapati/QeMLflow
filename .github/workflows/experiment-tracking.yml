name: Experiment Tracking Infrastructure

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/qemlflow/reproducibility/experiment_tracking.py'
      - 'src/qemlflow/reproducibility/result_validation.py'
      - 'config/experiment_tracking.yml'
      - 'tests/reproducibility/test_experiment_tracking.py'
      - '.github/workflows/experiment-tracking.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/qemlflow/reproducibility/experiment_tracking.py'
      - 'src/qemlflow/reproducibility/result_validation.py'
      - 'config/experiment_tracking.yml'
      - 'tests/reproducibility/test_experiment_tracking.py'
      - '.github/workflows/experiment-tracking.yml'
  schedule:
    # Run daily at 2 AM UTC to validate experiment tracking
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Test scope (basic, comprehensive, performance)'
        required: false
        default: 'comprehensive'
        type: choice
        options:
          - basic
          - comprehensive
          - performance

env:
  PYTHON_VERSION: '3.11'
  EXPERIMENT_BASE_DIR: 'test_experiments'
  QEMLFLOW_LOG_LEVEL: 'DEBUG'

jobs:
  experiment-tracking-validation:
    name: Validate Experiment Tracking
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11']
        test-scenario:
          - 'basic-functionality'
          - 'data-versioning'
          - 'result-validation'
          - 'parameter-tracking'
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          lfs: true
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Create Virtual Environment
        run: |
          python -m venv venv
          source venv/bin/activate
          echo "VIRTUAL_ENV=$VIRTUAL_ENV" >> $GITHUB_ENV
          echo "$VIRTUAL_ENV/bin" >> $GITHUB_PATH
      
      - name: Install Dependencies
        run: |
          pip install --upgrade pip setuptools wheel
          pip install -e ".[dev,test]"
          pip install pytest-xdist pytest-cov pytest-benchmark
      
      - name: Validate Configuration
        run: |
          python -c "
          import yaml
          import sys
          from pathlib import Path
          
          config_path = Path('config/experiment_tracking.yml')
          if not config_path.exists():
              print('ERROR: Experiment tracking config not found')
              sys.exit(1)
          
          with open(config_path) as f:
              config = yaml.safe_load(f)
          
          required_sections = ['storage', 'metadata', 'parameters', 'data_versioning', 'results']
          missing = [s for s in required_sections if s not in config]
          if missing:
              print(f'ERROR: Missing config sections: {missing}')
              sys.exit(1)
          
          print('âœ… Configuration validation passed')
          "
      
      - name: Test Basic Functionality
        if: matrix.test-scenario == 'basic-functionality'
        run: |
          pytest tests/reproducibility/test_experiment_tracking.py::test_experiment_creation \
                 tests/reproducibility/test_experiment_tracking.py::test_parameter_tracking \
                 tests/reproducibility/test_experiment_tracking.py::test_metadata_capture \
                 -v --tb=short --durations=10
      
      - name: Test Data Versioning
        if: matrix.test-scenario == 'data-versioning'
        run: |
          pytest tests/reproducibility/test_experiment_tracking.py::test_data_versioning \
                 tests/reproducibility/test_experiment_tracking.py::test_data_fingerprinting \
                 tests/reproducibility/test_experiment_tracking.py::test_data_lineage \
                 -v --tb=short --durations=10
      
      - name: Test Result Validation
        if: matrix.test-scenario == 'result-validation'
        run: |
          pytest tests/reproducibility/test_experiment_tracking.py::test_result_validation \
                 tests/reproducibility/test_experiment_tracking.py::test_result_comparison \
                 tests/reproducibility/test_experiment_tracking.py::test_statistical_validation \
                 -v --tb=short --durations=10
      
      - name: Test Parameter Tracking
        if: matrix.test-scenario == 'parameter-tracking'
        run: |
          pytest tests/reproducibility/test_experiment_tracking.py::test_parameter_validation \
                 tests/reproducibility/test_experiment_tracking.py::test_parameter_versioning \
                 tests/reproducibility/test_experiment_tracking.py::test_sensitive_parameters \
                 -v --tb=short --durations=10
      
      - name: Integration Test with Environment Determinism
        run: |
          python -c "
          from src.qemlflow.reproducibility import (
              ExperimentTracker, get_environment_manager
          )
          import tempfile
          from pathlib import Path
          
          # Test integration
          with tempfile.TemporaryDirectory() as tmp_dir:
              tracker = ExperimentTracker(base_dir=tmp_dir)
              env_manager = get_environment_manager()
              
              # Create experiment with environment capture
              experiment = tracker.create_experiment(
                  name='integration_test',
                  description='Test integration with environment determinism'
              )
              
              # Capture environment
              env_fingerprint = env_manager.capture_environment()
              experiment.add_environment_info(env_fingerprint.to_dict())
              
              # Add parameters
              experiment.log_parameter('test_param', 42)
              experiment.log_parameter('model_type', 'test')
              
              # Simulate results
              experiment.log_metric('accuracy', 0.95)
              experiment.log_metric('loss', 0.05)
              
              # Finish experiment
              experiment.finish()
              
              print('âœ… Integration test passed')
          "
      
      - name: Performance Benchmark
        if: github.event.inputs.test_scope == 'performance' || github.event_name == 'schedule'
        run: |
          pytest tests/reproducibility/test_experiment_tracking.py::test_performance_benchmarks \
                 --benchmark-only --benchmark-sort=mean \
                 --benchmark-min-rounds=5 \
                 -v
      
      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: experiment-tracking-results-${{ matrix.python-version }}-${{ matrix.test-scenario }}
          path: |
            test_experiments/
            logs/
            .coverage
          retention-days: 7

  experiment-data-integrity:
    name: Test Data Integrity & Versioning
    runs-on: ubuntu-latest
    needs: experiment-tracking-validation
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[dev,test]"
      
      - name: Test Data Integrity
        run: |
          python -c "
          from src.qemlflow.reproducibility.experiment_tracking import ExperimentTracker
          from src.qemlflow.reproducibility.result_validation import ResultValidator
          import tempfile
          import numpy as np
          
          with tempfile.TemporaryDirectory() as tmp_dir:
              tracker = ExperimentTracker(base_dir=tmp_dir)
              validator = ResultValidator()
              
              # Create experiment
              exp = tracker.create_experiment(
                  name='data_integrity_test',
                  description='Test data integrity and versioning'
              )
              
              # Generate test data
              data = np.random.rand(1000, 10)
              results = np.random.rand(100)
              
              # Log data with versioning
              exp.log_data('input_data', data)
              exp.log_data('results', results)
              
              # Verify data integrity
              retrieved_data = exp.get_data('input_data')
              assert np.array_equal(data, retrieved_data), 'Data integrity check failed'
              
              # Test data versioning
              modified_data = data * 2
              exp.log_data('input_data', modified_data, version='v2')
              
              # Verify versioning
              v1_data = exp.get_data('input_data', version='v1')
              v2_data = exp.get_data('input_data', version='v2')
              
              assert np.array_equal(v1_data, data), 'Version 1 data mismatch'
              assert np.array_equal(v2_data, modified_data), 'Version 2 data mismatch'
              
              exp.finish()
              print('âœ… Data integrity and versioning tests passed')
          "

  experiment-comparison-validation:
    name: Validate Experiment Comparison
    runs-on: ubuntu-latest
    needs: experiment-tracking-validation
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[dev,test]"
      
      - name: Test Experiment Comparison
        run: |
          python -c "
          from src.qemlflow.reproducibility.experiment_tracking import ExperimentTracker
          import tempfile
          import numpy as np
          
          with tempfile.TemporaryDirectory() as tmp_dir:
              tracker = ExperimentTracker(base_dir=tmp_dir)
              
              # Create multiple experiments
              experiments = []
              for i in range(3):
                  exp = tracker.create_experiment(
                      name=f'comparison_test_{i}',
                      description=f'Test experiment {i} for comparison'
                  )
                  
                  # Log different parameters and metrics
                  exp.log_parameter('learning_rate', 0.01 * (i + 1))
                  exp.log_parameter('batch_size', 32 * (i + 1))
                  exp.log_metric('accuracy', 0.8 + i * 0.05)
                  exp.log_metric('loss', 0.5 - i * 0.1)
                  
                  exp.finish()
                  experiments.append(exp)
              
              # Test comparison functionality
              comparison = tracker.compare_experiments([exp.id for exp in experiments])
              
              assert 'parameters' in comparison, 'Parameters not in comparison'
              assert 'metrics' in comparison, 'Metrics not in comparison'
              assert len(comparison['experiments']) == 3, 'Wrong number of experiments'
              
              print('âœ… Experiment comparison validation passed')
          "

  security-compliance-check:
    name: Security & Compliance Check
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Check Sensitive Data Handling
        run: |
          # Check for hardcoded secrets or sensitive data
          if grep -r "password\|secret\|key\|token" src/qemlflow/reproducibility/experiment_tracking.py --exclude-dir=__pycache__ | grep -v "# noqa" | grep -v "TODO" | grep -v "FIXME"; then
            echo "WARNING: Potential sensitive data found in experiment tracking code"
            exit 1
          fi
          
          echo "âœ… No sensitive data found in experiment tracking code"
      
      - name: Validate Configuration Security
        run: |
          python -c "
          import yaml
          from pathlib import Path
          
          config_path = Path('config/experiment_tracking.yml')
          with open(config_path) as f:
              config = yaml.safe_load(f)
          
          # Check security settings
          security = config.get('security', {})
          
          # Verify encryption options are available
          assert 'encryption' in security, 'Encryption settings missing'
          
          # Verify audit logging is configured
          assert 'audit' in security, 'Audit logging settings missing'
          
          # Verify sensitive parameter handling
          params = config.get('parameters', {})
          assert 'sensitive_params' in params, 'Sensitive parameter handling missing'
          
          print('âœ… Security configuration validation passed')
          "

  deployment-readiness:
    name: Deployment Readiness Check
    runs-on: ubuntu-latest
    needs: [experiment-tracking-validation, experiment-data-integrity, experiment-comparison-validation]
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Validate Production Configuration
        run: |
          python -c "
          import yaml
          from pathlib import Path
          
          config_path = Path('config/experiment_tracking.yml')
          with open(config_path) as f:
              config = yaml.safe_load(f)
          
          # Check production readiness
          storage = config.get('storage', {})
          assert storage.get('compression', {}).get('enabled', False), 'Compression should be enabled for production'
          assert storage.get('retention', {}).get('default_days', 0) > 0, 'Retention policy required'
          
          monitoring = config.get('monitoring', {})
          assert monitoring.get('performance', {}).get('enabled', False), 'Performance monitoring required'
          
          print('âœ… Production configuration validation passed')
          "
      
      - name: Performance Requirements Check
        run: |
          # Verify performance meets requirements
          pytest tests/reproducibility/test_experiment_tracking.py::test_performance_requirements \
                 --benchmark-only -v
      
      - name: Create Deployment Summary
        run: |
          echo "## Experiment Tracking Deployment Summary" > deployment-summary.md
          echo "" >> deployment-summary.md
          echo "### âœ… All Tests Passed" >> deployment-summary.md
          echo "- Basic functionality validated" >> deployment-summary.md
          echo "- Data versioning working correctly" >> deployment-summary.md
          echo "- Result validation implemented" >> deployment-summary.md
          echo "- Parameter tracking functional" >> deployment-summary.md
          echo "- Security compliance verified" >> deployment-summary.md
          echo "- Performance requirements met" >> deployment-summary.md
          echo "" >> deployment-summary.md
          echo "### Configuration" >> deployment-summary.md
          echo "- Storage: Local with compression enabled" >> deployment-summary.md
          echo "- Retention: 365 days default, 180 days archive" >> deployment-summary.md
          echo "- Security: Audit logging enabled" >> deployment-summary.md
          echo "- Monitoring: Performance tracking enabled" >> deployment-summary.md
          echo "" >> deployment-summary.md
          echo "**Status: Ready for Production Deployment** ðŸš€" >> deployment-summary.md
      
      - name: Upload Deployment Summary
        uses: actions/upload-artifact@v3
        with:
          name: experiment-tracking-deployment-summary
          path: deployment-summary.md
          retention-days: 30
